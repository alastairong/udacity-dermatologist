{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dermatologist-AI Project: Single Network Approach\n",
    "Workbook for a single network approach that classifies images into melanomas, nevus, or SBK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 total categories.\n",
      "There are 2750 total images.\n",
      "\n",
      "There are 2000 training images.\n",
      "There are 150 validation images.\n",
      "There are 600 test images.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "#define dataset import function\n",
    "def load_dataset(path, shuffle):\n",
    "    data = load_files(path, shuffle=shuffle)\n",
    "    file_paths = np.array(data['filenames'])\n",
    "    one_hot_labels = np_utils.to_categorical(np.array(data['target']),3)\n",
    "    return file_paths, one_hot_labels\n",
    "\n",
    "#import datasets\n",
    "train_files, train_labels = load_dataset('../data/train', True)\n",
    "valid_files, valid_labels = load_dataset('../data/valid', True)\n",
    "test_files, test_labels = load_dataset('../data/test', False)\n",
    "\n",
    "# load list of skin condition names\n",
    "skin_names = [item[14:-1] for item in sorted(glob(\"../data/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total categories.' % len(skin_names))\n",
    "print('There are %s total images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training images.' % len(train_files))\n",
    "print('There are %d validation images.' % len(valid_files))\n",
    "print('There are %d test images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert images into 4D tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from tqdm import tqdm\n",
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "img_width = 224\n",
    "img_height = 224\n",
    "\n",
    "#define image processing functions (from udacity dog project)\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(img_width, img_height))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    x = preprocess_input(x) # convert format to VGG19 compatible\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [03:56<00:00,  8.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 150/150 [00:32<00:00,  4.58it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 600/600 [03:34<00:00,  9.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Pre-process the data for Keras\n",
    "# Skip if we already have pickled files\n",
    "\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickle tensors to pick up where I left off in future\n",
    "# Skip if we already have pickled files\n",
    "\n",
    "with open('train_tensors.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_tensors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('valid_tensors.pickle', 'wb') as handle:\n",
    "    pickle.dump(valid_tensors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('test_tensors.pickle', 'wb') as handle:\n",
    "    pickle.dump(test_tensors, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un-pickle tensors\n",
    "with open('train_tensors.pickle', 'rb') as handle:\n",
    "    train_tensors = pickle.load(handle)\n",
    "    \n",
    "with open('valid_tensors.pickle', 'rb') as handle:\n",
    "    valid_tensors = pickle.load(handle)\n",
    "\n",
    "with open('test_tensors.pickle', 'rb') as handle:\n",
    "    test_tensors = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new network based on first 2 blocks of VGGNet, plus 1 trainable convolutional block, followed by fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 32)      896       \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 32)      9248      \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 64)      18496     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 6,923,939\n",
      "Trainable params: 6,923,939\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "\n",
    "network = Sequential()\n",
    "\n",
    "network.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', name='block1_conv1', \n",
    "                        input_shape=(224, 224, 3)))\n",
    "network.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', name='block1_conv2'))\n",
    "network.add(MaxPooling2D(pool_size=2, name='block1_pool'))\n",
    "network.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', name='block2_conv1'))\n",
    "network.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', name='block2_conv2'))\n",
    "network.add(MaxPooling2D(pool_size=2, name='block2_pool'))\n",
    "network.add(Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', name='block3_conv1'))\n",
    "network.add(Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', name='block3_conv2'))\n",
    "network.add(MaxPooling2D(pool_size=2, name='block3_pool'))\n",
    "network.add(Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', name='block4_conv4'))\n",
    "network.add(MaxPooling2D(pool_size=2, name='block4_pool'))\n",
    "network.add(Flatten())\n",
    "network.add(Dense(256, activation='relu'))\n",
    "network.add(Dropout(0.1))\n",
    "network.add(Dense(256, activation='relu'))\n",
    "network.add(Dropout(0.1))\n",
    "network.add(Dense(3, activation='softmax'))\n",
    "\n",
    "network.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "opt = optimizers.rmsprop(lr=0.001, decay=1e-5)\n",
    "network.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.06269316 -0.0566688  -0.02081369 -0.13329379  0.05764002 -0.11850986\n",
      " -0.13227654  0.0306948  -0.01983214 -0.02267387 -0.13450496 -0.1080693\n",
      "  0.02717148  0.03668171  0.02062982 -0.01437289  0.03317909  0.07267973\n",
      "  0.11336608 -0.12567437 -0.0963694  -0.07831686  0.11212419  0.01953557\n",
      "  0.03681415  0.03333473 -0.09230891  0.05864665  0.00588362 -0.04044356\n",
      "  0.06333105 -0.1293999 ]\n",
      "[-0.009663    0.09304436 -0.09162223  0.03078239 -0.02486993 -0.02157797\n",
      " -0.00193892  0.02193516  0.09954248  0.02634992  0.00176598  0.04917076\n",
      "  0.03281501  0.10029614  0.08075579 -0.03865772  0.04147071 -0.07012057\n",
      "  0.02585784 -0.07144655  0.07710044  0.0739364   0.07543296  0.08544782\n",
      " -0.08829326  0.09243272 -0.02244203 -0.07893455 -0.08762927 -0.04034744\n",
      "  0.0185064  -0.00754954]\n"
     ]
    }
   ],
   "source": [
    "layer = network.layers[0]\n",
    "weights = layer.get_weights()\n",
    "print(weights[0][0][0][0])\n",
    "\n",
    "layer = network.layers[1]\n",
    "weights = layer.get_weights()\n",
    "print(weights[0][0][1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19 samples, validate on 150 samples\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "epochs = 1\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/best.weights.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "network.fit(valid_tensors[:19], valid_labels[:19],\n",
    "            validation_data=(valid_tensors, valid_labels),\n",
    "            epochs=epochs,\n",
    "            batch_size=20,\n",
    "            callbacks=[checkpointer],\n",
    "            verbose=1)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05128064  0.09263844 -0.03486281 -0.07652774  0.04095103 -0.01146138\n",
      " -0.07247883  0.07988345  0.0801482  -0.09025099 -0.09624356 -0.13937876\n",
      " -0.06018323  0.11773434 -0.13404813  0.11191047  0.0563373   0.09155356\n",
      " -0.05743978  0.05993506 -0.11269011  0.08011148 -0.13677289  0.01127938\n",
      "  0.12040504 -0.09471595  0.01799991 -0.03334678 -0.02066643 -0.00258884\n",
      "  0.00497484 -0.03094213]\n"
     ]
    }
   ],
   "source": [
    "layer = network.layers[0]\n",
    "weights = layer.get_weights()\n",
    "print(weights[0][0][0][0])\n",
    "\n",
    "layer = network.layers[1]\n",
    "weights = layer.get_weights()\n",
    "print(weights[0][0][1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05128064  0.09263844 -0.03486281 -0.07652774  0.04095103 -0.01146138\n",
      " -0.07247883  0.07988345  0.0801482  -0.09025099 -0.09624356 -0.13937876\n",
      " -0.06018323  0.11773434 -0.13404813  0.11191047  0.0563373   0.09155356\n",
      " -0.05743978  0.05993506 -0.11269011  0.08011148 -0.13677289  0.01127938\n",
      "  0.12040504 -0.09471595  0.01799991 -0.03334678 -0.02066643 -0.00258884\n",
      "  0.00497484 -0.03094213]\n"
     ]
    }
   ],
   "source": [
    "layer = network.layers[0]\n",
    "weights = layer.get_weights()\n",
    "print(weights[0][0][0][0])\n",
    "\n",
    "layer = network.layers[1]\n",
    "weights = layer.get_weights()\n",
    "print(weights[0][0][1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05128064  0.09263844 -0.03486281 -0.07652774  0.04095103 -0.01146138\n",
      " -0.07247883  0.07988345  0.0801482  -0.09025099 -0.09624356 -0.13937876\n",
      " -0.06018323  0.11773434 -0.13404813  0.11191047  0.0563373   0.09155356\n",
      " -0.05743978  0.05993506 -0.11269011  0.08011148 -0.13677289  0.01127938\n",
      "  0.12040504 -0.09471595  0.01799991 -0.03334678 -0.02066643 -0.00258884\n",
      "  0.00497484 -0.03094213]\n"
     ]
    }
   ],
   "source": [
    "layer = network.layers[0]\n",
    "weights = layer.get_weights()\n",
    "print(weights[0][0][0][0])\n",
    "\n",
    "layer = network.layers[1]\n",
    "weights = layer.get_weights()\n",
    "print(weights[0][0][1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create output excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the model weights with the best validation loss.\n",
    "\n",
    "network.load_weights('saved_models/best.weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 600/600 [00:04<00:00, 143.42it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "y_pred = pd.DataFrame(columns=[\"task_1\", \"task_2\"])\n",
    "\n",
    "for ii in tqdm(range(len(test_files))):\n",
    "    path = test_files[ii]\n",
    "    prediction = np.argmax(network.predict(np.expand_dims(test_tensors[ii], axis=0)))\n",
    "    if prediction == 0:\n",
    "        y_pred.loc[path] = [1, 0]\n",
    "    if prediction == 2:\n",
    "        y_pred.loc[path] = [0, 1]\n",
    "    else:\n",
    "        y_pred.loc[path] = [0, 0]\n",
    "\n",
    "y_pred.to_csv(\"predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
