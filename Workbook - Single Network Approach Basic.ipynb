{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dermatologist-AI Project: Single Network Approach\n",
    "Workbook for a single network approach that classifies images into melanomas, nevus, or SBK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 total categories.\n",
      "There are 2750 total images.\n",
      "\n",
      "There are 2000 training images.\n",
      "There are 150 validation images.\n",
      "There are 600 test images.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "#define dataset import function\n",
    "def load_dataset(path, shuffle):\n",
    "    data = load_files(path, shuffle=shuffle)\n",
    "    file_paths = np.array(data['filenames'])\n",
    "    one_hot_labels = np_utils.to_categorical(np.array(data['target']),3)\n",
    "    return file_paths, one_hot_labels\n",
    "\n",
    "#import datasets\n",
    "train_files, train_labels = load_dataset('../data/train', True)\n",
    "valid_files, valid_labels = load_dataset('../data/valid', True)\n",
    "test_files, test_labels = load_dataset('../data/test', False)\n",
    "\n",
    "# load list of skin condition names\n",
    "skin_names = [item[14:-1] for item in sorted(glob(\"../data/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total categories.' % len(skin_names))\n",
    "print('There are %s total images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training images.' % len(train_files))\n",
    "print('There are %d validation images.' % len(valid_files))\n",
    "print('There are %d test images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert images into 4D tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from tqdm import tqdm\n",
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "img_width = 224\n",
    "img_height = 224\n",
    "\n",
    "#define image processing functions (from udacity dog project)\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(img_width, img_height))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    x = preprocess_input(x) # convert format to VGG19 compatible\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-process the data for Keras\n",
    "# Skip if we already have pickled files\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickle tensors to pick up where I left off in future\n",
    "# Skip if we already have pickled files\n",
    "\n",
    "with open('train_tensors.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_tensors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('valid_tensors.pickle', 'wb') as handle:\n",
    "    pickle.dump(valid_tensors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('test_tensors.pickle', 'wb') as handle:\n",
    "    pickle.dump(test_tensors, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Un-pickle tensors\n",
    "with open('train_tensors.pickle', 'rb') as handle:\n",
    "    train_tensors = pickle.load(handle)\n",
    "    \n",
    "with open('valid_tensors.pickle', 'rb') as handle:\n",
    "    valid_tensors = pickle.load(handle)\n",
    "\n",
    "with open('test_tensors.pickle', 'rb') as handle:\n",
    "    test_tensors = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new network based on first 2 blocks of VGGNet, plus 1 trainable convolutional block, followed by fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 32)      896       \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 32)      9248      \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 64)      18496     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 6,923,939\n",
      "Trainable params: 6,923,939\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "\n",
    "network = Sequential()\n",
    "\n",
    "network.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', name='block1_conv1', \n",
    "                        input_shape=(224, 224, 3)))\n",
    "network.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', name='block1_conv2'))\n",
    "network.add(MaxPooling2D(pool_size=2, name='block1_pool'))\n",
    "network.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', name='block2_conv1'))\n",
    "network.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', name='block2_conv2'))\n",
    "network.add(MaxPooling2D(pool_size=2, name='block2_pool'))\n",
    "network.add(Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', name='block3_conv1'))\n",
    "network.add(Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', name='block3_conv2'))\n",
    "network.add(MaxPooling2D(pool_size=2, name='block3_pool'))\n",
    "network.add(Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', name='block4_conv4'))\n",
    "network.add(MaxPooling2D(pool_size=2, name='block4_pool'))\n",
    "network.add(Flatten())\n",
    "network.add(Dense(256, activation='relu'))\n",
    "network.add(Dropout(0.1))\n",
    "network.add(Dense(256, activation='relu'))\n",
    "network.add(Dropout(0.1))\n",
    "network.add(Dense(3, activation='softmax'))\n",
    "\n",
    "network.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "opt = optimizers.rmsprop(lr=0.001, decay=1e-5)\n",
    "network.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.08407555  0.0898973  -0.11789246  0.07885088  0.01906516 -0.01534862\n",
      "  0.06545247  0.08131252 -0.12821057 -0.0716939  -0.11466567  0.1022532\n",
      " -0.11136031 -0.03130054 -0.00631005  0.05485012 -0.0221526  -0.11823421\n",
      " -0.01435578 -0.03684946 -0.04736159 -0.09313647 -0.05099712  0.00373368\n",
      "  0.02384506  0.0475778  -0.03917969 -0.09484509 -0.08085485  0.03216121\n",
      "  0.00047222  0.09072375]\n",
      "[ 0.08358634  0.03556395  0.05611759 -0.05931219  0.01745763  0.03733291\n",
      "  0.08405814  0.0496532  -0.0580952  -0.07578161 -0.03508082 -0.04225526\n",
      " -0.02258917  0.04271607 -0.08879847 -0.01265575 -0.06838452  0.01660739\n",
      " -0.05623283 -0.10075863 -0.03741194 -0.04985768  0.09860714 -0.07436102\n",
      " -0.08941995 -0.02911378 -0.07874042 -0.00496401 -0.07723515  0.08905144\n",
      "  0.09574768 -0.04981188]\n"
     ]
    }
   ],
   "source": [
    "layer = network.layers[0]\n",
    "weights = layer.get_weights()\n",
    "print(weights[0][0][0][0])\n",
    "\n",
    "layer = network.layers[1]\n",
    "weights = layer.get_weights()\n",
    "print(weights[0][0][1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19 samples, validate on 150 samples\n",
      "Epoch 1/1\n",
      "Epoch 00000: val_loss improved from inf to 7.73669, saving model to saved_models/best.weights.hdf5\n",
      "19/19 [==============================] - 0s - loss: 2.5450 - acc: 0.8421 - val_loss: 7.7367 - val_acc: 0.5200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d580384ef0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "epochs = 1\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/best.weights.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "network.fit(valid_tensors[:19], valid_labels[:19],\n",
    "            validation_data=(valid_tensors, valid_labels),\n",
    "            epochs=epochs,\n",
    "            batch_size=20,\n",
    "            callbacks=[checkpointer],\n",
    "            verbose=1)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.08091331  0.08673506 -0.11473022  0.07568864  0.01590291 -0.01218638\n",
      "  0.06861471  0.08447476 -0.13137282 -0.07485614 -0.11150343  0.09909096\n",
      " -0.11452255 -0.02813831 -0.00947229  0.05801236 -0.01899038 -0.12139645\n",
      " -0.01751802 -0.0400117  -0.05052383 -0.09629871 -0.05415936  0.00689592\n",
      "  0.02068282  0.04441556 -0.03601744 -0.09168284 -0.08401709  0.03532345\n",
      "  0.00363446  0.093886  ]\n",
      "[ 0.08674857  0.03872617  0.05295535 -0.05614994  0.01429538  0.03417067\n",
      "  0.0808959   0.05281544 -0.06125744 -0.07261938 -0.03191858 -0.03909305\n",
      " -0.02575133  0.03955384 -0.08563669 -0.00949351 -0.06522228  0.01976943\n",
      " -0.05307069 -0.0975964  -0.0342497  -0.04669544  0.10176938 -0.0711988\n",
      " -0.0862577  -0.03227602 -0.08190266 -0.00180177 -0.07407291  0.0858893\n",
      "  0.09258544 -0.04664964]\n"
     ]
    }
   ],
   "source": [
    "layer = network.layers[0]\n",
    "weights = layer.get_weights()\n",
    "print(weights[0][0][0][0])\n",
    "\n",
    "layer = network.layers[1]\n",
    "weights = layer.get_weights()\n",
    "print(weights[0][0][1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.08091331  0.08673506 -0.11473022  0.07568864  0.01590291 -0.01218638\n",
      "  0.06861471  0.08447476 -0.13137282 -0.07485614 -0.11150343  0.09909096\n",
      " -0.11452255 -0.02813831 -0.00947229  0.05801236 -0.01899038 -0.12139645\n",
      " -0.01751802 -0.0400117  -0.05052383 -0.09629871 -0.05415936  0.00689592\n",
      "  0.02068282  0.04441556 -0.03601744 -0.09168284 -0.08401709  0.03532345\n",
      "  0.00363446  0.093886  ]\n",
      "[ 0.08674857  0.03872617  0.05295535 -0.05614994  0.01429538  0.03417067\n",
      "  0.0808959   0.05281544 -0.06125744 -0.07261938 -0.03191858 -0.03909305\n",
      " -0.02575133  0.03955384 -0.08563669 -0.00949351 -0.06522228  0.01976943\n",
      " -0.05307069 -0.0975964  -0.0342497  -0.04669544  0.10176938 -0.0711988\n",
      " -0.0862577  -0.03227602 -0.08190266 -0.00180177 -0.07407291  0.0858893\n",
      "  0.09258544 -0.04664964]\n"
     ]
    }
   ],
   "source": [
    "layer = network.layers[0]\n",
    "weights = layer.get_weights()\n",
    "print(weights[0][0][0][0])\n",
    "\n",
    "layer = network.layers[1]\n",
    "weights = layer.get_weights()\n",
    "print(weights[0][0][1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.08091331  0.08673506 -0.11473022  0.07568864  0.01590291 -0.01218638\n",
      "  0.06861471  0.08447476 -0.13137282 -0.07485614 -0.11150343  0.09909096\n",
      " -0.11452255 -0.02813831 -0.00947229  0.05801236 -0.01899038 -0.12139645\n",
      " -0.01751802 -0.0400117  -0.05052383 -0.09629871 -0.05415936  0.00689592\n",
      "  0.02068282  0.04441556 -0.03601744 -0.09168284 -0.08401709  0.03532345\n",
      "  0.00363446  0.093886  ]\n",
      "[ 0.08674857  0.03872617  0.05295535 -0.05614994  0.01429538  0.03417067\n",
      "  0.0808959   0.05281544 -0.06125744 -0.07261938 -0.03191858 -0.03909305\n",
      " -0.02575133  0.03955384 -0.08563669 -0.00949351 -0.06522228  0.01976943\n",
      " -0.05307069 -0.0975964  -0.0342497  -0.04669544  0.10176938 -0.0711988\n",
      " -0.0862577  -0.03227602 -0.08190266 -0.00180177 -0.07407291  0.0858893\n",
      "  0.09258544 -0.04664964]\n"
     ]
    }
   ],
   "source": [
    "layer = network.layers[0]\n",
    "weights = layer.get_weights()\n",
    "print(weights[0][0][0][0])\n",
    "\n",
    "layer = network.layers[1]\n",
    "weights = layer.get_weights()\n",
    "print(weights[0][0][1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create output excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the model weights with the best validation loss.\n",
    "\n",
    "network.load_weights('saved_models/best.weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 600/600 [00:04<00:00, 143.42it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "y_pred = pd.DataFrame(columns=[\"task_1\", \"task_2\"])\n",
    "\n",
    "for ii in tqdm(range(len(test_files))):\n",
    "    path = test_files[ii]\n",
    "    prediction = np.argmax(network.predict(np.expand_dims(test_tensors[ii], axis=0)))\n",
    "    if prediction == 0:\n",
    "        y_pred.loc[path] = [1, 0]\n",
    "    if prediction == 2:\n",
    "        y_pred.loc[path] = [0, 1]\n",
    "    else:\n",
    "        y_pred.loc[path] = [0, 0]\n",
    "\n",
    "y_pred.to_csv(\"predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
