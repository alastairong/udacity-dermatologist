{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dermatologist-AI Project: Single Network Approach\n",
    "Workbook for a single network approach that classifies images into melanomas, nevus, or SBK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 total categories.\n",
      "There are 2750 total images.\n",
      "\n",
      "There are 2000 training images.\n",
      "There are 150 validation images.\n",
      "There are 600 test images.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "#define dataset import function\n",
    "def load_dataset(path, shuffle):\n",
    "    data = load_files(path, shuffle=shuffle)\n",
    "    file_paths = np.array(data['filenames'])\n",
    "    one_hot_labels = np_utils.to_categorical(np.array(data['target']),3)\n",
    "    return file_paths, one_hot_labels\n",
    "\n",
    "#import datasets\n",
    "train_files, train_labels = load_dataset('../data/train', True)\n",
    "valid_files, valid_labels = load_dataset('../data/valid', True)\n",
    "test_files, test_labels = load_dataset('../data/test', False)\n",
    "\n",
    "# load list of skin condition names\n",
    "skin_names = [item[14:-1] for item in sorted(glob(\"../data/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total categories.' % len(skin_names))\n",
    "print('There are %s total images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training images.' % len(train_files))\n",
    "print('There are %d validation images.' % len(valid_files))\n",
    "print('There are %d test images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's first understand the dataset\n",
    "Various simple analysis to understand types of pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     374.0\n",
      "1    1372.0\n",
      "2     254.0\n",
      "dtype: float64\n",
      "0    30.0\n",
      "1    78.0\n",
      "2    42.0\n",
      "dtype: float64\n",
      "0    117.0\n",
      "1    393.0\n",
      "2     90.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.DataFrame(train_labels)\n",
    "valid_data = pd.DataFrame(valid_labels)\n",
    "test_data = pd.DataFrame(test_labels)\n",
    "\n",
    "print(train_data.sum(axis=0))\n",
    "print(valid_data.sum(axis=0))\n",
    "print(test_data.sum(axis=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert images into 4D tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "img_width = 224\n",
    "img_height = 224\n",
    "\n",
    "#define image processing functions (from udacity dog project)\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(img_width, img_height))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [04:15<00:00,  7.82it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 150/150 [00:34<00:00,  4.96it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 600/600 [03:42<00:00,  9.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# Pre-process the data for Keras\n",
    "# Skip if we already have pickled files\n",
    "## FIX PREPROCESSING!\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickle tensors to pick up where I left off in future\n",
    "# Skip if we already have pickled files\n",
    "\n",
    "with open('train_tensors.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_tensors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('valid_tensors.pickle', 'wb') as handle:\n",
    "    pickle.dump(valid_tensors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('test_tensors.pickle', 'wb') as handle:\n",
    "    pickle.dump(test_tensors, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Un-pickle tensors\n",
    "with open('train_tensors.pickle', 'rb') as handle:\n",
    "    train_tensors = pickle.load(handle)\n",
    "    \n",
    "with open('valid_tensors.pickle', 'rb') as handle:\n",
    "    valid_tensors = pickle.load(handle)\n",
    "\n",
    "with open('test_tensors.pickle', 'rb') as handle:\n",
    "    test_tensors = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new network based on first 2 blocks of VGGNet, plus 1 trainable convolutional block, followed by fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2048)              51382272  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 6147      \n",
      "=================================================================\n",
      "Total params: 70,889,539\n",
      "Trainable params: 60,304,387\n",
      "Non-trainable params: 10,585,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "\n",
    "network = Sequential()\n",
    "\n",
    "#First 4 blocks of VGG19\n",
    "network.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', trainable=False, name='block1_conv1', \n",
    "                        input_shape=(224, 224, 3)))\n",
    "network.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', trainable=False, name='block1_conv2'))\n",
    "network.add(MaxPooling2D(pool_size=2, name='block1_pool'))\n",
    "network.add(Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', trainable=False, name='block2_conv1'))\n",
    "network.add(Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', trainable=False, name='block2_conv2'))\n",
    "network.add(MaxPooling2D(pool_size=2, name='block2_pool'))\n",
    "network.add(Conv2D(filters=256, kernel_size=3, padding='same', activation='relu', trainable=False, name='block3_conv1'))\n",
    "network.add(Conv2D(filters=256, kernel_size=3, padding='same', activation='relu', trainable=False, name='block3_conv2'))\n",
    "network.add(Conv2D(filters=256, kernel_size=3, padding='same', activation='relu', trainable=False, name='block3_conv3'))\n",
    "network.add(Conv2D(filters=256, kernel_size=3, padding='same', activation='relu', trainable=False, name='block3_conv4'))\n",
    "network.add(MaxPooling2D(pool_size=2, name='block3_pool'))\n",
    "network.add(Conv2D(filters=512, kernel_size=3, padding='same', activation='relu', trainable=False, name='block4_conv1'))\n",
    "network.add(Conv2D(filters=512, kernel_size=3, padding='same', activation='relu', trainable=False, name='block4_conv2'))\n",
    "network.add(Conv2D(filters=512, kernel_size=3, padding='same', activation='relu', trainable=False, name='block4_conv3'))\n",
    "network.add(Conv2D(filters=512, kernel_size=3, padding='same', activation='relu', trainable=False, name='block4_conv4'))\n",
    "network.add(MaxPooling2D(pool_size=2, name='block4_pool'))\n",
    "\n",
    "#First 1 trainable convnet block\n",
    "network.add(Dropout(0.5))\n",
    "network.add(Conv2D(filters=512, kernel_size=3, padding='same', activation='relu', trainable=True, name='block5_conv1'))\n",
    "network.add(Conv2D(filters=512, kernel_size=3, padding='same', activation='relu', trainable=True, name='block5_conv2'))\n",
    "network.add(MaxPooling2D(pool_size=2, name='block5_pool'))\n",
    "network.add(Dropout(0.5))\n",
    "\n",
    "#Trainable dense layers\n",
    "network.add(Flatten())\n",
    "network.add(Dense(2048, activation='relu'))\n",
    "network.add(Dropout(0.5))\n",
    "network.add(Dense(2048, activation='relu'))\n",
    "network.add(Dropout(0.5))\n",
    "network.add(Dense(3, activation='softmax'))\n",
    "\n",
    "network.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download VGG19 weights and set for VGGnet layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "weights_path = 'vgg19_weights.h5'#'https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "f = h5py.File(weights_path, 'r')\n",
    "\n",
    "VGGnet_layers = ['block1_conv1',\n",
    "                 'block1_conv2',\n",
    "                 'block2_conv1',\n",
    "                 'block2_conv2',\n",
    "                 'block3_conv1',\n",
    "                 'block3_conv2',\n",
    "                 'block3_conv3',\n",
    "                 'block3_conv4',\n",
    "                 'block4_conv1',\n",
    "                 'block4_conv2',\n",
    "                 'block4_conv3',\n",
    "                 'block4_conv4']\n",
    "layer_names = [layer.name for layer in network.layers]\n",
    "\n",
    "for i in VGGnet_layers:\n",
    "    weight_names = f[i].attrs[\"weight_names\"]\n",
    "    weights = [f[i][j] for j in weight_names]    \n",
    "    index = layer_names.index(i)\n",
    "    network.layers[index].set_weights(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "opt = optimizers.rmsprop(lr=0.00001, decay=1e-6)\n",
    "network.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up image augmentation generator and trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "\n",
    "# create and configure augmented image generator\n",
    "datagen_train = ImageDataGenerator(\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (10% of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (10% of total height)\n",
    "    rotation_range=30, # randomly rotate images up to 30 degrees\n",
    "    zoom_range=0.2, # randomly zoom in and out by 10%\n",
    "    horizontal_flip=True) # randomly flip images horizontally\n",
    "\n",
    "# create and configure augmented image generator\n",
    "datagen_valid = ImageDataGenerator(\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (10% of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (10% of total height)\n",
    "    rotation_range=30, # randomly rotate images up to 30 degrees\n",
    "    zoom_range=0.2, # randomly zoom in and out by 10%\n",
    "    horizontal_flip=True) # randomly flip images horizontally\n",
    "\n",
    "# fit augmented image generator on data\n",
    "datagen_train.fit(train_tensors)\n",
    "datagen_valid.fit(valid_tensors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create callback to print recall and precision during training\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
    "        val_targ = self.validation_data[1]\n",
    "        _val_f1 = f1_score(val_targ, val_predict, average='weighted')\n",
    "        _val_recall = recall_score(val_targ, val_predict, average='weighted')\n",
    "        _val_precision = precision_score(val_targ, val_predict, average='weighted')\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        print(\"val_f1: %f — val_precision: %f — val_recall %f\"%(_val_f1, _val_precision, _val_recall))\n",
    "        return\n",
    "\n",
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 99/100 [============================>.] - ETA: 91s - loss: 5.6413 - acc: 0.65 - ETA: 62s - loss: 5.2384 - acc: 0.67 - ETA: 52s - loss: 5.1041 - acc: 0.68 - ETA: 47s - loss: 4.0295 - acc: 0.75 - ETA: 44s - loss: 4.5131 - acc: 0.72 - ETA: 42s - loss: 4.5668 - acc: 0.71 - ETA: 40s - loss: 4.8354 - acc: 0.70 - ETA: 39s - loss: 4.7347 - acc: 0.70 - ETA: 38s - loss: 5.0145 - acc: 0.68 - ETA: 37s - loss: 4.9160 - acc: 0.69 - ETA: 36s - loss: 4.9820 - acc: 0.69 - ETA: 35s - loss: 4.9697 - acc: 0.69 - ETA: 34s - loss: 5.0834 - acc: 0.68 - ETA: 34s - loss: 5.0657 - acc: 0.68 - ETA: 33s - loss: 5.0503 - acc: 0.68 - ETA: 32s - loss: 5.0873 - acc: 0.68 - ETA: 32s - loss: 5.0725 - acc: 0.68 - ETA: 31s - loss: 5.1488 - acc: 0.68 - ETA: 31s - loss: 5.0475 - acc: 0.68 - ETA: 30s - loss: 5.1981 - acc: 0.67 - ETA: 30s - loss: 5.2576 - acc: 0.67 - ETA: 29s - loss: 5.2750 - acc: 0.67 - ETA: 29s - loss: 5.2209 - acc: 0.67 - ETA: 28s - loss: 5.2720 - acc: 0.67 - ETA: 28s - loss: 5.2223 - acc: 0.67 - ETA: 28s - loss: 5.0524 - acc: 0.68 - ETA: 27s - loss: 5.0742 - acc: 0.68 - ETA: 27s - loss: 5.0081 - acc: 0.68 - ETA: 26s - loss: 4.9466 - acc: 0.69 - ETA: 26s - loss: 4.9429 - acc: 0.69 - ETA: 25s - loss: 4.8874 - acc: 0.69 - ETA: 25s - loss: 4.9110 - acc: 0.69 - ETA: 25s - loss: 4.9820 - acc: 0.69 - ETA: 24s - loss: 4.9302 - acc: 0.69 - ETA: 24s - loss: 4.9506 - acc: 0.69 - ETA: 23s - loss: 4.9250 - acc: 0.69 - ETA: 23s - loss: 5.0097 - acc: 0.68 - ETA: 23s - loss: 5.0263 - acc: 0.68 - ETA: 22s - loss: 5.0214 - acc: 0.68 - ETA: 22s - loss: 5.0772 - acc: 0.68 - ETA: 21s - loss: 5.0320 - acc: 0.68 - ETA: 21s - loss: 5.0273 - acc: 0.68 - ETA: 21s - loss: 4.9854 - acc: 0.69 - ETA: 20s - loss: 5.0003 - acc: 0.68 - ETA: 20s - loss: 5.0145 - acc: 0.68 - ETA: 19s - loss: 5.0281 - acc: 0.68 - ETA: 19s - loss: 4.9898 - acc: 0.69 - ETA: 19s - loss: 5.0537 - acc: 0.68 - ETA: 18s - loss: 5.0328 - acc: 0.68 - ETA: 18s - loss: 4.9805 - acc: 0.69 - ETA: 17s - loss: 4.9776 - acc: 0.69 - ETA: 17s - loss: 4.9749 - acc: 0.69 - ETA: 17s - loss: 4.9723 - acc: 0.69 - ETA: 16s - loss: 5.0444 - acc: 0.68 - ETA: 16s - loss: 5.0259 - acc: 0.68 - ETA: 16s - loss: 5.0657 - acc: 0.68 - ETA: 15s - loss: 5.0192 - acc: 0.68 - ETA: 15s - loss: 5.0022 - acc: 0.68 - ETA: 14s - loss: 4.9993 - acc: 0.68 - ETA: 14s - loss: 5.0100 - acc: 0.68 - ETA: 14s - loss: 4.9808 - acc: 0.69 - ETA: 13s - loss: 4.9654 - acc: 0.69 - ETA: 13s - loss: 4.9889 - acc: 0.69 - ETA: 13s - loss: 4.9865 - acc: 0.69 - ETA: 12s - loss: 5.0090 - acc: 0.68 - ETA: 12s - loss: 5.0430 - acc: 0.68 - ETA: 12s - loss: 5.0640 - acc: 0.68 - ETA: 11s - loss: 5.0606 - acc: 0.68 - ETA: 11s - loss: 5.1274 - acc: 0.68 - ETA: 10s - loss: 5.1463 - acc: 0.68 - ETA: 10s - loss: 5.1533 - acc: 0.68 - ETA: 10s - loss: 5.1264 - acc: 0.68 - ETA: 9s - loss: 5.1004 - acc: 0.6836 - ETA: 9s - loss: 5.1186 - acc: 0.682 - ETA: 9s - loss: 5.1578 - acc: 0.680 - ETA: 8s - loss: 5.1429 - acc: 0.680 - ETA: 8s - loss: 5.1390 - acc: 0.681 - ETA: 7s - loss: 5.1144 - acc: 0.682 - ETA: 7s - loss: 5.1211 - acc: 0.682 - ETA: 7s - loss: 5.1477 - acc: 0.680 - ETA: 6s - loss: 5.1140 - acc: 0.682 - ETA: 6s - loss: 5.1499 - acc: 0.680 - ETA: 6s - loss: 5.1364 - acc: 0.681 - ETA: 5s - loss: 5.1520 - acc: 0.680 - ETA: 5s - loss: 5.1578 - acc: 0.680 - ETA: 5s - loss: 5.1540 - acc: 0.680 - ETA: 4s - loss: 5.1133 - acc: 0.682 - ETA: 4s - loss: 5.1102 - acc: 0.683 - ETA: 3s - loss: 5.1252 - acc: 0.682 - ETA: 3s - loss: 5.1220 - acc: 0.682 - ETA: 3s - loss: 5.1011 - acc: 0.683 - ETA: 2s - loss: 5.0719 - acc: 0.685 - ETA: 2s - loss: 5.0434 - acc: 0.687 - ETA: 2s - loss: 5.0326 - acc: 0.687 - ETA: 1s - loss: 5.0221 - acc: 0.688 - ETA: 1s - loss: 5.0369 - acc: 0.687 - ETA: 1s - loss: 5.0348 - acc: 0.687 - ETA: 0s - loss: 5.0410 - acc: 0.687 - ETA: 0s - loss: 5.0715 - acc: 0.6854Epoch 00000: val_loss improved from inf to 7.71366, saving model to saved_models/best.weights.hdf5\n",
      "100/100 [==============================] - 40s - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7137 - val_acc: 0.5214\n",
      "Epoch 2/20\n",
      " 99/100 [============================>.] - ETA: 35s - loss: 4.8354 - acc: 0.70 - ETA: 35s - loss: 3.6266 - acc: 0.77 - ETA: 34s - loss: 3.4923 - acc: 0.78 - ETA: 34s - loss: 3.8280 - acc: 0.76 - ETA: 33s - loss: 4.6742 - acc: 0.71 - ETA: 33s - loss: 4.8354 - acc: 0.70 - ETA: 33s - loss: 4.7203 - acc: 0.70 - ETA: 32s - loss: 4.7347 - acc: 0.70 - ETA: 32s - loss: 4.5668 - acc: 0.71 - ETA: 32s - loss: 4.5131 - acc: 0.72 - ETA: 31s - loss: 4.4691 - acc: 0.72 - ETA: 31s - loss: 4.2310 - acc: 0.73 - ETA: 30s - loss: 4.2775 - acc: 0.73 - ETA: 30s - loss: 4.3173 - acc: 0.73 - ETA: 30s - loss: 4.5131 - acc: 0.72 - ETA: 29s - loss: 4.4828 - acc: 0.72 - ETA: 29s - loss: 4.5510 - acc: 0.71 - ETA: 29s - loss: 4.5220 - acc: 0.71 - ETA: 28s - loss: 4.6233 - acc: 0.71 - ETA: 28s - loss: 4.7548 - acc: 0.70 - ETA: 28s - loss: 4.9122 - acc: 0.69 - ETA: 27s - loss: 4.9087 - acc: 0.69 - ETA: 27s - loss: 4.9055 - acc: 0.69 - ETA: 27s - loss: 4.9362 - acc: 0.69 - ETA: 26s - loss: 4.8354 - acc: 0.70 - ETA: 26s - loss: 4.7734 - acc: 0.70 - ETA: 25s - loss: 4.8951 - acc: 0.69 - ETA: 25s - loss: 4.8066 - acc: 0.70 - ETA: 25s - loss: 4.8076 - acc: 0.70 - ETA: 24s - loss: 4.7280 - acc: 0.70 - ETA: 24s - loss: 4.6535 - acc: 0.71 - ETA: 24s - loss: 4.6088 - acc: 0.71 - ETA: 23s - loss: 4.6401 - acc: 0.71 - ETA: 23s - loss: 4.7169 - acc: 0.70 - ETA: 23s - loss: 4.6973 - acc: 0.70 - ETA: 22s - loss: 4.6563 - acc: 0.71 - ETA: 22s - loss: 4.6176 - acc: 0.71 - ETA: 22s - loss: 4.6446 - acc: 0.71 - ETA: 21s - loss: 4.6495 - acc: 0.71 - ETA: 21s - loss: 4.6944 - acc: 0.70 - ETA: 20s - loss: 4.7371 - acc: 0.70 - ETA: 20s - loss: 4.8162 - acc: 0.70 - ETA: 20s - loss: 4.8542 - acc: 0.69 - ETA: 19s - loss: 4.7988 - acc: 0.70 - ETA: 19s - loss: 4.7817 - acc: 0.70 - ETA: 19s - loss: 4.7654 - acc: 0.70 - ETA: 18s - loss: 4.7840 - acc: 0.70 - ETA: 18s - loss: 4.8018 - acc: 0.70 - ETA: 18s - loss: 4.7696 - acc: 0.70 - ETA: 17s - loss: 4.8032 - acc: 0.70 - ETA: 17s - loss: 4.8512 - acc: 0.69 - ETA: 17s - loss: 4.8974 - acc: 0.69 - ETA: 16s - loss: 4.9571 - acc: 0.69 - ETA: 16s - loss: 4.9996 - acc: 0.68 - ETA: 16s - loss: 4.9966 - acc: 0.69 - ETA: 15s - loss: 5.0225 - acc: 0.68 - ETA: 15s - loss: 5.0051 - acc: 0.68 - ETA: 14s - loss: 5.0439 - acc: 0.68 - ETA: 14s - loss: 5.0540 - acc: 0.68 - ETA: 14s - loss: 5.0772 - acc: 0.68 - ETA: 13s - loss: 5.0600 - acc: 0.68 - ETA: 13s - loss: 5.1214 - acc: 0.68 - ETA: 13s - loss: 5.1041 - acc: 0.68 - ETA: 12s - loss: 5.0747 - acc: 0.68 - ETA: 12s - loss: 5.0338 - acc: 0.68 - ETA: 12s - loss: 5.0064 - acc: 0.68 - ETA: 11s - loss: 5.0519 - acc: 0.68 - ETA: 11s - loss: 5.0725 - acc: 0.68 - ETA: 11s - loss: 5.0690 - acc: 0.68 - ETA: 10s - loss: 5.0887 - acc: 0.68 - ETA: 10s - loss: 5.1192 - acc: 0.68 - ETA: 9s - loss: 5.1264 - acc: 0.6819 - ETA: 9s - loss: 5.1666 - acc: 0.679 - ETA: 9s - loss: 5.1839 - acc: 0.678 - ETA: 8s - loss: 5.1470 - acc: 0.680 - ETA: 8s - loss: 5.1111 - acc: 0.682 - ETA: 8s - loss: 5.1285 - acc: 0.681 - ETA: 7s - loss: 5.1041 - acc: 0.683 - ETA: 7s - loss: 5.1211 - acc: 0.682 - ETA: 7s - loss: 5.1578 - acc: 0.680 - ETA: 6s - loss: 5.1538 - acc: 0.680 - ETA: 6s - loss: 5.1401 - acc: 0.681 - ETA: 6s - loss: 5.1364 - acc: 0.681 - ETA: 5s - loss: 5.1424 - acc: 0.681 - ETA: 5s - loss: 5.1104 - acc: 0.682 - ETA: 4s - loss: 5.0978 - acc: 0.683 - ETA: 4s - loss: 5.1041 - acc: 0.683 - ETA: 4s - loss: 5.0827 - acc: 0.684 - ETA: 3s - loss: 5.0980 - acc: 0.683 - ETA: 3s - loss: 5.0772 - acc: 0.685 - ETA: 3s - loss: 5.0923 - acc: 0.684 - ETA: 2s - loss: 5.0895 - acc: 0.684 - ETA: 2s - loss: 5.0954 - acc: 0.683 - ETA: 2s - loss: 5.1012 - acc: 0.683 - ETA: 1s - loss: 5.1239 - acc: 0.682 - ETA: 1s - loss: 5.0957 - acc: 0.683 - ETA: 1s - loss: 5.0764 - acc: 0.685 - ETA: 0s - loss: 5.0657 - acc: 0.685 - ETA: 0s - loss: 5.0715 - acc: 0.6854Epoch 00001: val_loss improved from 7.71366 to 7.31514, saving model to saved_models/best.weights.hdf5\n",
      "100/100 [==============================] - 39s - loss: 5.0611 - acc: 0.6860 - val_loss: 7.3151 - val_acc: 0.5462\n",
      "Epoch 3/20\n",
      " 99/100 [============================>.] - ETA: 35s - loss: 4.8354 - acc: 0.70 - ETA: 34s - loss: 4.8354 - acc: 0.70 - ETA: 34s - loss: 5.1041 - acc: 0.68 - ETA: 34s - loss: 5.2384 - acc: 0.67 - ETA: 33s - loss: 4.8354 - acc: 0.70 - ETA: 33s - loss: 4.7011 - acc: 0.70 - ETA: 33s - loss: 4.8354 - acc: 0.70 - ETA: 32s - loss: 4.9362 - acc: 0.69 - ETA: 32s - loss: 4.6563 - acc: 0.71 - ETA: 32s - loss: 4.5937 - acc: 0.71 - ETA: 31s - loss: 4.6156 - acc: 0.71 - ETA: 31s - loss: 4.5668 - acc: 0.71 - ETA: 30s - loss: 4.4635 - acc: 0.72 - ETA: 30s - loss: 4.5476 - acc: 0.71 - ETA: 30s - loss: 4.5668 - acc: 0.71 - ETA: 29s - loss: 4.5836 - acc: 0.71 - ETA: 29s - loss: 4.5036 - acc: 0.72 - ETA: 29s - loss: 4.5220 - acc: 0.71 - ETA: 28s - loss: 4.5385 - acc: 0.71 - ETA: 28s - loss: 4.6742 - acc: 0.71 - ETA: 28s - loss: 4.6819 - acc: 0.70 - ETA: 27s - loss: 4.7988 - acc: 0.70 - ETA: 27s - loss: 4.9055 - acc: 0.69 - ETA: 27s - loss: 4.8690 - acc: 0.69 - ETA: 26s - loss: 4.9966 - acc: 0.69 - ETA: 26s - loss: 5.0214 - acc: 0.68 - ETA: 26s - loss: 5.0145 - acc: 0.68 - ETA: 25s - loss: 4.9506 - acc: 0.69 - ETA: 25s - loss: 4.9188 - acc: 0.69 - ETA: 24s - loss: 4.9697 - acc: 0.69 - ETA: 24s - loss: 4.9134 - acc: 0.69 - ETA: 24s - loss: 4.9614 - acc: 0.69 - ETA: 23s - loss: 4.9087 - acc: 0.69 - ETA: 23s - loss: 4.8828 - acc: 0.69 - ETA: 23s - loss: 4.8585 - acc: 0.69 - ETA: 22s - loss: 4.8354 - acc: 0.70 - ETA: 22s - loss: 4.8136 - acc: 0.70 - ETA: 22s - loss: 4.8142 - acc: 0.70 - ETA: 21s - loss: 4.7734 - acc: 0.70 - ETA: 21s - loss: 4.8153 - acc: 0.70 - ETA: 21s - loss: 4.7568 - acc: 0.70 - ETA: 20s - loss: 4.7587 - acc: 0.70 - ETA: 20s - loss: 4.8167 - acc: 0.70 - ETA: 19s - loss: 4.8537 - acc: 0.69 - ETA: 19s - loss: 4.8533 - acc: 0.69 - ETA: 19s - loss: 4.8354 - acc: 0.70 - ETA: 18s - loss: 4.8697 - acc: 0.69 - ETA: 18s - loss: 4.8522 - acc: 0.69 - ETA: 18s - loss: 4.8025 - acc: 0.70 - ETA: 17s - loss: 4.7548 - acc: 0.70 - ETA: 17s - loss: 4.7248 - acc: 0.70 - ETA: 17s - loss: 4.6959 - acc: 0.70 - ETA: 16s - loss: 4.7290 - acc: 0.70 - ETA: 16s - loss: 4.7160 - acc: 0.70 - ETA: 16s - loss: 4.7475 - acc: 0.70 - ETA: 15s - loss: 4.7923 - acc: 0.70 - ETA: 15s - loss: 4.7930 - acc: 0.70 - ETA: 14s - loss: 4.8076 - acc: 0.70 - ETA: 14s - loss: 4.8354 - acc: 0.70 - ETA: 14s - loss: 4.8623 - acc: 0.69 - ETA: 13s - loss: 4.8883 - acc: 0.69 - ETA: 13s - loss: 4.8874 - acc: 0.69 - ETA: 13s - loss: 4.9122 - acc: 0.69 - ETA: 12s - loss: 4.9110 - acc: 0.69 - ETA: 12s - loss: 4.9470 - acc: 0.69 - ETA: 12s - loss: 4.9331 - acc: 0.69 - ETA: 12s - loss: 4.9437 - acc: 0.69 - ETA: 11s - loss: 4.9658 - acc: 0.69 - ETA: 11s - loss: 4.9522 - acc: 0.69 - ETA: 11s - loss: 4.9160 - acc: 0.69 - ETA: 10s - loss: 4.9149 - acc: 0.69 - ETA: 10s - loss: 4.9250 - acc: 0.69 - ETA: 9s - loss: 4.9348 - acc: 0.6938 - ETA: 9s - loss: 4.9117 - acc: 0.695 - ETA: 9s - loss: 4.8677 - acc: 0.698 - ETA: 8s - loss: 4.8672 - acc: 0.698 - ETA: 8s - loss: 4.8982 - acc: 0.696 - ETA: 8s - loss: 4.9078 - acc: 0.695 - ETA: 7s - loss: 4.9578 - acc: 0.692 - ETA: 7s - loss: 4.9765 - acc: 0.691 - ETA: 7s - loss: 4.9648 - acc: 0.692 - ETA: 6s - loss: 4.9632 - acc: 0.692 - ETA: 6s - loss: 4.9617 - acc: 0.692 - ETA: 5s - loss: 4.9602 - acc: 0.692 - ETA: 5s - loss: 4.9682 - acc: 0.691 - ETA: 5s - loss: 4.9760 - acc: 0.691 - ETA: 4s - loss: 4.9836 - acc: 0.690 - ETA: 4s - loss: 4.9911 - acc: 0.690 - ETA: 4s - loss: 4.9894 - acc: 0.690 - ETA: 3s - loss: 5.0145 - acc: 0.688 - ETA: 3s - loss: 5.0037 - acc: 0.689 - ETA: 3s - loss: 5.0194 - acc: 0.688 - ETA: 2s - loss: 5.0521 - acc: 0.686 - ETA: 2s - loss: 5.0755 - acc: 0.685 - ETA: 1s - loss: 5.0645 - acc: 0.685 - ETA: 1s - loss: 5.0453 - acc: 0.687 - ETA: 1s - loss: 5.0431 - acc: 0.687 - ETA: 0s - loss: 5.0657 - acc: 0.685 - ETA: 0s - loss: 5.0389 - acc: 0.6874Epoch 00002: val_loss did not improve\n",
      "100/100 [==============================] - 39s - loss: 5.0611 - acc: 0.6860 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 4/20\n",
      " 99/100 [============================>.] - ETA: 35s - loss: 4.8354 - acc: 0.70 - ETA: 34s - loss: 6.0443 - acc: 0.62 - ETA: 35s - loss: 5.3727 - acc: 0.66 - ETA: 34s - loss: 5.0369 - acc: 0.68 - ETA: 34s - loss: 5.4802 - acc: 0.66 - ETA: 33s - loss: 5.2384 - acc: 0.67 - ETA: 33s - loss: 5.0657 - acc: 0.68 - ETA: 32s - loss: 4.6340 - acc: 0.71 - ETA: 32s - loss: 4.4772 - acc: 0.72 - ETA: 32s - loss: 4.7548 - acc: 0.70 - ETA: 31s - loss: 4.8354 - acc: 0.70 - ETA: 31s - loss: 4.9697 - acc: 0.69 - ETA: 31s - loss: 4.8974 - acc: 0.69 - ETA: 30s - loss: 5.0081 - acc: 0.68 - ETA: 30s - loss: 5.0503 - acc: 0.68 - ETA: 30s - loss: 5.0873 - acc: 0.68 - ETA: 29s - loss: 5.1673 - acc: 0.67 - ETA: 29s - loss: 4.9697 - acc: 0.69 - ETA: 29s - loss: 5.1323 - acc: 0.68 - ETA: 28s - loss: 5.1175 - acc: 0.68 - ETA: 28s - loss: 5.0273 - acc: 0.68 - ETA: 28s - loss: 5.0552 - acc: 0.68 - ETA: 27s - loss: 5.1157 - acc: 0.68 - ETA: 27s - loss: 5.1376 - acc: 0.68 - ETA: 26s - loss: 5.2545 - acc: 0.67 - ETA: 26s - loss: 5.3004 - acc: 0.67 - ETA: 26s - loss: 5.2235 - acc: 0.67 - ETA: 25s - loss: 5.2384 - acc: 0.67 - ETA: 25s - loss: 5.2523 - acc: 0.67 - ETA: 25s - loss: 5.1041 - acc: 0.68 - ETA: 24s - loss: 5.0694 - acc: 0.68 - ETA: 24s - loss: 5.0873 - acc: 0.68 - ETA: 24s - loss: 5.1529 - acc: 0.68 - ETA: 23s - loss: 5.2147 - acc: 0.67 - ETA: 23s - loss: 5.1808 - acc: 0.67 - ETA: 22s - loss: 5.2384 - acc: 0.67 - ETA: 22s - loss: 5.1839 - acc: 0.67 - ETA: 22s - loss: 5.1323 - acc: 0.68 - ETA: 21s - loss: 5.1247 - acc: 0.68 - ETA: 21s - loss: 5.1175 - acc: 0.68 - ETA: 21s - loss: 5.1696 - acc: 0.67 - ETA: 20s - loss: 5.0849 - acc: 0.68 - ETA: 20s - loss: 5.1728 - acc: 0.67 - ETA: 20s - loss: 5.1834 - acc: 0.67 - ETA: 19s - loss: 5.2115 - acc: 0.67 - ETA: 19s - loss: 5.1683 - acc: 0.67 - ETA: 19s - loss: 5.1955 - acc: 0.67 - ETA: 18s - loss: 5.2048 - acc: 0.67 - ETA: 18s - loss: 5.1644 - acc: 0.67 - ETA: 17s - loss: 5.1578 - acc: 0.68 - ETA: 17s - loss: 5.1515 - acc: 0.68 - ETA: 17s - loss: 5.1764 - acc: 0.67 - ETA: 16s - loss: 5.1547 - acc: 0.68 - ETA: 16s - loss: 5.1936 - acc: 0.67 - ETA: 16s - loss: 5.1724 - acc: 0.67 - ETA: 15s - loss: 5.1520 - acc: 0.68 - ETA: 15s - loss: 5.1323 - acc: 0.68 - ETA: 15s - loss: 5.1411 - acc: 0.68 - ETA: 14s - loss: 5.0813 - acc: 0.68 - ETA: 14s - loss: 5.0638 - acc: 0.68 - ETA: 13s - loss: 5.0732 - acc: 0.68 - ETA: 13s - loss: 5.0434 - acc: 0.68 - ETA: 13s - loss: 5.0529 - acc: 0.68 - ETA: 12s - loss: 4.9991 - acc: 0.68 - ETA: 12s - loss: 5.0338 - acc: 0.68 - ETA: 12s - loss: 5.0308 - acc: 0.68 - ETA: 11s - loss: 5.0159 - acc: 0.68 - ETA: 11s - loss: 4.9895 - acc: 0.69 - ETA: 11s - loss: 5.0106 - acc: 0.68 - ETA: 10s - loss: 4.9966 - acc: 0.69 - ETA: 10s - loss: 5.0284 - acc: 0.68 - ETA: 10s - loss: 5.0033 - acc: 0.68 - ETA: 9s - loss: 4.9789 - acc: 0.6911 - ETA: 9s - loss: 4.9443 - acc: 0.693 - ETA: 8s - loss: 4.9429 - acc: 0.693 - ETA: 8s - loss: 4.9733 - acc: 0.691 - ETA: 8s - loss: 4.9924 - acc: 0.690 - ETA: 7s - loss: 5.0214 - acc: 0.688 - ETA: 7s - loss: 5.0089 - acc: 0.689 - ETA: 7s - loss: 4.9865 - acc: 0.690 - ETA: 6s - loss: 5.0046 - acc: 0.689 - ETA: 6s - loss: 4.9829 - acc: 0.690 - ETA: 6s - loss: 4.9811 - acc: 0.691 - ETA: 5s - loss: 4.9410 - acc: 0.693 - ETA: 5s - loss: 4.9682 - acc: 0.691 - ETA: 5s - loss: 4.9854 - acc: 0.690 - ETA: 4s - loss: 4.9744 - acc: 0.691 - ETA: 4s - loss: 5.0094 - acc: 0.689 - ETA: 3s - loss: 5.0165 - acc: 0.688 - ETA: 3s - loss: 4.9787 - acc: 0.691 - ETA: 3s - loss: 4.9860 - acc: 0.690 - ETA: 2s - loss: 5.0281 - acc: 0.688 - ETA: 2s - loss: 5.0087 - acc: 0.689 - ETA: 2s - loss: 5.0069 - acc: 0.689 - ETA: 1s - loss: 4.9966 - acc: 0.690 - ETA: 1s - loss: 5.0117 - acc: 0.689 - ETA: 1s - loss: 5.0182 - acc: 0.688 - ETA: 0s - loss: 5.0328 - acc: 0.687 - ETA: 0s - loss: 5.0552 - acc: 0.6864Epoch 00003: val_loss did not improve\n",
      "100/100 [==============================] - 37s - loss: 5.0611 - acc: 0.6860 - val_loss: 7.6871 - val_acc: 0.5231\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 34s - loss: 3.2236 - acc: 0.80 - ETA: 34s - loss: 3.2236 - acc: 0.80 - ETA: 34s - loss: 4.5668 - acc: 0.71 - ETA: 34s - loss: 5.2384 - acc: 0.67 - ETA: 34s - loss: 4.5131 - acc: 0.72 - ETA: 33s - loss: 4.5668 - acc: 0.71 - ETA: 33s - loss: 5.0657 - acc: 0.68 - ETA: 32s - loss: 4.9362 - acc: 0.69 - ETA: 32s - loss: 4.7459 - acc: 0.70 - ETA: 32s - loss: 4.8354 - acc: 0.70 - ETA: 31s - loss: 4.7622 - acc: 0.70 - ETA: 31s - loss: 4.7011 - acc: 0.70 - ETA: 31s - loss: 4.8354 - acc: 0.70 - ETA: 30s - loss: 4.8354 - acc: 0.70 - ETA: 30s - loss: 4.8354 - acc: 0.70 - ETA: 30s - loss: 4.9865 - acc: 0.69 - ETA: 29s - loss: 4.9776 - acc: 0.69 - ETA: 29s - loss: 4.7907 - acc: 0.70 - ETA: 28s - loss: 4.8778 - acc: 0.69 - ETA: 28s - loss: 4.8354 - acc: 0.70 - ETA: 28s - loss: 4.9122 - acc: 0.69 - ETA: 27s - loss: 4.8721 - acc: 0.69 - ETA: 27s - loss: 4.8004 - acc: 0.70 - ETA: 27s - loss: 4.8690 - acc: 0.69 - ETA: 26s - loss: 4.7387 - acc: 0.70 - ETA: 26s - loss: 4.7424 - acc: 0.70 - ETA: 26s - loss: 4.8354 - acc: 0.70 - ETA: 25s - loss: 4.8354 - acc: 0.70 - ETA: 25s - loss: 4.8910 - acc: 0.69 - ETA: 24s - loss: 4.9160 - acc: 0.69 - ETA: 24s - loss: 4.9654 - acc: 0.69 - ETA: 24s - loss: 4.9865 - acc: 0.69 - ETA: 23s - loss: 4.8843 - acc: 0.69 - ETA: 23s - loss: 4.7880 - acc: 0.70 - ETA: 23s - loss: 4.8354 - acc: 0.70 - ETA: 22s - loss: 4.8130 - acc: 0.70 - ETA: 22s - loss: 4.7265 - acc: 0.70 - ETA: 22s - loss: 4.6658 - acc: 0.71 - ETA: 21s - loss: 4.6908 - acc: 0.70 - ETA: 21s - loss: 4.6944 - acc: 0.70 - ETA: 21s - loss: 4.7568 - acc: 0.70 - ETA: 20s - loss: 4.7395 - acc: 0.70 - ETA: 20s - loss: 4.6668 - acc: 0.71 - ETA: 19s - loss: 4.7622 - acc: 0.70 - ETA: 19s - loss: 4.7101 - acc: 0.70 - ETA: 19s - loss: 4.7829 - acc: 0.70 - ETA: 18s - loss: 4.7497 - acc: 0.70 - ETA: 18s - loss: 4.7515 - acc: 0.70 - ETA: 18s - loss: 4.8190 - acc: 0.70 - ETA: 17s - loss: 4.8677 - acc: 0.69 - ETA: 17s - loss: 4.8196 - acc: 0.70 - ETA: 17s - loss: 4.8044 - acc: 0.70 - ETA: 16s - loss: 4.8354 - acc: 0.70 - ETA: 16s - loss: 4.8354 - acc: 0.70 - ETA: 16s - loss: 4.8061 - acc: 0.70 - ETA: 15s - loss: 4.8066 - acc: 0.70 - ETA: 15s - loss: 4.8354 - acc: 0.70 - ETA: 14s - loss: 4.7937 - acc: 0.70 - ETA: 14s - loss: 4.8081 - acc: 0.70 - ETA: 14s - loss: 4.8220 - acc: 0.70 - ETA: 13s - loss: 4.8222 - acc: 0.70 - ETA: 13s - loss: 4.8484 - acc: 0.69 - ETA: 13s - loss: 4.8226 - acc: 0.70 - ETA: 12s - loss: 4.8606 - acc: 0.69 - ETA: 12s - loss: 4.8354 - acc: 0.70 - ETA: 12s - loss: 4.8354 - acc: 0.70 - ETA: 11s - loss: 4.8835 - acc: 0.69 - ETA: 11s - loss: 4.9302 - acc: 0.69 - ETA: 11s - loss: 4.9055 - acc: 0.69 - ETA: 10s - loss: 4.9506 - acc: 0.69 - ETA: 10s - loss: 4.9716 - acc: 0.69 - ETA: 9s - loss: 4.9586 - acc: 0.6924 - ETA: 9s - loss: 4.9900 - acc: 0.690 - ETA: 9s - loss: 4.9988 - acc: 0.689 - ETA: 8s - loss: 5.0074 - acc: 0.689 - ETA: 8s - loss: 4.9945 - acc: 0.690 - ETA: 8s - loss: 4.9820 - acc: 0.690 - ETA: 7s - loss: 4.9904 - acc: 0.690 - ETA: 7s - loss: 4.9986 - acc: 0.689 - ETA: 7s - loss: 5.0268 - acc: 0.688 - ETA: 6s - loss: 5.0344 - acc: 0.687 - ETA: 6s - loss: 5.0222 - acc: 0.688 - ETA: 6s - loss: 5.0490 - acc: 0.686 - ETA: 5s - loss: 5.0561 - acc: 0.686 - ETA: 5s - loss: 5.0725 - acc: 0.685 - ETA: 4s - loss: 5.0697 - acc: 0.685 - ETA: 4s - loss: 5.0855 - acc: 0.684 - ETA: 4s - loss: 5.0919 - acc: 0.684 - ETA: 3s - loss: 5.0980 - acc: 0.683 - ETA: 3s - loss: 5.1309 - acc: 0.681 - ETA: 3s - loss: 5.1720 - acc: 0.679 - ETA: 2s - loss: 5.1508 - acc: 0.680 - ETA: 2s - loss: 5.1561 - acc: 0.680 - ETA: 2s - loss: 5.1355 - acc: 0.681 - ETA: 1s - loss: 5.1239 - acc: 0.682 - ETA: 1s - loss: 5.1209 - acc: 0.682 - ETA: 1s - loss: 5.1096 - acc: 0.683 - ETA: 0s - loss: 5.0904 - acc: 0.684 - ETA: 0s - loss: 5.0796 - acc: 0.6848Epoch 00004: val_loss did not improve\n",
      "100/100 [==============================] - 37s - loss: 5.0611 - acc: 0.6860 - val_loss: 8.3070 - val_acc: 0.4846\n",
      "Epoch 6/20\n",
      " 99/100 [============================>.] - ETA: 35s - loss: 6.4472 - acc: 0.60 - ETA: 34s - loss: 7.2531 - acc: 0.55 - ETA: 34s - loss: 6.7159 - acc: 0.58 - ETA: 34s - loss: 6.2458 - acc: 0.61 - ETA: 33s - loss: 6.2861 - acc: 0.61 - ETA: 33s - loss: 6.1786 - acc: 0.61 - ETA: 33s - loss: 5.9867 - acc: 0.62 - ETA: 32s - loss: 6.1450 - acc: 0.61 - ETA: 32s - loss: 5.8204 - acc: 0.63 - ETA: 32s - loss: 5.9637 - acc: 0.63 - ETA: 31s - loss: 5.9344 - acc: 0.63 - ETA: 31s - loss: 6.1786 - acc: 0.61 - ETA: 30s - loss: 6.1373 - acc: 0.61 - ETA: 30s - loss: 6.0443 - acc: 0.62 - ETA: 30s - loss: 5.9100 - acc: 0.63 - ETA: 29s - loss: 5.8428 - acc: 0.63 - ETA: 29s - loss: 5.6887 - acc: 0.64 - ETA: 29s - loss: 5.6413 - acc: 0.65 - ETA: 28s - loss: 5.5989 - acc: 0.65 - ETA: 28s - loss: 5.5204 - acc: 0.65 - ETA: 28s - loss: 5.4878 - acc: 0.65 - ETA: 27s - loss: 5.3849 - acc: 0.66 - ETA: 27s - loss: 5.2909 - acc: 0.67 - ETA: 27s - loss: 5.1712 - acc: 0.67 - ETA: 26s - loss: 5.1900 - acc: 0.67 - ETA: 26s - loss: 5.1454 - acc: 0.68 - ETA: 26s - loss: 5.0444 - acc: 0.68 - ETA: 25s - loss: 5.0657 - acc: 0.68 - ETA: 25s - loss: 5.1133 - acc: 0.68 - ETA: 25s - loss: 5.1847 - acc: 0.67 - ETA: 24s - loss: 5.1734 - acc: 0.67 - ETA: 24s - loss: 5.1628 - acc: 0.67 - ETA: 23s - loss: 5.2017 - acc: 0.67 - ETA: 23s - loss: 5.2147 - acc: 0.67 - ETA: 23s - loss: 5.1348 - acc: 0.68 - ETA: 22s - loss: 5.1488 - acc: 0.68 - ETA: 22s - loss: 5.1839 - acc: 0.67 - ETA: 22s - loss: 5.1323 - acc: 0.68 - ETA: 21s - loss: 5.2074 - acc: 0.67 - ETA: 21s - loss: 5.1779 - acc: 0.67 - ETA: 21s - loss: 5.1892 - acc: 0.67 - ETA: 20s - loss: 5.2768 - acc: 0.67 - ETA: 20s - loss: 5.3227 - acc: 0.66 - ETA: 20s - loss: 5.3300 - acc: 0.66 - ETA: 19s - loss: 5.3190 - acc: 0.67 - ETA: 19s - loss: 5.3260 - acc: 0.66 - ETA: 18s - loss: 5.3498 - acc: 0.66 - ETA: 18s - loss: 5.3559 - acc: 0.66 - ETA: 18s - loss: 5.3288 - acc: 0.66 - ETA: 17s - loss: 5.3029 - acc: 0.67 - ETA: 17s - loss: 5.2779 - acc: 0.67 - ETA: 17s - loss: 5.2539 - acc: 0.67 - ETA: 16s - loss: 5.2460 - acc: 0.67 - ETA: 16s - loss: 5.2682 - acc: 0.67 - ETA: 16s - loss: 5.2164 - acc: 0.67 - ETA: 15s - loss: 5.2240 - acc: 0.67 - ETA: 15s - loss: 5.2030 - acc: 0.67 - ETA: 15s - loss: 5.2245 - acc: 0.67 - ETA: 14s - loss: 5.2862 - acc: 0.67 - ETA: 14s - loss: 5.2249 - acc: 0.67 - ETA: 13s - loss: 5.1921 - acc: 0.67 - ETA: 13s - loss: 5.1604 - acc: 0.67 - ETA: 13s - loss: 5.1552 - acc: 0.68 - ETA: 12s - loss: 5.1502 - acc: 0.68 - ETA: 12s - loss: 5.1082 - acc: 0.68 - ETA: 12s - loss: 5.1041 - acc: 0.68 - ETA: 11s - loss: 5.0640 - acc: 0.68 - ETA: 11s - loss: 5.0606 - acc: 0.68 - ETA: 11s - loss: 5.0573 - acc: 0.68 - ETA: 10s - loss: 5.0772 - acc: 0.68 - ETA: 10s - loss: 5.0624 - acc: 0.68 - ETA: 10s - loss: 5.0593 - acc: 0.68 - ETA: 9s - loss: 5.0893 - acc: 0.6842 - ETA: 9s - loss: 5.0641 - acc: 0.685 - ETA: 8s - loss: 5.0718 - acc: 0.685 - ETA: 8s - loss: 5.0369 - acc: 0.687 - ETA: 8s - loss: 5.0343 - acc: 0.687 - ETA: 7s - loss: 5.0317 - acc: 0.687 - ETA: 7s - loss: 4.9986 - acc: 0.689 - ETA: 7s - loss: 4.9966 - acc: 0.690 - ETA: 6s - loss: 4.9946 - acc: 0.690 - ETA: 6s - loss: 5.0025 - acc: 0.689 - ETA: 6s - loss: 5.0005 - acc: 0.689 - ETA: 5s - loss: 5.0177 - acc: 0.688 - ETA: 5s - loss: 5.0345 - acc: 0.687 - ETA: 5s - loss: 5.0603 - acc: 0.686 - ETA: 4s - loss: 5.0485 - acc: 0.686 - ETA: 4s - loss: 5.0827 - acc: 0.684 - ETA: 3s - loss: 5.0890 - acc: 0.684 - ETA: 3s - loss: 5.0772 - acc: 0.685 - ETA: 3s - loss: 5.0834 - acc: 0.684 - ETA: 2s - loss: 5.0544 - acc: 0.686 - ETA: 2s - loss: 5.0434 - acc: 0.687 - ETA: 2s - loss: 5.0412 - acc: 0.687 - ETA: 1s - loss: 5.0390 - acc: 0.687 - ETA: 1s - loss: 5.0537 - acc: 0.686 - ETA: 1s - loss: 5.0431 - acc: 0.687 - ETA: 0s - loss: 5.0657 - acc: 0.685 - ETA: 0s - loss: 5.0471 - acc: 0.6869Epoch 00005: val_loss did not improve\n",
      "100/100 [==============================] - 37s - loss: 5.0611 - acc: 0.6860 - val_loss: 8.1830 - val_acc: 0.4923\n",
      "Epoch 7/20\n",
      " 99/100 [============================>.] - ETA: 35s - loss: 5.6413 - acc: 0.65 - ETA: 34s - loss: 5.2384 - acc: 0.67 - ETA: 34s - loss: 5.3727 - acc: 0.66 - ETA: 34s - loss: 5.2384 - acc: 0.67 - ETA: 33s - loss: 5.1578 - acc: 0.68 - ETA: 33s - loss: 5.1041 - acc: 0.68 - ETA: 33s - loss: 4.7203 - acc: 0.70 - ETA: 32s - loss: 5.0369 - acc: 0.68 - ETA: 32s - loss: 4.9250 - acc: 0.69 - ETA: 32s - loss: 4.7548 - acc: 0.70 - ETA: 31s - loss: 4.6889 - acc: 0.70 - ETA: 31s - loss: 4.7683 - acc: 0.70 - ETA: 31s - loss: 4.9594 - acc: 0.69 - ETA: 30s - loss: 5.0657 - acc: 0.68 - ETA: 30s - loss: 5.1041 - acc: 0.68 - ETA: 29s - loss: 5.2384 - acc: 0.67 - ETA: 29s - loss: 5.2147 - acc: 0.67 - ETA: 29s - loss: 5.1041 - acc: 0.68 - ETA: 28s - loss: 5.0051 - acc: 0.68 - ETA: 28s - loss: 4.9160 - acc: 0.69 - ETA: 28s - loss: 4.8738 - acc: 0.69 - ETA: 27s - loss: 4.9453 - acc: 0.69 - ETA: 27s - loss: 4.9756 - acc: 0.69 - ETA: 27s - loss: 5.0033 - acc: 0.68 - ETA: 26s - loss: 5.1578 - acc: 0.68 - ETA: 26s - loss: 5.1144 - acc: 0.68 - ETA: 26s - loss: 5.1638 - acc: 0.67 - ETA: 25s - loss: 5.1233 - acc: 0.68 - ETA: 25s - loss: 5.0577 - acc: 0.68 - ETA: 24s - loss: 5.0235 - acc: 0.68 - ETA: 24s - loss: 5.0174 - acc: 0.68 - ETA: 24s - loss: 5.0369 - acc: 0.68 - ETA: 23s - loss: 5.0064 - acc: 0.68 - ETA: 23s - loss: 5.0251 - acc: 0.68 - ETA: 23s - loss: 5.0887 - acc: 0.68 - ETA: 22s - loss: 5.1936 - acc: 0.67 - ETA: 22s - loss: 5.2275 - acc: 0.67 - ETA: 22s - loss: 5.1111 - acc: 0.68 - ETA: 21s - loss: 5.1661 - acc: 0.67 - ETA: 21s - loss: 5.1981 - acc: 0.67 - ETA: 21s - loss: 5.1892 - acc: 0.67 - ETA: 20s - loss: 5.1616 - acc: 0.67 - ETA: 20s - loss: 5.2478 - acc: 0.67 - ETA: 20s - loss: 5.2567 - acc: 0.67 - ETA: 19s - loss: 5.2115 - acc: 0.67 - ETA: 19s - loss: 5.2033 - acc: 0.67 - ETA: 18s - loss: 5.1784 - acc: 0.67 - ETA: 18s - loss: 5.1376 - acc: 0.68 - ETA: 18s - loss: 5.0986 - acc: 0.68 - ETA: 17s - loss: 5.1256 - acc: 0.68 - ETA: 17s - loss: 5.0883 - acc: 0.68 - ETA: 17s - loss: 5.0834 - acc: 0.68 - ETA: 16s - loss: 5.0331 - acc: 0.68 - ETA: 16s - loss: 5.0444 - acc: 0.68 - ETA: 16s - loss: 5.0406 - acc: 0.68 - ETA: 15s - loss: 5.0369 - acc: 0.68 - ETA: 15s - loss: 5.0475 - acc: 0.68 - ETA: 15s - loss: 5.0300 - acc: 0.68 - ETA: 14s - loss: 5.0676 - acc: 0.68 - ETA: 14s - loss: 5.0235 - acc: 0.68 - ETA: 13s - loss: 5.0336 - acc: 0.68 - ETA: 13s - loss: 5.0434 - acc: 0.68 - ETA: 13s - loss: 5.0529 - acc: 0.68 - ETA: 12s - loss: 5.0117 - acc: 0.68 - ETA: 12s - loss: 5.0338 - acc: 0.68 - ETA: 12s - loss: 4.9820 - acc: 0.69 - ETA: 11s - loss: 4.9918 - acc: 0.69 - ETA: 11s - loss: 4.9895 - acc: 0.69 - ETA: 11s - loss: 4.9873 - acc: 0.69 - ETA: 10s - loss: 4.9851 - acc: 0.69 - ETA: 10s - loss: 4.9716 - acc: 0.69 - ETA: 10s - loss: 4.9697 - acc: 0.69 - ETA: 9s - loss: 4.9679 - acc: 0.6918 - ETA: 9s - loss: 4.9552 - acc: 0.692 - ETA: 8s - loss: 4.9106 - acc: 0.695 - ETA: 8s - loss: 4.9203 - acc: 0.694 - ETA: 8s - loss: 4.9401 - acc: 0.693 - ETA: 7s - loss: 4.9387 - acc: 0.693 - ETA: 7s - loss: 4.9374 - acc: 0.693 - ETA: 7s - loss: 4.9059 - acc: 0.695 - ETA: 6s - loss: 4.9051 - acc: 0.695 - ETA: 6s - loss: 4.9042 - acc: 0.695 - ETA: 6s - loss: 4.9519 - acc: 0.692 - ETA: 5s - loss: 4.9793 - acc: 0.691 - ETA: 5s - loss: 4.9682 - acc: 0.691 - ETA: 5s - loss: 4.9947 - acc: 0.690 - ETA: 4s - loss: 4.9744 - acc: 0.691 - ETA: 4s - loss: 5.0186 - acc: 0.688 - ETA: 3s - loss: 5.0256 - acc: 0.688 - ETA: 3s - loss: 4.9966 - acc: 0.690 - ETA: 3s - loss: 4.9683 - acc: 0.691 - ETA: 2s - loss: 5.0194 - acc: 0.688 - ETA: 2s - loss: 5.0347 - acc: 0.687 - ETA: 2s - loss: 5.0155 - acc: 0.688 - ETA: 1s - loss: 5.0221 - acc: 0.688 - ETA: 1s - loss: 5.0369 - acc: 0.687 - ETA: 1s - loss: 5.0681 - acc: 0.685 - ETA: 0s - loss: 5.0575 - acc: 0.686 - ETA: 0s - loss: 5.0796 - acc: 0.6848Epoch 00006: val_loss improved from 7.31514 to 7.06716, saving model to saved_models/best.weights.hdf5\n",
      "100/100 [==============================] - 40s - loss: 5.0611 - acc: 0.6860 - val_loss: 7.0672 - val_acc: 0.5615\n",
      "Epoch 8/20\n",
      " 99/100 [============================>.] - ETA: 36s - loss: 3.2236 - acc: 0.80 - ETA: 35s - loss: 5.2384 - acc: 0.67 - ETA: 35s - loss: 5.3727 - acc: 0.66 - ETA: 34s - loss: 5.8428 - acc: 0.63 - ETA: 34s - loss: 5.8025 - acc: 0.64 - ETA: 34s - loss: 5.7757 - acc: 0.64 - ETA: 33s - loss: 5.9867 - acc: 0.62 - ETA: 33s - loss: 5.9435 - acc: 0.63 - ETA: 32s - loss: 5.8204 - acc: 0.63 - ETA: 32s - loss: 5.9637 - acc: 0.63 - ETA: 32s - loss: 5.9344 - acc: 0.63 - ETA: 31s - loss: 5.8428 - acc: 0.63 - ETA: 31s - loss: 5.8273 - acc: 0.63 - ETA: 31s - loss: 5.7565 - acc: 0.64 - ETA: 30s - loss: 5.9100 - acc: 0.63 - ETA: 30s - loss: 5.8428 - acc: 0.63 - ETA: 29s - loss: 5.6887 - acc: 0.64 - ETA: 29s - loss: 5.5518 - acc: 0.65 - ETA: 29s - loss: 5.5565 - acc: 0.65 - ETA: 28s - loss: 5.5607 - acc: 0.65 - ETA: 28s - loss: 5.4878 - acc: 0.65 - ETA: 28s - loss: 5.4215 - acc: 0.66 - ETA: 27s - loss: 5.4311 - acc: 0.66 - ETA: 27s - loss: 5.3055 - acc: 0.67 - ETA: 27s - loss: 5.2867 - acc: 0.67 - ETA: 26s - loss: 5.3004 - acc: 0.67 - ETA: 26s - loss: 5.4622 - acc: 0.66 - ETA: 25s - loss: 5.4686 - acc: 0.66 - ETA: 25s - loss: 5.4468 - acc: 0.66 - ETA: 25s - loss: 5.4802 - acc: 0.66 - ETA: 24s - loss: 5.5373 - acc: 0.65 - ETA: 24s - loss: 5.4650 - acc: 0.66 - ETA: 24s - loss: 5.3971 - acc: 0.66 - ETA: 23s - loss: 5.4991 - acc: 0.65 - ETA: 23s - loss: 5.4341 - acc: 0.66 - ETA: 23s - loss: 5.4175 - acc: 0.66 - ETA: 22s - loss: 5.4453 - acc: 0.66 - ETA: 22s - loss: 5.4080 - acc: 0.66 - ETA: 21s - loss: 5.3934 - acc: 0.66 - ETA: 21s - loss: 5.3996 - acc: 0.66 - ETA: 21s - loss: 5.3465 - acc: 0.66 - ETA: 20s - loss: 5.4303 - acc: 0.66 - ETA: 20s - loss: 5.3789 - acc: 0.66 - ETA: 20s - loss: 5.3666 - acc: 0.66 - ETA: 19s - loss: 5.3369 - acc: 0.66 - ETA: 19s - loss: 5.3610 - acc: 0.66 - ETA: 19s - loss: 5.3155 - acc: 0.67 - ETA: 18s - loss: 5.3727 - acc: 0.66 - ETA: 18s - loss: 5.4275 - acc: 0.66 - ETA: 17s - loss: 5.4157 - acc: 0.66 - ETA: 17s - loss: 5.3885 - acc: 0.66 - ETA: 17s - loss: 5.3779 - acc: 0.66 - ETA: 16s - loss: 5.3828 - acc: 0.66 - ETA: 16s - loss: 5.3876 - acc: 0.66 - ETA: 16s - loss: 5.4069 - acc: 0.66 - ETA: 15s - loss: 5.4111 - acc: 0.66 - ETA: 15s - loss: 5.4858 - acc: 0.65 - ETA: 15s - loss: 5.4468 - acc: 0.66 - ETA: 14s - loss: 5.4091 - acc: 0.66 - ETA: 14s - loss: 5.3861 - acc: 0.66 - ETA: 14s - loss: 5.3507 - acc: 0.66 - ETA: 13s - loss: 5.3684 - acc: 0.66 - ETA: 13s - loss: 5.3599 - acc: 0.66 - ETA: 12s - loss: 5.3391 - acc: 0.66 - ETA: 12s - loss: 5.3190 - acc: 0.67 - ETA: 12s - loss: 5.3239 - acc: 0.66 - ETA: 11s - loss: 5.2925 - acc: 0.67 - ETA: 11s - loss: 5.2739 - acc: 0.67 - ETA: 11s - loss: 5.2442 - acc: 0.67 - ETA: 10s - loss: 5.2384 - acc: 0.67 - ETA: 10s - loss: 5.2100 - acc: 0.67 - ETA: 10s - loss: 5.2496 - acc: 0.67 - ETA: 9s - loss: 5.2439 - acc: 0.6747 - ETA: 9s - loss: 5.2275 - acc: 0.675 - ETA: 8s - loss: 5.2223 - acc: 0.676 - ETA: 8s - loss: 5.2066 - acc: 0.677 - ETA: 8s - loss: 5.2122 - acc: 0.676 - ETA: 7s - loss: 5.1971 - acc: 0.677 - ETA: 7s - loss: 5.1925 - acc: 0.677 - ETA: 7s - loss: 5.2082 - acc: 0.676 - ETA: 7s - loss: 5.2036 - acc: 0.677 - ETA: 6s - loss: 5.1991 - acc: 0.677 - ETA: 6s - loss: 5.1850 - acc: 0.678 - ETA: 5s - loss: 5.1904 - acc: 0.678 - ETA: 5s - loss: 5.2052 - acc: 0.677 - ETA: 5s - loss: 5.2009 - acc: 0.677 - ETA: 4s - loss: 5.1967 - acc: 0.677 - ETA: 4s - loss: 5.1834 - acc: 0.678 - ETA: 4s - loss: 5.1252 - acc: 0.682 - ETA: 3s - loss: 5.1041 - acc: 0.683 - ETA: 3s - loss: 5.0923 - acc: 0.684 - ETA: 2s - loss: 5.0982 - acc: 0.683 - ETA: 2s - loss: 5.0867 - acc: 0.684 - ETA: 2s - loss: 5.0926 - acc: 0.684 - ETA: 1s - loss: 5.0899 - acc: 0.684 - ETA: 1s - loss: 5.1041 - acc: 0.683 - ETA: 1s - loss: 5.1013 - acc: 0.683 - ETA: 0s - loss: 5.0904 - acc: 0.684 - ETA: 0s - loss: 5.0796 - acc: 0.6848Epoch 00007: val_loss did not improve\n",
      "100/100 [==============================] - 38s - loss: 5.0611 - acc: 0.6860 - val_loss: 7.8288 - val_acc: 0.5143\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 35s - loss: 4.0295 - acc: 0.75 - ETA: 34s - loss: 5.6413 - acc: 0.65 - ETA: 34s - loss: 5.3727 - acc: 0.66 - ETA: 34s - loss: 5.6413 - acc: 0.65 - ETA: 34s - loss: 4.8354 - acc: 0.70 - ETA: 33s - loss: 4.7011 - acc: 0.70 - ETA: 33s - loss: 5.4111 - acc: 0.66 - ETA: 32s - loss: 5.4399 - acc: 0.66 - ETA: 32s - loss: 5.2832 - acc: 0.67 - ETA: 32s - loss: 5.3996 - acc: 0.66 - ETA: 31s - loss: 5.3483 - acc: 0.66 - ETA: 31s - loss: 5.3727 - acc: 0.66 - ETA: 31s - loss: 5.2694 - acc: 0.67 - ETA: 30s - loss: 5.1808 - acc: 0.67 - ETA: 30s - loss: 5.1578 - acc: 0.68 - ETA: 30s - loss: 5.2384 - acc: 0.67 - ETA: 29s - loss: 5.2621 - acc: 0.67 - ETA: 29s - loss: 5.2832 - acc: 0.67 - ETA: 29s - loss: 5.2172 - acc: 0.67 - ETA: 28s - loss: 5.2384 - acc: 0.67 - ETA: 28s - loss: 5.3343 - acc: 0.66 - ETA: 28s - loss: 5.3483 - acc: 0.66 - ETA: 27s - loss: 5.3961 - acc: 0.66 - ETA: 27s - loss: 5.4063 - acc: 0.66 - ETA: 26s - loss: 5.3834 - acc: 0.66 - ETA: 26s - loss: 5.3314 - acc: 0.66 - ETA: 26s - loss: 5.3130 - acc: 0.67 - ETA: 25s - loss: 5.3535 - acc: 0.66 - ETA: 25s - loss: 5.3356 - acc: 0.66 - ETA: 25s - loss: 5.3458 - acc: 0.66 - ETA: 24s - loss: 5.3814 - acc: 0.66 - ETA: 24s - loss: 5.3391 - acc: 0.66 - ETA: 24s - loss: 5.3483 - acc: 0.66 - ETA: 23s - loss: 5.3569 - acc: 0.66 - ETA: 23s - loss: 5.3650 - acc: 0.66 - ETA: 23s - loss: 5.2608 - acc: 0.67 - ETA: 22s - loss: 5.2275 - acc: 0.67 - ETA: 22s - loss: 5.2384 - acc: 0.67 - ETA: 21s - loss: 5.2074 - acc: 0.67 - ETA: 21s - loss: 5.2182 - acc: 0.67 - ETA: 21s - loss: 5.2089 - acc: 0.67 - ETA: 20s - loss: 5.2000 - acc: 0.67 - ETA: 20s - loss: 5.1540 - acc: 0.68 - ETA: 20s - loss: 5.1285 - acc: 0.68 - ETA: 19s - loss: 5.1399 - acc: 0.68 - ETA: 19s - loss: 5.1683 - acc: 0.67 - ETA: 19s - loss: 5.1441 - acc: 0.68 - ETA: 18s - loss: 5.1880 - acc: 0.67 - ETA: 18s - loss: 5.2137 - acc: 0.67 - ETA: 17s - loss: 5.1578 - acc: 0.68 - ETA: 17s - loss: 5.1989 - acc: 0.67 - ETA: 17s - loss: 5.2074 - acc: 0.67 - ETA: 16s - loss: 5.1395 - acc: 0.68 - ETA: 16s - loss: 5.1787 - acc: 0.67 - ETA: 16s - loss: 5.1578 - acc: 0.68 - ETA: 15s - loss: 5.1664 - acc: 0.67 - ETA: 15s - loss: 5.1748 - acc: 0.67 - ETA: 15s - loss: 5.1272 - acc: 0.68 - ETA: 14s - loss: 5.1359 - acc: 0.68 - ETA: 14s - loss: 5.1175 - acc: 0.68 - ETA: 14s - loss: 5.0732 - acc: 0.68 - ETA: 13s - loss: 5.0694 - acc: 0.68 - ETA: 13s - loss: 5.0657 - acc: 0.68 - ETA: 12s - loss: 5.0873 - acc: 0.68 - ETA: 12s - loss: 5.0834 - acc: 0.68 - ETA: 12s - loss: 5.0430 - acc: 0.68 - ETA: 12s - loss: 5.0880 - acc: 0.68 - ETA: 11s - loss: 5.0725 - acc: 0.68 - ETA: 11s - loss: 5.0573 - acc: 0.68 - ETA: 11s - loss: 5.1002 - acc: 0.68 - ETA: 10s - loss: 5.0738 - acc: 0.68 - ETA: 10s - loss: 5.0929 - acc: 0.68 - ETA: 10s - loss: 5.1004 - acc: 0.68 - ETA: 9s - loss: 5.1186 - acc: 0.6824 - ETA: 9s - loss: 5.1041 - acc: 0.683 - ETA: 8s - loss: 5.1111 - acc: 0.682 - ETA: 8s - loss: 5.1285 - acc: 0.681 - ETA: 8s - loss: 5.1351 - acc: 0.681 - ETA: 7s - loss: 5.1415 - acc: 0.681 - ETA: 7s - loss: 5.1477 - acc: 0.680 - ETA: 7s - loss: 5.1538 - acc: 0.680 - ETA: 6s - loss: 5.1598 - acc: 0.679 - ETA: 6s - loss: 5.1558 - acc: 0.680 - ETA: 5s - loss: 5.1520 - acc: 0.680 - ETA: 5s - loss: 5.1104 - acc: 0.682 - ETA: 5s - loss: 5.1259 - acc: 0.682 - ETA: 4s - loss: 5.1319 - acc: 0.681 - ETA: 4s - loss: 5.1102 - acc: 0.683 - ETA: 4s - loss: 5.0890 - acc: 0.684 - ETA: 3s - loss: 5.0951 - acc: 0.683 - ETA: 3s - loss: 5.1011 - acc: 0.683 - ETA: 2s - loss: 5.0895 - acc: 0.684 - ETA: 2s - loss: 5.0867 - acc: 0.684 - ETA: 2s - loss: 5.0669 - acc: 0.685 - ETA: 1s - loss: 5.0475 - acc: 0.686 - ETA: 1s - loss: 5.0621 - acc: 0.685 - ETA: 1s - loss: 5.0431 - acc: 0.687 - ETA: 0s - loss: 5.0328 - acc: 0.687 - ETA: 0s - loss: 5.0471 - acc: 0.6869Epoch 00008: val_loss did not improve\n",
      "100/100 [==============================] - 38s - loss: 5.0611 - acc: 0.6860 - val_loss: 7.3151 - val_acc: 0.5462\n",
      "Epoch 10/20\n",
      " 99/100 [============================>.] - ETA: 34s - loss: 1.6118 - acc: 0.90 - ETA: 34s - loss: 2.4177 - acc: 0.85 - ETA: 34s - loss: 4.5668 - acc: 0.71 - ETA: 34s - loss: 4.2310 - acc: 0.73 - ETA: 33s - loss: 4.6742 - acc: 0.71 - ETA: 33s - loss: 4.7011 - acc: 0.70 - ETA: 33s - loss: 4.9506 - acc: 0.69 - ETA: 32s - loss: 4.7347 - acc: 0.70 - ETA: 32s - loss: 4.8354 - acc: 0.70 - ETA: 32s - loss: 4.5937 - acc: 0.71 - ETA: 31s - loss: 4.2493 - acc: 0.73 - ETA: 31s - loss: 4.1638 - acc: 0.74 - ETA: 31s - loss: 4.1535 - acc: 0.74 - ETA: 30s - loss: 4.0871 - acc: 0.74 - ETA: 30s - loss: 4.0833 - acc: 0.74 - ETA: 29s - loss: 4.0799 - acc: 0.74 - ETA: 29s - loss: 4.2666 - acc: 0.73 - ETA: 29s - loss: 4.2982 - acc: 0.73 - ETA: 28s - loss: 4.2840 - acc: 0.73 - ETA: 28s - loss: 4.3519 - acc: 0.73 - ETA: 28s - loss: 4.3749 - acc: 0.72 - ETA: 27s - loss: 4.4691 - acc: 0.72 - ETA: 27s - loss: 4.5201 - acc: 0.71 - ETA: 27s - loss: 4.5332 - acc: 0.71 - ETA: 26s - loss: 4.5131 - acc: 0.72 - ETA: 26s - loss: 4.6185 - acc: 0.71 - ETA: 26s - loss: 4.6862 - acc: 0.70 - ETA: 25s - loss: 4.6340 - acc: 0.71 - ETA: 25s - loss: 4.6409 - acc: 0.71 - ETA: 25s - loss: 4.6742 - acc: 0.71 - ETA: 24s - loss: 4.7314 - acc: 0.70 - ETA: 24s - loss: 4.7095 - acc: 0.70 - ETA: 23s - loss: 4.7866 - acc: 0.70 - ETA: 23s - loss: 4.7406 - acc: 0.70 - ETA: 23s - loss: 4.6973 - acc: 0.70 - ETA: 22s - loss: 4.8354 - acc: 0.70 - ETA: 22s - loss: 4.9443 - acc: 0.69 - ETA: 22s - loss: 4.9839 - acc: 0.69 - ETA: 21s - loss: 4.9801 - acc: 0.69 - ETA: 21s - loss: 4.9563 - acc: 0.69 - ETA: 21s - loss: 4.9337 - acc: 0.69 - ETA: 20s - loss: 5.0081 - acc: 0.68 - ETA: 20s - loss: 4.9666 - acc: 0.69 - ETA: 20s - loss: 4.9270 - acc: 0.69 - ETA: 19s - loss: 4.9071 - acc: 0.69 - ETA: 19s - loss: 4.9405 - acc: 0.69 - ETA: 19s - loss: 4.9555 - acc: 0.69 - ETA: 18s - loss: 4.9865 - acc: 0.69 - ETA: 18s - loss: 5.0328 - acc: 0.68 - ETA: 18s - loss: 4.9966 - acc: 0.69 - ETA: 17s - loss: 4.9934 - acc: 0.69 - ETA: 17s - loss: 4.9594 - acc: 0.69 - ETA: 16s - loss: 4.9875 - acc: 0.69 - ETA: 16s - loss: 5.0294 - acc: 0.68 - ETA: 16s - loss: 5.0699 - acc: 0.68 - ETA: 15s - loss: 5.0801 - acc: 0.68 - ETA: 15s - loss: 5.1182 - acc: 0.68 - ETA: 15s - loss: 5.0855 - acc: 0.68 - ETA: 14s - loss: 5.0540 - acc: 0.68 - ETA: 14s - loss: 5.0369 - acc: 0.68 - ETA: 14s - loss: 5.0336 - acc: 0.68 - ETA: 13s - loss: 5.0174 - acc: 0.68 - ETA: 13s - loss: 5.0145 - acc: 0.68 - ETA: 12s - loss: 5.0117 - acc: 0.68 - ETA: 12s - loss: 5.0586 - acc: 0.68 - ETA: 12s - loss: 5.0674 - acc: 0.68 - ETA: 11s - loss: 5.0519 - acc: 0.68 - ETA: 11s - loss: 5.0606 - acc: 0.68 - ETA: 11s - loss: 5.0340 - acc: 0.68 - ETA: 10s - loss: 5.0427 - acc: 0.68 - ETA: 10s - loss: 5.0397 - acc: 0.68 - ETA: 10s - loss: 5.0033 - acc: 0.68 - ETA: 9s - loss: 5.0231 - acc: 0.6884 - ETA: 9s - loss: 5.0097 - acc: 0.689 - ETA: 9s - loss: 5.0181 - acc: 0.688 - ETA: 8s - loss: 5.0157 - acc: 0.688 - ETA: 8s - loss: 4.9924 - acc: 0.690 - ETA: 7s - loss: 5.0317 - acc: 0.687 - ETA: 7s - loss: 5.0293 - acc: 0.688 - ETA: 7s - loss: 5.0369 - acc: 0.687 - ETA: 6s - loss: 5.0344 - acc: 0.687 - ETA: 6s - loss: 5.0713 - acc: 0.685 - ETA: 6s - loss: 5.0879 - acc: 0.684 - ETA: 5s - loss: 5.0849 - acc: 0.684 - ETA: 5s - loss: 5.1104 - acc: 0.682 - ETA: 5s - loss: 5.0884 - acc: 0.684 - ETA: 4s - loss: 5.0763 - acc: 0.685 - ETA: 4s - loss: 5.0827 - acc: 0.684 - ETA: 3s - loss: 5.1071 - acc: 0.683 - ETA: 3s - loss: 5.1041 - acc: 0.683 - ETA: 3s - loss: 5.1188 - acc: 0.682 - ETA: 2s - loss: 5.0807 - acc: 0.684 - ETA: 2s - loss: 5.0694 - acc: 0.685 - ETA: 2s - loss: 5.0498 - acc: 0.686 - ETA: 1s - loss: 5.0560 - acc: 0.686 - ETA: 1s - loss: 5.0621 - acc: 0.685 - ETA: 1s - loss: 5.0681 - acc: 0.685 - ETA: 0s - loss: 5.0657 - acc: 0.685 - ETA: 0s - loss: 5.0715 - acc: 0.6854Epoch 00009: val_loss did not improve\n",
      "100/100 [==============================] - 37s - loss: 5.0611 - acc: 0.6860 - val_loss: 7.3151 - val_acc: 0.5462\n",
      "Epoch 11/20\n",
      " 99/100 [============================>.] - ETA: 35s - loss: 3.2236 - acc: 0.80 - ETA: 35s - loss: 5.6413 - acc: 0.65 - ETA: 34s - loss: 6.4472 - acc: 0.60 - ETA: 34s - loss: 7.0517 - acc: 0.56 - ETA: 34s - loss: 6.4472 - acc: 0.60 - ETA: 33s - loss: 6.1786 - acc: 0.61 - ETA: 33s - loss: 5.9867 - acc: 0.62 - ETA: 33s - loss: 6.4472 - acc: 0.60 - ETA: 32s - loss: 6.0891 - acc: 0.62 - ETA: 32s - loss: 5.9637 - acc: 0.63 - ETA: 32s - loss: 5.7146 - acc: 0.64 - ETA: 31s - loss: 5.4399 - acc: 0.66 - ETA: 31s - loss: 5.4554 - acc: 0.66 - ETA: 31s - loss: 5.3535 - acc: 0.66 - ETA: 30s - loss: 5.4264 - acc: 0.66 - ETA: 30s - loss: 5.3391 - acc: 0.66 - ETA: 29s - loss: 5.3569 - acc: 0.66 - ETA: 29s - loss: 5.2832 - acc: 0.67 - ETA: 29s - loss: 5.4293 - acc: 0.66 - ETA: 28s - loss: 5.3190 - acc: 0.67 - ETA: 28s - loss: 5.2959 - acc: 0.67 - ETA: 28s - loss: 5.2384 - acc: 0.67 - ETA: 27s - loss: 5.0807 - acc: 0.68 - ETA: 27s - loss: 5.1376 - acc: 0.68 - ETA: 27s - loss: 5.0611 - acc: 0.68 - ETA: 26s - loss: 4.9904 - acc: 0.69 - ETA: 26s - loss: 5.0742 - acc: 0.68 - ETA: 25s - loss: 5.0081 - acc: 0.68 - ETA: 25s - loss: 5.0022 - acc: 0.68 - ETA: 25s - loss: 5.1041 - acc: 0.68 - ETA: 24s - loss: 5.1214 - acc: 0.68 - ETA: 24s - loss: 5.1125 - acc: 0.68 - ETA: 24s - loss: 5.0308 - acc: 0.68 - ETA: 23s - loss: 5.0251 - acc: 0.68 - ETA: 23s - loss: 5.0657 - acc: 0.68 - ETA: 23s - loss: 5.1041 - acc: 0.68 - ETA: 22s - loss: 5.0968 - acc: 0.68 - ETA: 22s - loss: 5.0899 - acc: 0.68 - ETA: 21s - loss: 5.0007 - acc: 0.68 - ETA: 21s - loss: 5.0571 - acc: 0.68 - ETA: 21s - loss: 5.0320 - acc: 0.68 - ETA: 20s - loss: 4.9889 - acc: 0.69 - ETA: 20s - loss: 5.0228 - acc: 0.68 - ETA: 20s - loss: 5.0186 - acc: 0.68 - ETA: 19s - loss: 4.9966 - acc: 0.69 - ETA: 19s - loss: 4.9931 - acc: 0.69 - ETA: 19s - loss: 5.0412 - acc: 0.68 - ETA: 18s - loss: 5.0033 - acc: 0.68 - ETA: 18s - loss: 4.9999 - acc: 0.68 - ETA: 17s - loss: 4.9805 - acc: 0.69 - ETA: 17s - loss: 4.9460 - acc: 0.69 - ETA: 17s - loss: 4.9439 - acc: 0.69 - ETA: 16s - loss: 4.9875 - acc: 0.69 - ETA: 16s - loss: 5.0294 - acc: 0.68 - ETA: 16s - loss: 5.0259 - acc: 0.68 - ETA: 15s - loss: 5.1089 - acc: 0.68 - ETA: 15s - loss: 5.1323 - acc: 0.68 - ETA: 15s - loss: 5.1272 - acc: 0.68 - ETA: 14s - loss: 5.0950 - acc: 0.68 - ETA: 14s - loss: 5.0772 - acc: 0.68 - ETA: 13s - loss: 5.0997 - acc: 0.68 - ETA: 13s - loss: 5.1214 - acc: 0.68 - ETA: 13s - loss: 5.1296 - acc: 0.68 - ETA: 12s - loss: 5.1251 - acc: 0.68 - ETA: 12s - loss: 5.1082 - acc: 0.68 - ETA: 12s - loss: 5.0674 - acc: 0.68 - ETA: 11s - loss: 5.0760 - acc: 0.68 - ETA: 11s - loss: 5.0606 - acc: 0.68 - ETA: 11s - loss: 5.0223 - acc: 0.68 - ETA: 10s - loss: 4.9966 - acc: 0.69 - ETA: 10s - loss: 4.9716 - acc: 0.69 - ETA: 10s - loss: 5.0033 - acc: 0.68 - ETA: 9s - loss: 5.0341 - acc: 0.6877 - ETA: 9s - loss: 5.0315 - acc: 0.687 - ETA: 8s - loss: 5.0181 - acc: 0.688 - ETA: 8s - loss: 5.0157 - acc: 0.688 - ETA: 8s - loss: 5.0238 - acc: 0.688 - ETA: 7s - loss: 5.0524 - acc: 0.686 - ETA: 7s - loss: 5.0395 - acc: 0.687 - ETA: 7s - loss: 5.0470 - acc: 0.686 - ETA: 6s - loss: 5.0245 - acc: 0.688 - ETA: 6s - loss: 5.0418 - acc: 0.687 - ETA: 6s - loss: 5.0296 - acc: 0.688 - ETA: 5s - loss: 5.0465 - acc: 0.686 - ETA: 5s - loss: 5.0535 - acc: 0.686 - ETA: 5s - loss: 5.0322 - acc: 0.687 - ETA: 4s - loss: 5.0300 - acc: 0.687 - ETA: 4s - loss: 5.0552 - acc: 0.686 - ETA: 3s - loss: 5.0528 - acc: 0.686 - ETA: 3s - loss: 5.0593 - acc: 0.686 - ETA: 3s - loss: 5.0391 - acc: 0.687 - ETA: 2s - loss: 5.0369 - acc: 0.687 - ETA: 2s - loss: 5.0521 - acc: 0.686 - ETA: 2s - loss: 5.0412 - acc: 0.687 - ETA: 1s - loss: 5.0645 - acc: 0.685 - ETA: 1s - loss: 5.0705 - acc: 0.685 - ETA: 1s - loss: 5.0681 - acc: 0.685 - ETA: 0s - loss: 5.0821 - acc: 0.684 - ETA: 0s - loss: 5.0715 - acc: 0.6854Epoch 00010: val_loss did not improve\n",
      "100/100 [==============================] - 37s - loss: 5.0611 - acc: 0.6860 - val_loss: 7.5631 - val_acc: 0.5308\n",
      "Epoch 12/20\n",
      " 99/100 [============================>.] - ETA: 34s - loss: 6.4472 - acc: 0.60 - ETA: 34s - loss: 6.0443 - acc: 0.62 - ETA: 34s - loss: 5.6413 - acc: 0.65 - ETA: 34s - loss: 5.0369 - acc: 0.68 - ETA: 33s - loss: 4.8354 - acc: 0.70 - ETA: 33s - loss: 4.8354 - acc: 0.70 - ETA: 32s - loss: 4.9506 - acc: 0.69 - ETA: 32s - loss: 4.7347 - acc: 0.70 - ETA: 32s - loss: 5.0145 - acc: 0.68 - ETA: 31s - loss: 5.2384 - acc: 0.67 - ETA: 31s - loss: 5.1285 - acc: 0.68 - ETA: 31s - loss: 5.1712 - acc: 0.67 - ETA: 30s - loss: 5.0214 - acc: 0.68 - ETA: 30s - loss: 5.0657 - acc: 0.68 - ETA: 30s - loss: 4.9966 - acc: 0.69 - ETA: 29s - loss: 4.9362 - acc: 0.69 - ETA: 29s - loss: 4.9302 - acc: 0.69 - ETA: 29s - loss: 4.9250 - acc: 0.69 - ETA: 28s - loss: 4.8354 - acc: 0.70 - ETA: 28s - loss: 4.9966 - acc: 0.69 - ETA: 28s - loss: 4.9506 - acc: 0.69 - ETA: 27s - loss: 4.9820 - acc: 0.69 - ETA: 27s - loss: 5.1508 - acc: 0.68 - ETA: 26s - loss: 5.1041 - acc: 0.68 - ETA: 26s - loss: 5.0288 - acc: 0.68 - ETA: 26s - loss: 5.1454 - acc: 0.68 - ETA: 25s - loss: 5.0444 - acc: 0.68 - ETA: 25s - loss: 5.0945 - acc: 0.68 - ETA: 25s - loss: 5.1133 - acc: 0.68 - ETA: 24s - loss: 5.1578 - acc: 0.68 - ETA: 24s - loss: 5.0954 - acc: 0.68 - ETA: 24s - loss: 5.0621 - acc: 0.68 - ETA: 23s - loss: 5.1285 - acc: 0.68 - ETA: 23s - loss: 5.1199 - acc: 0.68 - ETA: 23s - loss: 5.0657 - acc: 0.68 - ETA: 22s - loss: 5.0369 - acc: 0.68 - ETA: 22s - loss: 5.0532 - acc: 0.68 - ETA: 21s - loss: 5.0475 - acc: 0.68 - ETA: 21s - loss: 5.0421 - acc: 0.68 - ETA: 21s - loss: 4.9765 - acc: 0.69 - ETA: 20s - loss: 4.9141 - acc: 0.69 - ETA: 20s - loss: 4.8930 - acc: 0.69 - ETA: 20s - loss: 4.8354 - acc: 0.70 - ETA: 19s - loss: 4.8171 - acc: 0.70 - ETA: 19s - loss: 4.8712 - acc: 0.69 - ETA: 19s - loss: 4.8354 - acc: 0.70 - ETA: 18s - loss: 4.8183 - acc: 0.70 - ETA: 18s - loss: 4.8018 - acc: 0.70 - ETA: 18s - loss: 4.8025 - acc: 0.70 - ETA: 17s - loss: 4.7710 - acc: 0.70 - ETA: 17s - loss: 4.8196 - acc: 0.70 - ETA: 17s - loss: 4.8354 - acc: 0.70 - ETA: 16s - loss: 4.8050 - acc: 0.70 - ETA: 16s - loss: 4.7757 - acc: 0.70 - ETA: 15s - loss: 4.7768 - acc: 0.70 - ETA: 15s - loss: 4.7923 - acc: 0.70 - ETA: 15s - loss: 4.7647 - acc: 0.70 - ETA: 14s - loss: 4.7660 - acc: 0.70 - ETA: 14s - loss: 4.7398 - acc: 0.70 - ETA: 14s - loss: 4.7414 - acc: 0.70 - ETA: 13s - loss: 4.7826 - acc: 0.70 - ETA: 13s - loss: 4.8224 - acc: 0.70 - ETA: 13s - loss: 4.8738 - acc: 0.69 - ETA: 12s - loss: 4.9236 - acc: 0.69 - ETA: 12s - loss: 4.9346 - acc: 0.69 - ETA: 12s - loss: 4.9820 - acc: 0.69 - ETA: 11s - loss: 4.9798 - acc: 0.69 - ETA: 11s - loss: 4.9421 - acc: 0.69 - ETA: 10s - loss: 4.9172 - acc: 0.69 - ETA: 10s - loss: 4.9045 - acc: 0.69 - ETA: 10s - loss: 4.9149 - acc: 0.69 - ETA: 9s - loss: 4.9362 - acc: 0.6938 - ETA: 9s - loss: 4.9017 - acc: 0.695 - ETA: 9s - loss: 4.8899 - acc: 0.696 - ETA: 8s - loss: 4.8784 - acc: 0.697 - ETA: 8s - loss: 4.9203 - acc: 0.694 - ETA: 8s - loss: 4.8982 - acc: 0.696 - ETA: 7s - loss: 4.9181 - acc: 0.694 - ETA: 7s - loss: 4.9680 - acc: 0.691 - ETA: 7s - loss: 4.9865 - acc: 0.690 - ETA: 6s - loss: 4.9747 - acc: 0.691 - ETA: 6s - loss: 4.9632 - acc: 0.692 - ETA: 6s - loss: 4.9617 - acc: 0.692 - ETA: 5s - loss: 4.9602 - acc: 0.692 - ETA: 5s - loss: 4.9966 - acc: 0.690 - ETA: 4s - loss: 5.0135 - acc: 0.689 - ETA: 4s - loss: 5.0207 - acc: 0.688 - ETA: 4s - loss: 5.0277 - acc: 0.688 - ETA: 3s - loss: 5.0346 - acc: 0.687 - ETA: 3s - loss: 5.0324 - acc: 0.687 - ETA: 3s - loss: 5.0657 - acc: 0.685 - ETA: 2s - loss: 5.1157 - acc: 0.682 - ETA: 2s - loss: 5.0954 - acc: 0.683 - ETA: 2s - loss: 5.0755 - acc: 0.685 - ETA: 1s - loss: 5.0899 - acc: 0.684 - ETA: 1s - loss: 5.0873 - acc: 0.684 - ETA: 1s - loss: 5.0598 - acc: 0.686 - ETA: 0s - loss: 5.0575 - acc: 0.686 - ETA: 0s - loss: 5.0796 - acc: 0.6848Epoch 00011: val_loss improved from 7.06716 to 6.81919, saving model to saved_models/best.weights.hdf5\n",
      "100/100 [==============================] - 39s - loss: 5.0611 - acc: 0.6860 - val_loss: 6.8192 - val_acc: 0.5769\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 35s - loss: 8.8650 - acc: 0.45 - ETA: 34s - loss: 6.8502 - acc: 0.57 - ETA: 34s - loss: 6.4472 - acc: 0.60 - ETA: 34s - loss: 5.8428 - acc: 0.63 - ETA: 33s - loss: 5.1578 - acc: 0.68 - ETA: 33s - loss: 4.9697 - acc: 0.69 - ETA: 32s - loss: 4.7203 - acc: 0.70 - ETA: 32s - loss: 4.8354 - acc: 0.70 - ETA: 32s - loss: 4.9250 - acc: 0.69 - ETA: 31s - loss: 5.0772 - acc: 0.68 - ETA: 31s - loss: 5.2017 - acc: 0.67 - ETA: 31s - loss: 5.1041 - acc: 0.68 - ETA: 30s - loss: 5.2694 - acc: 0.67 - ETA: 30s - loss: 5.0081 - acc: 0.68 - ETA: 30s - loss: 5.0503 - acc: 0.68 - ETA: 29s - loss: 4.9865 - acc: 0.69 - ETA: 29s - loss: 4.9302 - acc: 0.69 - ETA: 29s - loss: 4.7907 - acc: 0.70 - ETA: 28s - loss: 4.9203 - acc: 0.69 - ETA: 28s - loss: 4.7145 - acc: 0.70 - ETA: 27s - loss: 4.6435 - acc: 0.71 - ETA: 27s - loss: 4.7988 - acc: 0.70 - ETA: 27s - loss: 4.6953 - acc: 0.70 - ETA: 26s - loss: 4.8690 - acc: 0.69 - ETA: 26s - loss: 4.8354 - acc: 0.70 - ETA: 26s - loss: 4.9284 - acc: 0.69 - ETA: 25s - loss: 4.8653 - acc: 0.69 - ETA: 25s - loss: 4.8642 - acc: 0.69 - ETA: 25s - loss: 4.8076 - acc: 0.70 - ETA: 24s - loss: 4.7817 - acc: 0.70 - ETA: 24s - loss: 4.6794 - acc: 0.70 - ETA: 24s - loss: 4.6843 - acc: 0.70 - ETA: 25s - loss: 4.6889 - acc: 0.70 - ETA: 25s - loss: 4.7880 - acc: 0.70 - ETA: 24s - loss: 4.8354 - acc: 0.70 - ETA: 24s - loss: 4.9474 - acc: 0.69 - ETA: 24s - loss: 4.9879 - acc: 0.69 - ETA: 23s - loss: 5.0051 - acc: 0.68 - ETA: 23s - loss: 5.0627 - acc: 0.68 - ETA: 22s - loss: 4.9765 - acc: 0.69 - ETA: 22s - loss: 4.9730 - acc: 0.69 - ETA: 21s - loss: 4.9889 - acc: 0.69 - ETA: 21s - loss: 4.9479 - acc: 0.69 - ETA: 21s - loss: 4.9453 - acc: 0.69 - ETA: 20s - loss: 4.9966 - acc: 0.69 - ETA: 20s - loss: 4.9931 - acc: 0.69 - ETA: 20s - loss: 5.0240 - acc: 0.68 - ETA: 19s - loss: 5.1209 - acc: 0.68 - ETA: 19s - loss: 5.0986 - acc: 0.68 - ETA: 18s - loss: 5.0450 - acc: 0.68 - ETA: 18s - loss: 5.0409 - acc: 0.68 - ETA: 18s - loss: 5.0524 - acc: 0.68 - ETA: 17s - loss: 5.0939 - acc: 0.68 - ETA: 17s - loss: 5.0444 - acc: 0.68 - ETA: 16s - loss: 5.0259 - acc: 0.68 - ETA: 16s - loss: 5.0801 - acc: 0.68 - ETA: 16s - loss: 5.1323 - acc: 0.68 - ETA: 15s - loss: 5.0855 - acc: 0.68 - ETA: 15s - loss: 5.0813 - acc: 0.68 - ETA: 14s - loss: 5.1175 - acc: 0.68 - ETA: 14s - loss: 5.1261 - acc: 0.68 - ETA: 14s - loss: 5.1344 - acc: 0.68 - ETA: 13s - loss: 5.1296 - acc: 0.68 - ETA: 13s - loss: 5.1251 - acc: 0.68 - ETA: 13s - loss: 5.0958 - acc: 0.68 - ETA: 12s - loss: 5.1041 - acc: 0.68 - ETA: 12s - loss: 5.0519 - acc: 0.68 - ETA: 11s - loss: 5.0843 - acc: 0.68 - ETA: 11s - loss: 5.0573 - acc: 0.68 - ETA: 11s - loss: 5.0311 - acc: 0.68 - ETA: 10s - loss: 4.9943 - acc: 0.69 - ETA: 10s - loss: 5.0145 - acc: 0.68 - ETA: 10s - loss: 5.0121 - acc: 0.68 - ETA: 9s - loss: 4.9770 - acc: 0.6912 - ETA: 9s - loss: 4.9644 - acc: 0.692 - ETA: 8s - loss: 4.9945 - acc: 0.690 - ETA: 8s - loss: 5.0029 - acc: 0.689 - ETA: 8s - loss: 5.0317 - acc: 0.687 - ETA: 7s - loss: 5.0701 - acc: 0.685 - ETA: 7s - loss: 5.0873 - acc: 0.684 - ETA: 7s - loss: 5.0543 - acc: 0.686 - ETA: 6s - loss: 5.0516 - acc: 0.686 - ETA: 6s - loss: 5.0490 - acc: 0.686 - ETA: 5s - loss: 5.0561 - acc: 0.686 - ETA: 5s - loss: 5.0725 - acc: 0.685 - ETA: 5s - loss: 5.0603 - acc: 0.686 - ETA: 4s - loss: 5.0577 - acc: 0.686 - ETA: 4s - loss: 5.0369 - acc: 0.687 - ETA: 4s - loss: 5.0618 - acc: 0.686 - ETA: 3s - loss: 5.0682 - acc: 0.685 - ETA: 3s - loss: 5.0745 - acc: 0.685 - ETA: 2s - loss: 5.1070 - acc: 0.683 - ETA: 2s - loss: 5.0867 - acc: 0.684 - ETA: 2s - loss: 5.0755 - acc: 0.685 - ETA: 1s - loss: 5.0475 - acc: 0.686 - ETA: 1s - loss: 5.0537 - acc: 0.686 - ETA: 1s - loss: 5.0681 - acc: 0.685 - ETA: 0s - loss: 5.0657 - acc: 0.685 - ETA: 0s - loss: 5.0634 - acc: 0.6859Epoch 00012: val_loss did not improve\n",
      "100/100 [==============================] - 38s - loss: 5.0611 - acc: 0.6860 - val_loss: 7.6871 - val_acc: 0.5231\n",
      "Epoch 14/20\n",
      " 99/100 [============================>.] - ETA: 35s - loss: 4.8354 - acc: 0.70 - ETA: 34s - loss: 4.4325 - acc: 0.72 - ETA: 34s - loss: 3.4923 - acc: 0.78 - ETA: 34s - loss: 4.2310 - acc: 0.73 - ETA: 33s - loss: 4.0295 - acc: 0.75 - ETA: 33s - loss: 4.0295 - acc: 0.75 - ETA: 33s - loss: 4.1447 - acc: 0.74 - ETA: 33s - loss: 4.4325 - acc: 0.72 - ETA: 42s - loss: 4.5668 - acc: 0.71 - ETA: 41s - loss: 4.7548 - acc: 0.70 - ETA: 39s - loss: 4.7622 - acc: 0.70 - ETA: 38s - loss: 4.6340 - acc: 0.71 - ETA: 37s - loss: 4.7114 - acc: 0.70 - ETA: 36s - loss: 4.7203 - acc: 0.70 - ETA: 35s - loss: 4.7817 - acc: 0.70 - ETA: 35s - loss: 4.8858 - acc: 0.69 - ETA: 34s - loss: 4.9302 - acc: 0.69 - ETA: 33s - loss: 4.9250 - acc: 0.69 - ETA: 33s - loss: 4.8354 - acc: 0.70 - ETA: 32s - loss: 4.9160 - acc: 0.69 - ETA: 31s - loss: 4.7971 - acc: 0.70 - ETA: 31s - loss: 4.8721 - acc: 0.69 - ETA: 30s - loss: 4.8354 - acc: 0.70 - ETA: 30s - loss: 4.8354 - acc: 0.70 - ETA: 29s - loss: 4.8032 - acc: 0.70 - ETA: 29s - loss: 4.8044 - acc: 0.70 - ETA: 28s - loss: 4.8354 - acc: 0.70 - ETA: 28s - loss: 4.8642 - acc: 0.69 - ETA: 27s - loss: 4.9188 - acc: 0.69 - ETA: 27s - loss: 4.8354 - acc: 0.70 - ETA: 26s - loss: 4.9394 - acc: 0.69 - ETA: 26s - loss: 4.8858 - acc: 0.69 - ETA: 25s - loss: 4.8843 - acc: 0.69 - ETA: 25s - loss: 4.9776 - acc: 0.69 - ETA: 25s - loss: 5.0196 - acc: 0.68 - ETA: 24s - loss: 5.0369 - acc: 0.68 - ETA: 24s - loss: 4.9443 - acc: 0.69 - ETA: 23s - loss: 4.9203 - acc: 0.69 - ETA: 23s - loss: 4.9594 - acc: 0.69 - ETA: 22s - loss: 4.9563 - acc: 0.69 - ETA: 22s - loss: 4.9927 - acc: 0.69 - ETA: 22s - loss: 5.0081 - acc: 0.68 - ETA: 21s - loss: 5.0041 - acc: 0.68 - ETA: 21s - loss: 5.0369 - acc: 0.68 - ETA: 20s - loss: 5.0145 - acc: 0.68 - ETA: 20s - loss: 5.0281 - acc: 0.68 - ETA: 20s - loss: 5.0412 - acc: 0.68 - ETA: 19s - loss: 5.0705 - acc: 0.68 - ETA: 19s - loss: 5.0986 - acc: 0.68 - ETA: 18s - loss: 5.0611 - acc: 0.68 - ETA: 18s - loss: 5.0251 - acc: 0.68 - ETA: 18s - loss: 5.0679 - acc: 0.68 - ETA: 17s - loss: 5.0939 - acc: 0.68 - ETA: 17s - loss: 5.1638 - acc: 0.67 - ETA: 16s - loss: 5.1431 - acc: 0.68 - ETA: 16s - loss: 5.2096 - acc: 0.67 - ETA: 16s - loss: 5.2313 - acc: 0.67 - ETA: 15s - loss: 5.1967 - acc: 0.67 - ETA: 15s - loss: 5.1906 - acc: 0.67 - ETA: 14s - loss: 5.1847 - acc: 0.67 - ETA: 14s - loss: 5.1261 - acc: 0.68 - ETA: 14s - loss: 5.1214 - acc: 0.68 - ETA: 13s - loss: 5.1041 - acc: 0.68 - ETA: 13s - loss: 5.0873 - acc: 0.68 - ETA: 13s - loss: 5.1330 - acc: 0.68 - ETA: 12s - loss: 5.1651 - acc: 0.67 - ETA: 12s - loss: 5.1361 - acc: 0.68 - ETA: 11s - loss: 5.1317 - acc: 0.68 - ETA: 11s - loss: 5.1391 - acc: 0.68 - ETA: 11s - loss: 5.1693 - acc: 0.67 - ETA: 10s - loss: 5.1305 - acc: 0.68 - ETA: 10s - loss: 5.1264 - acc: 0.68 - ETA: 10s - loss: 5.1114 - acc: 0.68 - ETA: 9s - loss: 5.1404 - acc: 0.6811 - ETA: 9s - loss: 5.1256 - acc: 0.682 - ETA: 8s - loss: 5.1429 - acc: 0.680 - ETA: 8s - loss: 5.1390 - acc: 0.681 - ETA: 8s - loss: 5.1247 - acc: 0.682 - ETA: 7s - loss: 5.1517 - acc: 0.680 - ETA: 7s - loss: 5.1376 - acc: 0.681 - ETA: 7s - loss: 5.1339 - acc: 0.681 - ETA: 6s - loss: 5.1598 - acc: 0.679 - ETA: 6s - loss: 5.1558 - acc: 0.680 - ETA: 6s - loss: 5.1233 - acc: 0.682 - ETA: 5s - loss: 5.1199 - acc: 0.682 - ETA: 5s - loss: 5.1166 - acc: 0.682 - ETA: 4s - loss: 5.1133 - acc: 0.682 - ETA: 4s - loss: 5.1010 - acc: 0.683 - ETA: 4s - loss: 5.1161 - acc: 0.682 - ETA: 3s - loss: 5.1309 - acc: 0.681 - ETA: 3s - loss: 5.1542 - acc: 0.680 - ETA: 3s - loss: 5.1333 - acc: 0.681 - ETA: 2s - loss: 5.1647 - acc: 0.679 - ETA: 2s - loss: 5.1869 - acc: 0.678 - ETA: 1s - loss: 5.1663 - acc: 0.679 - ETA: 1s - loss: 5.1460 - acc: 0.680 - ETA: 1s - loss: 5.1013 - acc: 0.683 - ETA: 0s - loss: 5.1233 - acc: 0.682 - ETA: 0s - loss: 5.0959 - acc: 0.6838Epoch 00013: val_loss did not improve\n",
      "100/100 [==============================] - 39s - loss: 5.0611 - acc: 0.6860 - val_loss: 7.8111 - val_acc: 0.5154\n",
      "Epoch 15/20\n",
      " 18/100 [====>.........................] - ETA: 35s - loss: 4.0295 - acc: 0.75 - ETA: 35s - loss: 4.0295 - acc: 0.75 - ETA: 34s - loss: 4.0295 - acc: 0.75 - ETA: 34s - loss: 3.8280 - acc: 0.76 - ETA: 33s - loss: 3.8683 - acc: 0.76 - ETA: 33s - loss: 3.8952 - acc: 0.75 - ETA: 33s - loss: 4.0295 - acc: 0.75 - ETA: 32s - loss: 4.2310 - acc: 0.73 - ETA: 32s - loss: 4.3877 - acc: 0.72 - ETA: 32s - loss: 4.3519 - acc: 0.73 - ETA: 31s - loss: 4.6889 - acc: 0.70 - ETA: 31s - loss: 4.7011 - acc: 0.70 - ETA: 31s - loss: 4.7114 - acc: 0.70 - ETA: 30s - loss: 4.8930 - acc: 0.69 - ETA: 30s - loss: 4.8892 - acc: 0.69 - ETA: 30s - loss: 4.8354 - acc: 0.70 - ETA: 29s - loss: 4.7880 - acc: 0.70 - ETA: 29s - loss: 4.7011 - acc: 0.7083"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-f88b86d09a91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatagen_valid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                     validation_steps=valid_tensors.shape[0] // 20)\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DLND_L2\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DLND_L2\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[0;32m   1119\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1120\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DLND_L2\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DLND_L2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2040\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2041\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2042\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2044\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DLND_L2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1760\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1761\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1762\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1763\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1764\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DLND_L2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2273\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2274\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DLND_L2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DLND_L2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DLND_L2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DLND_L2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DLND_L2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/best.weights.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "network.fit_generator(datagen_train.flow(train_tensors, train_labels, batch_size=20),\n",
    "                    steps_per_epoch=train_tensors.shape[0] // 20,\n",
    "                    epochs=epochs, verbose=1, callbacks=[checkpointer],\n",
    "                    validation_data=datagen_valid.flow(valid_tensors, valid_labels, batch_size=20),\n",
    "                    validation_steps=valid_tensors.shape[0] // 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create output excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the model weights with the best validation loss.\n",
    "\n",
    "network.load_weights('saved_models/best.weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 600/600 [02:10<00:00,  4.55it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "y_pred = pd.DataFrame(columns=[\"task_1\", \"task_2\"])\n",
    "\n",
    "for ii in tqdm(range(len(test_files))):\n",
    "    path = test_files[ii]\n",
    "    prediction = np.argmax(network.predict(np.expand_dims(test_tensors[ii], axis=0)))\n",
    "    if prediction == 0:\n",
    "        y_pred.loc[path] = [1, 0]\n",
    "    if prediction == 2:\n",
    "        y_pred.loc[path] = [0, 1]\n",
    "    else:\n",
    "        y_pred.loc[path] = [0, 0]\n",
    "\n",
    "y_pred.to_csv(\"predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Code playground to be deleted later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def plot_roc_auc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    This function plots the ROC curves and provides the scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize dictionaries and array\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = np.zeros(3)\n",
    "    \n",
    "    # prepare for figure\n",
    "    plt.figure()\n",
    "    colors = ['aqua', 'cornflowerblue']\n",
    "\n",
    "    # for both classification tasks (categories 1 and 2)\n",
    "    for i in range(2):\n",
    "        # obtain ROC curve\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true[:,i], y_pred[:,i])\n",
    "        # obtain ROC AUC\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        # plot ROC curve\n",
    "        plt.plot(fpr[i], tpr[i], color=colors[i], lw=2,\n",
    "                 label='ROC curve for task {d} (area = {f:.2f})'.format(d=i+1, f=roc_auc[i]))\n",
    "    # get score for category 3\n",
    "    roc_auc[2] = np.average(roc_auc[:2])\n",
    "    \n",
    "    # format figure\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curves')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    # print scores\n",
    "    for i in range(3):\n",
    "        print('Category {d} Score: {f:.3f}'. format(d=i+1, f=roc_auc[i]))\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, thresh, classes):\n",
    "    \"\"\"\n",
    "    This function plots the (normalized) confusion matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    # obtain class predictions from probabilities\n",
    "    y_pred = (y_pred>=thresh)*1\n",
    "    # obtain (unnormalized) confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # normalize confusion matrix\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], '.2f'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a070ffe358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 1 Score: 0.500\n",
      "Category 2 Score: 0.500\n",
      "Category 3 Score: 0.500\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVMAAAEmCAYAAADfpHMGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a056bbea20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds_path = sys.argv[1]\n",
    "thresh = 0.5\n",
    "\n",
    "# get ground truth labels for test dataset\n",
    "truth = pd.read_csv('ground_truth.csv')\n",
    "y_true = truth.as_matrix(columns=[\"task_1\", \"task_2\"])\n",
    "\n",
    "# get model predictions for test dataset\n",
    "y_pred = pd.read_csv(\"predictions.csv\")\n",
    "y_pred = y_pred.as_matrix(columns=[\"task_1\", \"task_2\"])\n",
    "\n",
    "# plot ROC curves and print scores\n",
    "plot_roc_auc(y_true, y_pred)\n",
    "# plot confusion matrix\n",
    "classes = ['benign', 'malignant']\n",
    "plot_confusion_matrix(y_true[:,0], y_pred[:,0], thresh, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
