{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dermatologist-AI Project: Single Network Approach\n",
    "Workbook for a single network approach that classifies images into melanomas, nevus, or SBK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path, shuffle):\n",
    "    data = load_files(path, shuffle=shuffle)\n",
    "    file_paths = np.array(data['filenames'])\n",
    "    one_hot_labels = np_utils.to_categorical(np.array(data['target']),3)\n",
    "    return file_paths, one_hot_labels\n",
    "\n",
    "#import datasets\n",
    "train_files, train_labels = load_dataset('../data/train', True)\n",
    "valid_files, valid_labels = load_dataset('../data/valid', True)\n",
    "test_files, test_labels = load_dataset('../data/test', False)\n",
    "\n",
    "# load list of skin condition names\n",
    "skin_names = [item[14:-1] for item in sorted(glob(\"../data/train/*/\"))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [03:55<00:00,  8.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 150/150 [00:32<00:00,  4.67it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 600/600 [03:30<00:00, 10.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 224, 224, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 224, 224, 32)      9248      \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 224, 224, 32)      1056      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 112, 112, 48)      13872     \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 112, 112, 48)      20784     \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 112, 112, 48)      2352      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 56, 56, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 56, 56, 64)        27712     \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 56, 56, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 56, 56, 64)        4160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               3211520   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 3,329,299\n",
      "Trainable params: 3,329,299\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "### TODO: Define your architecture.\n",
    "model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', \n",
    "                        input_shape=(224, 224, 3)))\n",
    "model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(Conv2D(filters=32, kernel_size=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(filters=48, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(Conv2D(filters=48, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(Conv2D(filters=48, kernel_size=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "opt = optimizers.rmsprop(lr=0.00005, decay=1e-6)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150 samples, validate on 150 samples\n",
      "Epoch 1/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 1.0220 - acc: 0.500 - ETA: 1s - loss: 1.0263 - acc: 0.475 - ETA: 1s - loss: 1.0090 - acc: 0.500 - ETA: 1s - loss: 1.0151 - acc: 0.487 - ETA: 0s - loss: 1.0232 - acc: 0.490 - ETA: 0s - loss: 1.0043 - acc: 0.516 - ETA: 0s - loss: 1.0040 - acc: 0.5143Epoch 00000: val_loss improved from inf to 1.03177, saving model to saved_models/weights.best.hdf5\n",
      "150/150 [==============================] - 3s - loss: 1.0073 - acc: 0.5200 - val_loss: 1.0318 - val_acc: 0.5200\n",
      "Epoch 2/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 0.9311 - acc: 0.600 - ETA: 1s - loss: 0.9705 - acc: 0.550 - ETA: 1s - loss: 0.9970 - acc: 0.533 - ETA: 1s - loss: 0.9790 - acc: 0.550 - ETA: 0s - loss: 1.0151 - acc: 0.520 - ETA: 0s - loss: 1.0071 - acc: 0.516 - ETA: 0s - loss: 1.0031 - acc: 0.5214Epoch 00001: val_loss did not improve\n",
      "150/150 [==============================] - 2s - loss: 1.0048 - acc: 0.5200 - val_loss: 1.0327 - val_acc: 0.5200\n",
      "Epoch 3/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 0.9992 - acc: 0.550 - ETA: 1s - loss: 0.9947 - acc: 0.575 - ETA: 1s - loss: 0.9927 - acc: 0.550 - ETA: 1s - loss: 0.9829 - acc: 0.550 - ETA: 0s - loss: 1.0218 - acc: 0.510 - ETA: 0s - loss: 1.0049 - acc: 0.525 - ETA: 0s - loss: 1.0025 - acc: 0.5286Epoch 00002: val_loss did not improve\n",
      "150/150 [==============================] - 2s - loss: 1.0045 - acc: 0.5200 - val_loss: 1.0325 - val_acc: 0.5200\n",
      "Epoch 4/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 1.0869 - acc: 0.450 - ETA: 1s - loss: 1.0245 - acc: 0.500 - ETA: 1s - loss: 1.0243 - acc: 0.483 - ETA: 1s - loss: 0.9942 - acc: 0.537 - ETA: 0s - loss: 0.9874 - acc: 0.540 - ETA: 0s - loss: 1.0068 - acc: 0.525 - ETA: 0s - loss: 1.0231 - acc: 0.5000Epoch 00003: val_loss improved from 1.03177 to 1.02673, saving model to saved_models/weights.best.hdf5\n",
      "150/150 [==============================] - 3s - loss: 1.0107 - acc: 0.5200 - val_loss: 1.0267 - val_acc: 0.5200\n",
      "Epoch 5/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 0.8962 - acc: 0.700 - ETA: 1s - loss: 0.9714 - acc: 0.575 - ETA: 1s - loss: 1.0788 - acc: 0.483 - ETA: 1s - loss: 1.0729 - acc: 0.462 - ETA: 0s - loss: 1.0547 - acc: 0.470 - ETA: 0s - loss: 1.0247 - acc: 0.508 - ETA: 0s - loss: 1.0198 - acc: 0.5071Epoch 00004: val_loss improved from 1.02673 to 1.01573, saving model to saved_models/weights.best.hdf5\n",
      "150/150 [==============================] - 3s - loss: 1.0056 - acc: 0.5200 - val_loss: 1.0157 - val_acc: 0.5200\n",
      "Epoch 6/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 0.8878 - acc: 0.650 - ETA: 1s - loss: 1.0027 - acc: 0.550 - ETA: 1s - loss: 0.9888 - acc: 0.550 - ETA: 1s - loss: 1.0107 - acc: 0.512 - ETA: 0s - loss: 1.0309 - acc: 0.490 - ETA: 0s - loss: 1.0323 - acc: 0.491 - ETA: 0s - loss: 1.0193 - acc: 0.5071Epoch 00005: val_loss did not improve\n",
      "150/150 [==============================] - 3s - loss: 1.0134 - acc: 0.5200 - val_loss: 1.0217 - val_acc: 0.5200\n",
      "Epoch 7/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 0.9353 - acc: 0.550 - ETA: 1s - loss: 1.0252 - acc: 0.500 - ETA: 1s - loss: 0.9975 - acc: 0.533 - ETA: 1s - loss: 0.9532 - acc: 0.575 - ETA: 0s - loss: 0.9699 - acc: 0.560 - ETA: 0s - loss: 0.9703 - acc: 0.558 - ETA: 0s - loss: 0.9875 - acc: 0.5357Epoch 00006: val_loss did not improve\n",
      "150/150 [==============================] - 2s - loss: 0.9987 - acc: 0.5200 - val_loss: 1.0368 - val_acc: 0.5200\n",
      "Epoch 8/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 1.0012 - acc: 0.400 - ETA: 1s - loss: 1.0161 - acc: 0.400 - ETA: 1s - loss: 0.9996 - acc: 0.466 - ETA: 1s - loss: 0.9990 - acc: 0.475 - ETA: 0s - loss: 0.9992 - acc: 0.480 - ETA: 0s - loss: 1.0062 - acc: 0.475 - ETA: 0s - loss: 0.9907 - acc: 0.5143Epoch 00007: val_loss improved from 1.01573 to 1.01203, saving model to saved_models/weights.best.hdf5\n",
      "150/150 [==============================] - 3s - loss: 0.9857 - acc: 0.5200 - val_loss: 1.0120 - val_acc: 0.5200\n",
      "Epoch 9/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 1.1671 - acc: 0.350 - ETA: 1s - loss: 1.0642 - acc: 0.450 - ETA: 1s - loss: 1.0384 - acc: 0.500 - ETA: 1s - loss: 1.0351 - acc: 0.512 - ETA: 0s - loss: 1.0208 - acc: 0.510 - ETA: 0s - loss: 1.0068 - acc: 0.516 - ETA: 0s - loss: 0.9953 - acc: 0.5214Epoch 00008: val_loss improved from 1.01203 to 1.00802, saving model to saved_models/weights.best.hdf5\n",
      "150/150 [==============================] - 3s - loss: 0.9929 - acc: 0.5200 - val_loss: 1.0080 - val_acc: 0.5200\n",
      "Epoch 10/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 0.8493 - acc: 0.650 - ETA: 1s - loss: 0.8875 - acc: 0.575 - ETA: 1s - loss: 0.9204 - acc: 0.583 - ETA: 1s - loss: 0.9386 - acc: 0.575 - ETA: 0s - loss: 0.9767 - acc: 0.530 - ETA: 0s - loss: 0.9860 - acc: 0.516 - ETA: 0s - loss: 0.9977 - acc: 0.5071Epoch 00009: val_loss did not improve\n",
      "150/150 [==============================] - 3s - loss: 0.9876 - acc: 0.5200 - val_loss: 1.0143 - val_acc: 0.5200\n",
      "Epoch 11/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 1.2002 - acc: 0.200 - ETA: 1s - loss: 1.0454 - acc: 0.425 - ETA: 1s - loss: 0.9790 - acc: 0.500 - ETA: 1s - loss: 0.9516 - acc: 0.550 - ETA: 0s - loss: 0.9414 - acc: 0.550 - ETA: 0s - loss: 0.9523 - acc: 0.541 - ETA: 0s - loss: 0.9692 - acc: 0.5357Epoch 00010: val_loss did not improve\n",
      "150/150 [==============================] - 3s - loss: 0.9859 - acc: 0.5200 - val_loss: 1.0325 - val_acc: 0.5200\n",
      "Epoch 12/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 0.9899 - acc: 0.550 - ETA: 1s - loss: 1.0009 - acc: 0.525 - ETA: 1s - loss: 0.9951 - acc: 0.516 - ETA: 1s - loss: 1.0053 - acc: 0.487 - ETA: 0s - loss: 0.9889 - acc: 0.520 - ETA: 0s - loss: 0.9994 - acc: 0.491 - ETA: 0s - loss: 0.9887 - acc: 0.5143Epoch 00011: val_loss improved from 1.00802 to 0.99590, saving model to saved_models/weights.best.hdf5\n",
      "150/150 [==============================] - 3s - loss: 0.9741 - acc: 0.5267 - val_loss: 0.9959 - val_acc: 0.5200\n",
      "Epoch 13/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 0.8929 - acc: 0.550 - ETA: 1s - loss: 0.9060 - acc: 0.550 - ETA: 1s - loss: 0.9411 - acc: 0.550 - ETA: 1s - loss: 0.9384 - acc: 0.562 - ETA: 0s - loss: 0.9458 - acc: 0.560 - ETA: 0s - loss: 0.9444 - acc: 0.566 - ETA: 0s - loss: 0.9707 - acc: 0.5357Epoch 00012: val_loss did not improve\n",
      "150/150 [==============================] - 3s - loss: 0.9741 - acc: 0.5200 - val_loss: 1.0215 - val_acc: 0.5200\n",
      "Epoch 14/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 1.1994 - acc: 0.200 - ETA: 1s - loss: 1.0303 - acc: 0.500 - ETA: 1s - loss: 1.0124 - acc: 0.483 - ETA: 1s - loss: 1.0034 - acc: 0.475 - ETA: 0s - loss: 0.9909 - acc: 0.520 - ETA: 0s - loss: 0.9731 - acc: 0.541 - ETA: 0s - loss: 0.9866 - acc: 0.5286Epoch 00013: val_loss did not improve\n",
      "150/150 [==============================] - 3s - loss: 0.9833 - acc: 0.5267 - val_loss: 1.0021 - val_acc: 0.5200\n",
      "Epoch 15/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 0.9867 - acc: 0.450 - ETA: 1s - loss: 0.9993 - acc: 0.500 - ETA: 1s - loss: 0.9550 - acc: 0.566 - ETA: 1s - loss: 0.9285 - acc: 0.575 - ETA: 0s - loss: 0.9481 - acc: 0.560 - ETA: 0s - loss: 0.9494 - acc: 0.558 - ETA: 0s - loss: 0.9820 - acc: 0.5286Epoch 00014: val_loss did not improve\n",
      "150/150 [==============================] - 3s - loss: 0.9812 - acc: 0.5333 - val_loss: 1.0098 - val_acc: 0.5200\n",
      "Epoch 16/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 1.0364 - acc: 0.500 - ETA: 1s - loss: 1.0439 - acc: 0.425 - ETA: 1s - loss: 1.0076 - acc: 0.466 - ETA: 1s - loss: 0.9926 - acc: 0.475 - ETA: 0s - loss: 0.9988 - acc: 0.460 - ETA: 0s - loss: 0.9794 - acc: 0.500 - ETA: 0s - loss: 0.9583 - acc: 0.5214Epoch 00015: val_loss improved from 0.99590 to 0.98437, saving model to saved_models/weights.best.hdf5\n",
      "150/150 [==============================] - 3s - loss: 0.9569 - acc: 0.5267 - val_loss: 0.9844 - val_acc: 0.5200\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/150 [===========================>..] - ETA: 2s - loss: 1.0104 - acc: 0.450 - ETA: 1s - loss: 0.9765 - acc: 0.525 - ETA: 1s - loss: 1.0142 - acc: 0.500 - ETA: 1s - loss: 0.9664 - acc: 0.550 - ETA: 0s - loss: 0.9557 - acc: 0.530 - ETA: 0s - loss: 0.9676 - acc: 0.516 - ETA: 0s - loss: 0.9792 - acc: 0.5143Epoch 00016: val_loss improved from 0.98437 to 0.98244, saving model to saved_models/weights.best.hdf5\n",
      "150/150 [==============================] - 3s - loss: 0.9690 - acc: 0.5200 - val_loss: 0.9824 - val_acc: 0.5200\n",
      "Epoch 18/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 0.8496 - acc: 0.600 - ETA: 1s - loss: 0.9976 - acc: 0.525 - ETA: 1s - loss: 1.0139 - acc: 0.500 - ETA: 1s - loss: 0.9890 - acc: 0.537 - ETA: 0s - loss: 0.9602 - acc: 0.550 - ETA: 0s - loss: 0.9565 - acc: 0.541 - ETA: 0s - loss: 0.9740 - acc: 0.5143Epoch 00017: val_loss did not improve\n",
      "150/150 [==============================] - 3s - loss: 0.9669 - acc: 0.5267 - val_loss: 0.9877 - val_acc: 0.5200\n",
      "Epoch 19/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 0.8598 - acc: 0.550 - ETA: 1s - loss: 0.8547 - acc: 0.575 - ETA: 1s - loss: 0.8504 - acc: 0.583 - ETA: 1s - loss: 0.8922 - acc: 0.537 - ETA: 0s - loss: 0.9365 - acc: 0.500 - ETA: 0s - loss: 0.9368 - acc: 0.508 - ETA: 0s - loss: 0.9181 - acc: 0.5429Epoch 00018: val_loss did not improve\n",
      "150/150 [==============================] - 3s - loss: 0.9358 - acc: 0.5267 - val_loss: 1.0098 - val_acc: 0.5200\n",
      "Epoch 20/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 0.8666 - acc: 0.600 - ETA: 1s - loss: 0.8807 - acc: 0.600 - ETA: 1s - loss: 0.9193 - acc: 0.566 - ETA: 1s - loss: 0.9625 - acc: 0.550 - ETA: 0s - loss: 0.9431 - acc: 0.550 - ETA: 0s - loss: 0.9597 - acc: 0.525 - ETA: 0s - loss: 0.9469 - acc: 0.5357Epoch 00019: val_loss did not improve\n",
      "150/150 [==============================] - 3s - loss: 0.9548 - acc: 0.5200 - val_loss: 0.9891 - val_acc: 0.5533\n",
      "Epoch 21/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 1.0403 - acc: 0.400 - ETA: 1s - loss: 0.9666 - acc: 0.525 - ETA: 1s - loss: 0.9561 - acc: 0.500 - ETA: 1s - loss: 0.9433 - acc: 0.525 - ETA: 0s - loss: 0.9456 - acc: 0.520 - ETA: 0s - loss: 0.9412 - acc: 0.525 - ETA: 0s - loss: 0.9391 - acc: 0.5357Epoch 00020: val_loss improved from 0.98244 to 0.96042, saving model to saved_models/weights.best.hdf5\n",
      "150/150 [==============================] - 3s - loss: 0.9307 - acc: 0.5333 - val_loss: 0.9604 - val_acc: 0.5200\n",
      "Epoch 22/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 0.8988 - acc: 0.600 - ETA: 1s - loss: 0.8593 - acc: 0.625 - ETA: 1s - loss: 0.8893 - acc: 0.583 - ETA: 1s - loss: 0.9036 - acc: 0.562 - ETA: 0s - loss: 0.9082 - acc: 0.560 - ETA: 0s - loss: 0.9343 - acc: 0.550 - ETA: 0s - loss: 0.9350 - acc: 0.5500Epoch 00021: val_loss did not improve\n",
      "150/150 [==============================] - 2s - loss: 0.9347 - acc: 0.5333 - val_loss: 0.9833 - val_acc: 0.6133\n",
      "Epoch 23/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 0.9703 - acc: 0.600 - ETA: 1s - loss: 0.9483 - acc: 0.675 - ETA: 1s - loss: 0.9041 - acc: 0.650 - ETA: 1s - loss: 0.9369 - acc: 0.612 - ETA: 0s - loss: 0.9208 - acc: 0.640 - ETA: 0s - loss: 0.9520 - acc: 0.608 - ETA: 0s - loss: 0.9425 - acc: 0.6143Epoch 00022: val_loss did not improve\n",
      "150/150 [==============================] - 3s - loss: 0.9467 - acc: 0.6067 - val_loss: 0.9796 - val_acc: 0.5667\n",
      "Epoch 24/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 0.9025 - acc: 0.650 - ETA: 1s - loss: 0.8644 - acc: 0.675 - ETA: 1s - loss: 0.8163 - acc: 0.650 - ETA: 1s - loss: 0.8346 - acc: 0.625 - ETA: 0s - loss: 0.8991 - acc: 0.580 - ETA: 0s - loss: 0.9374 - acc: 0.525 - ETA: 0s - loss: 0.9334 - acc: 0.5429Epoch 00023: val_loss did not improve\n",
      "150/150 [==============================] - 3s - loss: 0.9232 - acc: 0.5467 - val_loss: 0.9668 - val_acc: 0.6200\n",
      "Epoch 25/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 0.8912 - acc: 0.500 - ETA: 1s - loss: 0.9292 - acc: 0.525 - ETA: 1s - loss: 0.9219 - acc: 0.583 - ETA: 1s - loss: 0.9266 - acc: 0.562 - ETA: 0s - loss: 0.9392 - acc: 0.560 - ETA: 0s - loss: 0.9230 - acc: 0.575 - ETA: 0s - loss: 0.9120 - acc: 0.5786Epoch 00024: val_loss improved from 0.96042 to 0.93864, saving model to saved_models/weights.best.hdf5\n",
      "150/150 [==============================] - 3s - loss: 0.9005 - acc: 0.5867 - val_loss: 0.9386 - val_acc: 0.6067\n",
      "Epoch 26/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 0.9786 - acc: 0.500 - ETA: 1s - loss: 0.9242 - acc: 0.525 - ETA: 1s - loss: 0.9683 - acc: 0.550 - ETA: 1s - loss: 0.9453 - acc: 0.550 - ETA: 0s - loss: 0.8988 - acc: 0.610 - ETA: 0s - loss: 0.9363 - acc: 0.583 - ETA: 0s - loss: 0.9326 - acc: 0.5643Epoch 00025: val_loss improved from 0.93864 to 0.92467, saving model to saved_models/weights.best.hdf5\n",
      "150/150 [==============================] - 3s - loss: 0.9146 - acc: 0.5800 - val_loss: 0.9247 - val_acc: 0.6000\n",
      "Epoch 27/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 1.0187 - acc: 0.450 - ETA: 1s - loss: 0.9849 - acc: 0.500 - ETA: 1s - loss: 0.9251 - acc: 0.550 - ETA: 1s - loss: 0.9568 - acc: 0.525 - ETA: 0s - loss: 0.9448 - acc: 0.550 - ETA: 0s - loss: 0.9397 - acc: 0.550 - ETA: 0s - loss: 0.9210 - acc: 0.5929Epoch 00026: val_loss improved from 0.92467 to 0.91451, saving model to saved_models/weights.best.hdf5\n",
      "150/150 [==============================] - 3s - loss: 0.9083 - acc: 0.5933 - val_loss: 0.9145 - val_acc: 0.5800\n",
      "Epoch 28/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 0.8941 - acc: 0.550 - ETA: 1s - loss: 0.8846 - acc: 0.600 - ETA: 1s - loss: 0.9775 - acc: 0.550 - ETA: 1s - loss: 0.9638 - acc: 0.562 - ETA: 0s - loss: 0.9311 - acc: 0.580 - ETA: 0s - loss: 0.9048 - acc: 0.591 - ETA: 0s - loss: 0.9114 - acc: 0.5857Epoch 00027: val_loss improved from 0.91451 to 0.91055, saving model to saved_models/weights.best.hdf5\n",
      "150/150 [==============================] - 3s - loss: 0.9013 - acc: 0.5933 - val_loss: 0.9106 - val_acc: 0.6067\n",
      "Epoch 29/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 0.8281 - acc: 0.600 - ETA: 1s - loss: 0.9165 - acc: 0.575 - ETA: 1s - loss: 0.8783 - acc: 0.616 - ETA: 1s - loss: 0.9093 - acc: 0.575 - ETA: 0s - loss: 0.8959 - acc: 0.600 - ETA: 0s - loss: 0.9044 - acc: 0.591 - ETA: 0s - loss: 0.8977 - acc: 0.5929Epoch 00028: val_loss did not improve\n",
      "150/150 [==============================] - 3s - loss: 0.8971 - acc: 0.5933 - val_loss: 0.9478 - val_acc: 0.6600\n",
      "Epoch 30/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 0.7808 - acc: 0.600 - ETA: 1s - loss: 0.7882 - acc: 0.675 - ETA: 1s - loss: 0.8052 - acc: 0.666 - ETA: 1s - loss: 0.8370 - acc: 0.687 - ETA: 0s - loss: 0.8461 - acc: 0.660 - ETA: 0s - loss: 0.8488 - acc: 0.650 - ETA: 0s - loss: 0.8397 - acc: 0.6500Epoch 00029: val_loss improved from 0.91055 to 0.90578, saving model to saved_models/weights.best.hdf5\n",
      "150/150 [==============================] - 3s - loss: 0.8377 - acc: 0.6400 - val_loss: 0.9058 - val_acc: 0.6467\n",
      "Epoch 31/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 0.6942 - acc: 0.800 - ETA: 1s - loss: 0.6862 - acc: 0.725 - ETA: 1s - loss: 0.8069 - acc: 0.683 - ETA: 1s - loss: 0.8289 - acc: 0.650 - ETA: 0s - loss: 0.8620 - acc: 0.630 - ETA: 0s - loss: 0.8867 - acc: 0.608 - ETA: 0s - loss: 0.8936 - acc: 0.6214Epoch 00030: val_loss did not improve\n",
      "150/150 [==============================] - 2s - loss: 0.8872 - acc: 0.6333 - val_loss: 0.9130 - val_acc: 0.6600\n",
      "Epoch 32/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 1.0745 - acc: 0.450 - ETA: 1s - loss: 0.9760 - acc: 0.450 - ETA: 1s - loss: 0.9403 - acc: 0.550 - ETA: 1s - loss: 0.8890 - acc: 0.612 - ETA: 0s - loss: 0.8480 - acc: 0.610 - ETA: 0s - loss: 0.8670 - acc: 0.591 - ETA: 0s - loss: 0.8898 - acc: 0.5857Epoch 00031: val_loss improved from 0.90578 to 0.88564, saving model to saved_models/weights.best.hdf5\n",
      "150/150 [==============================] - 3s - loss: 0.8739 - acc: 0.6000 - val_loss: 0.8856 - val_acc: 0.6333\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/150 [===========================>..] - ETA: 2s - loss: 0.7746 - acc: 0.650 - ETA: 1s - loss: 0.8314 - acc: 0.650 - ETA: 1s - loss: 0.8584 - acc: 0.600 - ETA: 1s - loss: 0.8587 - acc: 0.587 - ETA: 0s - loss: 0.8662 - acc: 0.600 - ETA: 0s - loss: 0.8834 - acc: 0.608 - ETA: 0s - loss: 0.8672 - acc: 0.6286Epoch 00032: val_loss improved from 0.88564 to 0.87481, saving model to saved_models/weights.best.hdf5\n",
      "150/150 [==============================] - 3s - loss: 0.8642 - acc: 0.6267 - val_loss: 0.8748 - val_acc: 0.6200\n",
      "Epoch 34/50\n",
      "140/150 [===========================>..] - ETA: 2s - loss: 0.7802 - acc: 0.650 - ETA: 1s - loss: 0.8615 - acc: 0.625 - ETA: 1s - loss: 0.8149 - acc: 0.650 - ETA: 1s - loss: 0.8440 - acc: 0.650 - ETA: 0s - loss: 0.8345 - acc: 0.640 - ETA: 0s - loss: 0.8300 - acc: 0.641 - ETA: 0s - loss: 0.8284 - acc: 0.6429Epoch 00033: val_loss did not improve\n",
      "150/150 [==============================] - 3s - loss: 0.8406 - acc: 0.6400 - val_loss: 0.9108 - val_acc: 0.6667\n",
      "Epoch 35/50\n",
      " 20/150 [===>..........................] - ETA: 2s - loss: 0.6628 - acc: 0.8500"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "epochs = 50\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(valid_tensors, valid_labels,\n",
    "            validation_data=(valid_tensors, valid_labels),\n",
    "            epochs=epochs,\n",
    "            batch_size=20,\n",
    "            callbacks=[checkpointer],\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the model weights with the best validation loss.\n",
    "\n",
    "model.load_weights('saved_models/best.weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "y_pred = pd.DataFrame(columns=[\"task_1\", \"task_2\"])\n",
    "\n",
    "for ii in tqdm(range(len(test_files))):\n",
    "    path = test_files[ii]\n",
    "    prediction = np.argmax(network.predict(np.expand_dims(test_tensors[ii], axis=0)))\n",
    "    if prediction == 0:\n",
    "        y_pred.loc[path] = [1, 0]\n",
    "    if prediction == 2:\n",
    "        y_pred.loc[path] = [0, 1]\n",
    "    else:\n",
    "        y_pred.loc[path] = [0, 0]\n",
    "\n",
    "y_pred.to_csv(\"predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
