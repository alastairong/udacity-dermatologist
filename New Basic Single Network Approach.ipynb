{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dermatologist-AI Project: Single Network Approach\n",
    "Workbook for a single network approach that classifies images into melanomas, nevus, or SBK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from keras.preprocessing import image                  \n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from tqdm import tqdm\n",
    "from PIL import ImageFile                            \n",
    "import os\n",
    "import pickle\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True      \n",
    "\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path, shuffle):\n",
    "    data = load_files(path, shuffle=shuffle)\n",
    "    file_paths = np.array(data['filenames'])\n",
    "    one_hot_labels = np_utils.to_categorical(np.array(data['target']),3)\n",
    "    return file_paths, one_hot_labels\n",
    "\n",
    "# define functions to convert images into 4D tensors for convnets\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    x = preprocess_input(x)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors) \n",
    "\n",
    "# define function to decide whether to load tensors from a saved pickle or raw images\n",
    "def load_tensors_and_labels(pickle_file, raw_file_path, shuffle, force=False):\n",
    "    pickle_file = os.path.join('.', pickle_file)\n",
    " \n",
    "    if force or not os.path.exists(pickle_file):\n",
    "        files, labels = load_dataset(raw_file_path, shuffle)\n",
    "        tensors = paths_to_tensor(files).astype('float32')/255\n",
    "        with open(pickle_file, 'wb') as handle:\n",
    "            pickle.dump([tensors, labels], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        return tensors, labels\n",
    "\n",
    "    else:        \n",
    "        with open(pickle_file, 'rb') as handle:\n",
    "            tensors, labels = pickle.load(handle)        \n",
    "        return tensors, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [03:53<00:00,  8.57it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 150/150 [00:33<00:00,  4.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 600/600 [03:19<00:00, 11.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "train_tensors, train_labels = load_tensors_and_labels('train_data.pickle','../data/train', True)\n",
    "valid_tensors, valid_labels = load_tensors_and_labels('valid_data.pickle','../data/valid', True)\n",
    "test_tensors, test_labels = load_tensors_and_labels('test_data.pickle','../data/test', False)\n",
    "\n",
    "# load list of skin condition names\n",
    "skin_names = [item[14:-1] for item in sorted(glob(\"../data/train/*/\"))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network based on four three layers of VGG19 followed by trainable convnet and dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 21,760,835\n",
      "Trainable params: 11,175,683\n",
      "Non-trainable params: 10,585,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#First 4 blocks of VGG19\n",
    "model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', trainable=False, name='block1_conv1', \n",
    "                        input_shape=(224, 224, 3)))\n",
    "model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', trainable=False, name='block1_conv2'))\n",
    "model.add(MaxPooling2D(pool_size=2, name='block1_pool'))\n",
    "model.add(Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', trainable=False, name='block2_conv1'))\n",
    "model.add(Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', trainable=False, name='block2_conv2'))\n",
    "model.add(MaxPooling2D(pool_size=2, name='block2_pool'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(filters=256, kernel_size=3, padding='same', activation='relu', trainable=False, name='block3_conv1'))\n",
    "model.add(Conv2D(filters=256, kernel_size=3, padding='same', activation='relu', trainable=False, name='block3_conv2'))\n",
    "model.add(Conv2D(filters=256, kernel_size=3, padding='same', activation='relu', trainable=False, name='block3_conv3'))\n",
    "model.add(Conv2D(filters=256, kernel_size=3, padding='same', activation='relu', trainable=False, name='block3_conv4'))\n",
    "model.add(MaxPooling2D(pool_size=2, name='block3_pool'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(filters=512, kernel_size=3, padding='same', activation='relu', trainable=False, name='block4_conv1'))\n",
    "model.add(Conv2D(filters=512, kernel_size=3, padding='same', activation='relu', trainable=False, name='block4_conv2'))\n",
    "model.add(Conv2D(filters=512, kernel_size=3, padding='same', activation='relu', trainable=False, name='block4_conv3'))\n",
    "model.add(Conv2D(filters=512, kernel_size=3, padding='same', activation='relu', trainable=False, name='block4_conv4'))\n",
    "model.add(MaxPooling2D(pool_size=2, name='block4_pool'))\n",
    "\n",
    "#First 1 trainable convnet block\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(filters=512, kernel_size=3, padding='same', activation='relu', trainable=True, name='block5_conv1'))\n",
    "model.add(Conv2D(filters=512, kernel_size=3, padding='same', activation='relu', trainable=True, name='block5_conv2'))\n",
    "model.add(MaxPooling2D(pool_size=2, name='block5_pool'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#Trainable dense layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Import and assign VGG19 weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "opt = optimizers.rmsprop(lr=0.00005, decay=1e-6)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "import h5py\n",
    "\n",
    "weights_path = 'vgg19_weights.h5'\n",
    "f = h5py.File(weights_path, 'r')\n",
    "\n",
    "VGGnet_layers = ['block1_conv1',\n",
    "                 'block1_conv2',\n",
    "                 'block2_conv1',\n",
    "                 'block2_conv2',\n",
    "                 'block3_conv1',\n",
    "                 'block3_conv2',\n",
    "                 'block3_conv3',\n",
    "                 'block3_conv4',\n",
    "                 'block4_conv1',\n",
    "                 'block4_conv2',\n",
    "                 'block4_conv3',\n",
    "                 'block4_conv4']\n",
    "layer_names = [layer.name for layer in model.layers]\n",
    "\n",
    "for i in VGGnet_layers:\n",
    "    weight_names = f[i].attrs[\"weight_names\"]    \n",
    "    weights = [f[i][j] for j in weight_names]\n",
    "    index = layer_names.index(i)\n",
    "    model.layers[index].set_weights(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 150 samples\n",
      "Epoch 1/20\n",
      "1980/2000 [============================>.] - ETA: 171s - loss: 3.5028 - acc: 0.350 - ETA: 123s - loss: 2.7339 - acc: 0.575 - ETA: 106s - loss: 2.5942 - acc: 0.600 - ETA: 97s - loss: 2.5878 - acc: 0.512 - ETA: 91s - loss: 2.5962 - acc: 0.56 - ETA: 87s - loss: 3.1293 - acc: 0.54 - ETA: 84s - loss: 3.1372 - acc: 0.51 - ETA: 82s - loss: 3.0917 - acc: 0.47 - ETA: 80s - loss: 2.9872 - acc: 0.47 - ETA: 78s - loss: 2.8210 - acc: 0.48 - ETA: 77s - loss: 2.6895 - acc: 0.50 - ETA: 75s - loss: 2.5437 - acc: 0.52 - ETA: 74s - loss: 2.5052 - acc: 0.52 - ETA: 73s - loss: 2.4076 - acc: 0.52 - ETA: 71s - loss: 2.3275 - acc: 0.52 - ETA: 70s - loss: 2.2669 - acc: 0.52 - ETA: 69s - loss: 2.2277 - acc: 0.52 - ETA: 68s - loss: 2.1788 - acc: 0.52 - ETA: 67s - loss: 2.1018 - acc: 0.53 - ETA: 66s - loss: 2.0538 - acc: 0.53 - ETA: 65s - loss: 2.0531 - acc: 0.52 - ETA: 64s - loss: 2.0183 - acc: 0.52 - ETA: 63s - loss: 1.9882 - acc: 0.52 - ETA: 62s - loss: 1.9580 - acc: 0.52 - ETA: 61s - loss: 1.9095 - acc: 0.53 - ETA: 60s - loss: 1.8760 - acc: 0.53 - ETA: 59s - loss: 1.8399 - acc: 0.54 - ETA: 58s - loss: 1.8364 - acc: 0.54 - ETA: 58s - loss: 1.8138 - acc: 0.53 - ETA: 57s - loss: 1.7893 - acc: 0.53 - ETA: 56s - loss: 1.7627 - acc: 0.54 - ETA: 55s - loss: 1.7281 - acc: 0.54 - ETA: 54s - loss: 1.7173 - acc: 0.54 - ETA: 53s - loss: 1.6957 - acc: 0.54 - ETA: 52s - loss: 1.6867 - acc: 0.54 - ETA: 51s - loss: 1.6694 - acc: 0.54 - ETA: 51s - loss: 1.6546 - acc: 0.55 - ETA: 50s - loss: 1.6373 - acc: 0.55 - ETA: 49s - loss: 1.6152 - acc: 0.55 - ETA: 48s - loss: 1.6029 - acc: 0.56 - ETA: 47s - loss: 1.5974 - acc: 0.55 - ETA: 46s - loss: 1.5818 - acc: 0.55 - ETA: 45s - loss: 1.5707 - acc: 0.55 - ETA: 45s - loss: 1.5573 - acc: 0.55 - ETA: 44s - loss: 1.5361 - acc: 0.56 - ETA: 43s - loss: 1.5339 - acc: 0.55 - ETA: 42s - loss: 1.5230 - acc: 0.55 - ETA: 41s - loss: 1.5120 - acc: 0.56 - ETA: 40s - loss: 1.4976 - acc: 0.56 - ETA: 40s - loss: 1.4864 - acc: 0.56 - ETA: 39s - loss: 1.4763 - acc: 0.56 - ETA: 38s - loss: 1.4626 - acc: 0.56 - ETA: 37s - loss: 1.4503 - acc: 0.56 - ETA: 36s - loss: 1.4447 - acc: 0.56 - ETA: 35s - loss: 1.4340 - acc: 0.56 - ETA: 35s - loss: 1.4250 - acc: 0.56 - ETA: 34s - loss: 1.4170 - acc: 0.56 - ETA: 33s - loss: 1.4097 - acc: 0.56 - ETA: 32s - loss: 1.4119 - acc: 0.56 - ETA: 31s - loss: 1.4046 - acc: 0.56 - ETA: 31s - loss: 1.3892 - acc: 0.56 - ETA: 30s - loss: 1.3853 - acc: 0.56 - ETA: 29s - loss: 1.3784 - acc: 0.57 - ETA: 28s - loss: 1.3741 - acc: 0.57 - ETA: 27s - loss: 1.3670 - acc: 0.57 - ETA: 27s - loss: 1.3628 - acc: 0.57 - ETA: 26s - loss: 1.3563 - acc: 0.57 - ETA: 25s - loss: 1.3500 - acc: 0.57 - ETA: 24s - loss: 1.3507 - acc: 0.57 - ETA: 23s - loss: 1.3435 - acc: 0.57 - ETA: 23s - loss: 1.3372 - acc: 0.57 - ETA: 22s - loss: 1.3309 - acc: 0.57 - ETA: 21s - loss: 1.3219 - acc: 0.57 - ETA: 20s - loss: 1.3165 - acc: 0.57 - ETA: 19s - loss: 1.3119 - acc: 0.57 - ETA: 19s - loss: 1.3144 - acc: 0.58 - ETA: 18s - loss: 1.3116 - acc: 0.58 - ETA: 17s - loss: 1.3051 - acc: 0.58 - ETA: 16s - loss: 1.3020 - acc: 0.58 - ETA: 15s - loss: 1.2961 - acc: 0.58 - ETA: 15s - loss: 1.2886 - acc: 0.58 - ETA: 14s - loss: 1.2854 - acc: 0.58 - ETA: 13s - loss: 1.2829 - acc: 0.58 - ETA: 12s - loss: 1.2807 - acc: 0.58 - ETA: 11s - loss: 1.2764 - acc: 0.58 - ETA: 11s - loss: 1.2695 - acc: 0.58 - ETA: 10s - loss: 1.2615 - acc: 0.59 - ETA: 9s - loss: 1.2551 - acc: 0.5920 - ETA: 8s - loss: 1.2500 - acc: 0.594 - ETA: 7s - loss: 1.2441 - acc: 0.596 - ETA: 7s - loss: 1.2442 - acc: 0.596 - ETA: 6s - loss: 1.2411 - acc: 0.596 - ETA: 5s - loss: 1.2363 - acc: 0.596 - ETA: 4s - loss: 1.2302 - acc: 0.597 - ETA: 3s - loss: 1.2256 - acc: 0.597 - ETA: 3s - loss: 1.2206 - acc: 0.599 - ETA: 2s - loss: 1.2178 - acc: 0.600 - ETA: 1s - loss: 1.2119 - acc: 0.601 - ETA: 0s - loss: 1.2096 - acc: 0.6025Epoch 00000: val_loss improved from inf to 1.08661, saving model to saved_models/weights1.hdf5\n",
      "2000/2000 [==============================] - 84s - loss: 1.2061 - acc: 0.6020 - val_loss: 1.0866 - val_acc: 0.5133\n",
      "Epoch 2/20\n",
      "1980/2000 [============================>.] - ETA: 74s - loss: 1.1063 - acc: 0.45 - ETA: 74s - loss: 0.8629 - acc: 0.57 - ETA: 73s - loss: 0.8378 - acc: 0.63 - ETA: 73s - loss: 0.9071 - acc: 0.63 - ETA: 72s - loss: 1.0104 - acc: 0.61 - ETA: 71s - loss: 0.9910 - acc: 0.60 - ETA: 70s - loss: 0.9403 - acc: 0.64 - ETA: 69s - loss: 0.9255 - acc: 0.65 - ETA: 69s - loss: 0.9500 - acc: 0.65 - ETA: 68s - loss: 0.9432 - acc: 0.65 - ETA: 67s - loss: 0.9054 - acc: 0.67 - ETA: 66s - loss: 0.8861 - acc: 0.67 - ETA: 66s - loss: 0.8777 - acc: 0.67 - ETA: 65s - loss: 0.8701 - acc: 0.67 - ETA: 64s - loss: 0.8666 - acc: 0.68 - ETA: 63s - loss: 0.8685 - acc: 0.68 - ETA: 62s - loss: 0.8641 - acc: 0.67 - ETA: 62s - loss: 0.8747 - acc: 0.67 - ETA: 61s - loss: 0.8656 - acc: 0.68 - ETA: 60s - loss: 0.8593 - acc: 0.69 - ETA: 59s - loss: 0.8736 - acc: 0.68 - ETA: 59s - loss: 0.8743 - acc: 0.68 - ETA: 58s - loss: 0.8627 - acc: 0.68 - ETA: 57s - loss: 0.8775 - acc: 0.68 - ETA: 56s - loss: 0.8683 - acc: 0.69 - ETA: 56s - loss: 0.8730 - acc: 0.68 - ETA: 55s - loss: 0.8744 - acc: 0.68 - ETA: 54s - loss: 0.8726 - acc: 0.67 - ETA: 53s - loss: 0.8828 - acc: 0.66 - ETA: 53s - loss: 0.8807 - acc: 0.67 - ETA: 52s - loss: 0.8810 - acc: 0.67 - ETA: 51s - loss: 0.8844 - acc: 0.67 - ETA: 50s - loss: 0.8888 - acc: 0.66 - ETA: 50s - loss: 0.8920 - acc: 0.66 - ETA: 49s - loss: 0.8894 - acc: 0.66 - ETA: 48s - loss: 0.8925 - acc: 0.66 - ETA: 47s - loss: 0.8901 - acc: 0.66 - ETA: 47s - loss: 0.8880 - acc: 0.66 - ETA: 46s - loss: 0.8820 - acc: 0.66 - ETA: 45s - loss: 0.8825 - acc: 0.65 - ETA: 44s - loss: 0.8775 - acc: 0.65 - ETA: 44s - loss: 0.8817 - acc: 0.65 - ETA: 43s - loss: 0.8774 - acc: 0.65 - ETA: 42s - loss: 0.8746 - acc: 0.65 - ETA: 41s - loss: 0.8674 - acc: 0.66 - ETA: 40s - loss: 0.8827 - acc: 0.65 - ETA: 40s - loss: 0.8764 - acc: 0.66 - ETA: 39s - loss: 0.8732 - acc: 0.66 - ETA: 38s - loss: 0.8760 - acc: 0.65 - ETA: 37s - loss: 0.8722 - acc: 0.66 - ETA: 37s - loss: 0.8702 - acc: 0.65 - ETA: 36s - loss: 0.8627 - acc: 0.66 - ETA: 35s - loss: 0.8693 - acc: 0.66 - ETA: 34s - loss: 0.8652 - acc: 0.66 - ETA: 34s - loss: 0.8659 - acc: 0.66 - ETA: 33s - loss: 0.8691 - acc: 0.65 - ETA: 32s - loss: 0.8746 - acc: 0.65 - ETA: 31s - loss: 0.8734 - acc: 0.65 - ETA: 31s - loss: 0.8754 - acc: 0.65 - ETA: 30s - loss: 0.8760 - acc: 0.65 - ETA: 29s - loss: 0.8736 - acc: 0.65 - ETA: 28s - loss: 0.8779 - acc: 0.65 - ETA: 28s - loss: 0.8739 - acc: 0.65 - ETA: 27s - loss: 0.8711 - acc: 0.65 - ETA: 26s - loss: 0.8726 - acc: 0.65 - ETA: 25s - loss: 0.8699 - acc: 0.65 - ETA: 25s - loss: 0.8687 - acc: 0.65 - ETA: 24s - loss: 0.8668 - acc: 0.66 - ETA: 23s - loss: 0.8640 - acc: 0.66 - ETA: 22s - loss: 0.8615 - acc: 0.66 - ETA: 22s - loss: 0.8574 - acc: 0.66 - ETA: 21s - loss: 0.8617 - acc: 0.66 - ETA: 20s - loss: 0.8611 - acc: 0.66 - ETA: 19s - loss: 0.8607 - acc: 0.66 - ETA: 19s - loss: 0.8598 - acc: 0.65 - ETA: 18s - loss: 0.8567 - acc: 0.65 - ETA: 17s - loss: 0.8570 - acc: 0.65 - ETA: 16s - loss: 0.8571 - acc: 0.65 - ETA: 16s - loss: 0.8530 - acc: 0.66 - ETA: 15s - loss: 0.8543 - acc: 0.66 - ETA: 14s - loss: 0.8515 - acc: 0.66 - ETA: 13s - loss: 0.8481 - acc: 0.66 - ETA: 12s - loss: 0.8451 - acc: 0.66 - ETA: 12s - loss: 0.8459 - acc: 0.66 - ETA: 11s - loss: 0.8467 - acc: 0.66 - ETA: 10s - loss: 0.8445 - acc: 0.66 - ETA: 9s - loss: 0.8430 - acc: 0.6655 - ETA: 9s - loss: 0.8390 - acc: 0.668 - ETA: 8s - loss: 0.8366 - acc: 0.669 - ETA: 7s - loss: 0.8383 - acc: 0.669 - ETA: 6s - loss: 0.8357 - acc: 0.670 - ETA: 6s - loss: 0.8302 - acc: 0.672 - ETA: 5s - loss: 0.8343 - acc: 0.672 - ETA: 4s - loss: 0.8336 - acc: 0.671 - ETA: 3s - loss: 0.8319 - acc: 0.673 - ETA: 3s - loss: 0.8317 - acc: 0.673 - ETA: 2s - loss: 0.8296 - acc: 0.674 - ETA: 1s - loss: 0.8291 - acc: 0.673 - ETA: 0s - loss: 0.8278 - acc: 0.6737Epoch 00001: val_loss improved from 1.08661 to 0.87210, saving model to saved_models/weights1.hdf5\n",
      "2000/2000 [==============================] - 81s - loss: 0.8281 - acc: 0.6725 - val_loss: 0.8721 - val_acc: 0.6267\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/2000 [============================>.] - ETA: 73s - loss: 0.5114 - acc: 0.80 - ETA: 74s - loss: 0.5636 - acc: 0.80 - ETA: 72s - loss: 0.7199 - acc: 0.75 - ETA: 72s - loss: 0.7604 - acc: 0.71 - ETA: 71s - loss: 0.7981 - acc: 0.70 - ETA: 71s - loss: 0.7718 - acc: 0.70 - ETA: 70s - loss: 0.8101 - acc: 0.67 - ETA: 69s - loss: 0.7930 - acc: 0.67 - ETA: 68s - loss: 0.7875 - acc: 0.67 - ETA: 68s - loss: 0.7689 - acc: 0.68 - ETA: 67s - loss: 0.7632 - acc: 0.68 - ETA: 66s - loss: 0.7612 - acc: 0.67 - ETA: 65s - loss: 0.7408 - acc: 0.68 - ETA: 65s - loss: 0.7308 - acc: 0.68 - ETA: 64s - loss: 0.7274 - acc: 0.69 - ETA: 63s - loss: 0.7326 - acc: 0.68 - ETA: 62s - loss: 0.7190 - acc: 0.69 - ETA: 62s - loss: 0.7218 - acc: 0.68 - ETA: 61s - loss: 0.7180 - acc: 0.69 - ETA: 60s - loss: 0.7169 - acc: 0.69 - ETA: 59s - loss: 0.7136 - acc: 0.69 - ETA: 59s - loss: 0.7105 - acc: 0.70 - ETA: 58s - loss: 0.7068 - acc: 0.70 - ETA: 57s - loss: 0.7160 - acc: 0.70 - ETA: 56s - loss: 0.7112 - acc: 0.70 - ETA: 56s - loss: 0.7099 - acc: 0.70 - ETA: 55s - loss: 0.7143 - acc: 0.70 - ETA: 54s - loss: 0.7116 - acc: 0.71 - ETA: 53s - loss: 0.7088 - acc: 0.71 - ETA: 52s - loss: 0.7173 - acc: 0.71 - ETA: 52s - loss: 0.7185 - acc: 0.70 - ETA: 51s - loss: 0.7115 - acc: 0.71 - ETA: 50s - loss: 0.7089 - acc: 0.71 - ETA: 49s - loss: 0.7087 - acc: 0.71 - ETA: 49s - loss: 0.7003 - acc: 0.71 - ETA: 48s - loss: 0.7019 - acc: 0.71 - ETA: 47s - loss: 0.7067 - acc: 0.71 - ETA: 46s - loss: 0.7065 - acc: 0.71 - ETA: 46s - loss: 0.7058 - acc: 0.71 - ETA: 45s - loss: 0.7082 - acc: 0.71 - ETA: 44s - loss: 0.7116 - acc: 0.71 - ETA: 43s - loss: 0.7115 - acc: 0.71 - ETA: 43s - loss: 0.7114 - acc: 0.71 - ETA: 42s - loss: 0.7122 - acc: 0.71 - ETA: 41s - loss: 0.7116 - acc: 0.70 - ETA: 40s - loss: 0.7162 - acc: 0.70 - ETA: 40s - loss: 0.7121 - acc: 0.71 - ETA: 39s - loss: 0.7194 - acc: 0.70 - ETA: 38s - loss: 0.7227 - acc: 0.70 - ETA: 37s - loss: 0.7194 - acc: 0.70 - ETA: 37s - loss: 0.7145 - acc: 0.70 - ETA: 36s - loss: 0.7152 - acc: 0.70 - ETA: 35s - loss: 0.7184 - acc: 0.70 - ETA: 34s - loss: 0.7208 - acc: 0.69 - ETA: 34s - loss: 0.7209 - acc: 0.69 - ETA: 33s - loss: 0.7243 - acc: 0.69 - ETA: 32s - loss: 0.7245 - acc: 0.69 - ETA: 31s - loss: 0.7230 - acc: 0.69 - ETA: 31s - loss: 0.7180 - acc: 0.69 - ETA: 30s - loss: 0.7183 - acc: 0.69 - ETA: 29s - loss: 0.7231 - acc: 0.69 - ETA: 28s - loss: 0.7229 - acc: 0.69 - ETA: 27s - loss: 0.7198 - acc: 0.69 - ETA: 27s - loss: 0.7203 - acc: 0.69 - ETA: 26s - loss: 0.7251 - acc: 0.69 - ETA: 25s - loss: 0.7264 - acc: 0.69 - ETA: 24s - loss: 0.7244 - acc: 0.69 - ETA: 24s - loss: 0.7261 - acc: 0.69 - ETA: 23s - loss: 0.7265 - acc: 0.69 - ETA: 22s - loss: 0.7295 - acc: 0.69 - ETA: 21s - loss: 0.7256 - acc: 0.69 - ETA: 21s - loss: 0.7223 - acc: 0.70 - ETA: 20s - loss: 0.7191 - acc: 0.70 - ETA: 19s - loss: 0.7190 - acc: 0.70 - ETA: 18s - loss: 0.7191 - acc: 0.70 - ETA: 18s - loss: 0.7192 - acc: 0.70 - ETA: 17s - loss: 0.7181 - acc: 0.70 - ETA: 16s - loss: 0.7188 - acc: 0.70 - ETA: 15s - loss: 0.7198 - acc: 0.70 - ETA: 15s - loss: 0.7184 - acc: 0.70 - ETA: 14s - loss: 0.7219 - acc: 0.70 - ETA: 13s - loss: 0.7228 - acc: 0.69 - ETA: 12s - loss: 0.7245 - acc: 0.69 - ETA: 12s - loss: 0.7268 - acc: 0.69 - ETA: 11s - loss: 0.7273 - acc: 0.69 - ETA: 10s - loss: 0.7236 - acc: 0.69 - ETA: 9s - loss: 0.7264 - acc: 0.6966 - ETA: 9s - loss: 0.7258 - acc: 0.696 - ETA: 8s - loss: 0.7270 - acc: 0.695 - ETA: 7s - loss: 0.7263 - acc: 0.696 - ETA: 6s - loss: 0.7290 - acc: 0.694 - ETA: 6s - loss: 0.7319 - acc: 0.691 - ETA: 5s - loss: 0.7334 - acc: 0.689 - ETA: 4s - loss: 0.7351 - acc: 0.688 - ETA: 3s - loss: 0.7366 - acc: 0.688 - ETA: 3s - loss: 0.7412 - acc: 0.687 - ETA: 2s - loss: 0.7407 - acc: 0.686 - ETA: 1s - loss: 0.7414 - acc: 0.685 - ETA: 0s - loss: 0.7389 - acc: 0.6864Epoch 00002: val_loss improved from 0.87210 to 0.85021, saving model to saved_models/weights1.hdf5\n",
      "2000/2000 [==============================] - 81s - loss: 0.7447 - acc: 0.6840 - val_loss: 0.8502 - val_acc: 0.6267\n",
      "Epoch 4/20\n",
      "1980/2000 [============================>.] - ETA: 74s - loss: 0.5016 - acc: 0.80 - ETA: 74s - loss: 0.5273 - acc: 0.77 - ETA: 73s - loss: 0.6149 - acc: 0.73 - ETA: 72s - loss: 0.5867 - acc: 0.75 - ETA: 71s - loss: 0.6353 - acc: 0.71 - ETA: 71s - loss: 0.6432 - acc: 0.71 - ETA: 70s - loss: 0.7108 - acc: 0.70 - ETA: 69s - loss: 0.6881 - acc: 0.71 - ETA: 68s - loss: 0.7029 - acc: 0.70 - ETA: 68s - loss: 0.6973 - acc: 0.69 - ETA: 67s - loss: 0.6978 - acc: 0.69 - ETA: 66s - loss: 0.6886 - acc: 0.70 - ETA: 65s - loss: 0.6744 - acc: 0.70 - ETA: 65s - loss: 0.6716 - acc: 0.70 - ETA: 64s - loss: 0.6601 - acc: 0.70 - ETA: 63s - loss: 0.6797 - acc: 0.69 - ETA: 62s - loss: 0.6764 - acc: 0.69 - ETA: 61s - loss: 0.6769 - acc: 0.69 - ETA: 61s - loss: 0.6804 - acc: 0.69 - ETA: 60s - loss: 0.6789 - acc: 0.69 - ETA: 59s - loss: 0.6727 - acc: 0.70 - ETA: 59s - loss: 0.6675 - acc: 0.70 - ETA: 58s - loss: 0.6649 - acc: 0.71 - ETA: 57s - loss: 0.6743 - acc: 0.70 - ETA: 56s - loss: 0.6674 - acc: 0.70 - ETA: 56s - loss: 0.6622 - acc: 0.70 - ETA: 55s - loss: 0.6659 - acc: 0.70 - ETA: 54s - loss: 0.6624 - acc: 0.70 - ETA: 53s - loss: 0.6595 - acc: 0.70 - ETA: 53s - loss: 0.6672 - acc: 0.70 - ETA: 52s - loss: 0.6735 - acc: 0.70 - ETA: 51s - loss: 0.6763 - acc: 0.70 - ETA: 50s - loss: 0.6731 - acc: 0.70 - ETA: 51s - loss: 0.6722 - acc: 0.70 - ETA: 50s - loss: 0.6746 - acc: 0.69 - ETA: 49s - loss: 0.6789 - acc: 0.69 - ETA: 49s - loss: 0.6815 - acc: 0.69 - ETA: 48s - loss: 0.6758 - acc: 0.69 - ETA: 47s - loss: 0.6809 - acc: 0.69 - ETA: 46s - loss: 0.6779 - acc: 0.69 - ETA: 45s - loss: 0.6739 - acc: 0.70 - ETA: 45s - loss: 0.6820 - acc: 0.69 - ETA: 44s - loss: 0.6860 - acc: 0.69 - ETA: 43s - loss: 0.6837 - acc: 0.69 - ETA: 42s - loss: 0.6805 - acc: 0.69 - ETA: 41s - loss: 0.6829 - acc: 0.69 - ETA: 41s - loss: 0.6817 - acc: 0.69 - ETA: 40s - loss: 0.6856 - acc: 0.69 - ETA: 39s - loss: 0.6854 - acc: 0.69 - ETA: 38s - loss: 0.6844 - acc: 0.69 - ETA: 37s - loss: 0.6863 - acc: 0.69 - ETA: 37s - loss: 0.6843 - acc: 0.69 - ETA: 36s - loss: 0.6857 - acc: 0.69 - ETA: 35s - loss: 0.6812 - acc: 0.69 - ETA: 34s - loss: 0.6787 - acc: 0.69 - ETA: 33s - loss: 0.6785 - acc: 0.69 - ETA: 33s - loss: 0.6780 - acc: 0.69 - ETA: 32s - loss: 0.6775 - acc: 0.69 - ETA: 31s - loss: 0.6751 - acc: 0.70 - ETA: 30s - loss: 0.6750 - acc: 0.69 - ETA: 30s - loss: 0.6781 - acc: 0.69 - ETA: 29s - loss: 0.6736 - acc: 0.70 - ETA: 28s - loss: 0.6707 - acc: 0.70 - ETA: 27s - loss: 0.6703 - acc: 0.70 - ETA: 26s - loss: 0.6651 - acc: 0.70 - ETA: 26s - loss: 0.6632 - acc: 0.70 - ETA: 25s - loss: 0.6634 - acc: 0.70 - ETA: 24s - loss: 0.6670 - acc: 0.70 - ETA: 23s - loss: 0.6677 - acc: 0.70 - ETA: 23s - loss: 0.6666 - acc: 0.70 - ETA: 22s - loss: 0.6704 - acc: 0.70 - ETA: 21s - loss: 0.6713 - acc: 0.70 - ETA: 20s - loss: 0.6740 - acc: 0.70 - ETA: 19s - loss: 0.6751 - acc: 0.70 - ETA: 19s - loss: 0.6740 - acc: 0.70 - ETA: 18s - loss: 0.6746 - acc: 0.70 - ETA: 17s - loss: 0.6737 - acc: 0.70 - ETA: 16s - loss: 0.6693 - acc: 0.70 - ETA: 16s - loss: 0.6678 - acc: 0.70 - ETA: 15s - loss: 0.6685 - acc: 0.70 - ETA: 14s - loss: 0.6765 - acc: 0.70 - ETA: 13s - loss: 0.6766 - acc: 0.70 - ETA: 13s - loss: 0.6852 - acc: 0.70 - ETA: 12s - loss: 0.6867 - acc: 0.69 - ETA: 11s - loss: 0.6880 - acc: 0.69 - ETA: 10s - loss: 0.6884 - acc: 0.69 - ETA: 9s - loss: 0.6866 - acc: 0.6983 - ETA: 9s - loss: 0.6877 - acc: 0.697 - ETA: 8s - loss: 0.6882 - acc: 0.697 - ETA: 7s - loss: 0.6876 - acc: 0.697 - ETA: 6s - loss: 0.6863 - acc: 0.697 - ETA: 6s - loss: 0.6881 - acc: 0.696 - ETA: 5s - loss: 0.6845 - acc: 0.698 - ETA: 4s - loss: 0.6864 - acc: 0.697 - ETA: 3s - loss: 0.6844 - acc: 0.698 - ETA: 3s - loss: 0.6806 - acc: 0.700 - ETA: 2s - loss: 0.6840 - acc: 0.699 - ETA: 1s - loss: 0.6869 - acc: 0.696 - ETA: 0s - loss: 0.6862 - acc: 0.6970Epoch 00003: val_loss improved from 0.85021 to 0.83922, saving model to saved_models/weights1.hdf5\n",
      "2000/2000 [==============================] - 81s - loss: 0.6845 - acc: 0.6975 - val_loss: 0.8392 - val_acc: 0.5800\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/2000 [============================>.] - ETA: 74s - loss: 0.6512 - acc: 0.65 - ETA: 73s - loss: 0.7473 - acc: 0.60 - ETA: 73s - loss: 0.7147 - acc: 0.65 - ETA: 72s - loss: 0.6729 - acc: 0.67 - ETA: 71s - loss: 0.6829 - acc: 0.67 - ETA: 71s - loss: 0.6896 - acc: 0.68 - ETA: 70s - loss: 0.6926 - acc: 0.67 - ETA: 69s - loss: 0.6476 - acc: 0.70 - ETA: 69s - loss: 0.6562 - acc: 0.69 - ETA: 68s - loss: 0.6536 - acc: 0.70 - ETA: 67s - loss: 0.6471 - acc: 0.70 - ETA: 66s - loss: 0.6288 - acc: 0.71 - ETA: 65s - loss: 0.6575 - acc: 0.70 - ETA: 65s - loss: 0.6480 - acc: 0.71 - ETA: 64s - loss: 0.6255 - acc: 0.72 - ETA: 63s - loss: 0.6272 - acc: 0.72 - ETA: 62s - loss: 0.6133 - acc: 0.73 - ETA: 61s - loss: 0.6123 - acc: 0.73 - ETA: 61s - loss: 0.6319 - acc: 0.72 - ETA: 60s - loss: 0.6443 - acc: 0.72 - ETA: 59s - loss: 0.6368 - acc: 0.72 - ETA: 58s - loss: 0.6411 - acc: 0.72 - ETA: 58s - loss: 0.6468 - acc: 0.71 - ETA: 57s - loss: 0.6391 - acc: 0.72 - ETA: 56s - loss: 0.6376 - acc: 0.72 - ETA: 55s - loss: 0.6457 - acc: 0.72 - ETA: 55s - loss: 0.6462 - acc: 0.72 - ETA: 54s - loss: 0.6430 - acc: 0.72 - ETA: 53s - loss: 0.6449 - acc: 0.72 - ETA: 52s - loss: 0.6441 - acc: 0.72 - ETA: 52s - loss: 0.6431 - acc: 0.72 - ETA: 51s - loss: 0.6403 - acc: 0.72 - ETA: 50s - loss: 0.6376 - acc: 0.72 - ETA: 49s - loss: 0.6369 - acc: 0.72 - ETA: 49s - loss: 0.6424 - acc: 0.71 - ETA: 48s - loss: 0.6431 - acc: 0.71 - ETA: 47s - loss: 0.6409 - acc: 0.71 - ETA: 46s - loss: 0.6375 - acc: 0.71 - ETA: 46s - loss: 0.6361 - acc: 0.71 - ETA: 45s - loss: 0.6295 - acc: 0.71 - ETA: 44s - loss: 0.6275 - acc: 0.71 - ETA: 43s - loss: 0.6260 - acc: 0.72 - ETA: 43s - loss: 0.6249 - acc: 0.72 - ETA: 42s - loss: 0.6270 - acc: 0.71 - ETA: 41s - loss: 0.6258 - acc: 0.72 - ETA: 40s - loss: 0.6208 - acc: 0.72 - ETA: 40s - loss: 0.6234 - acc: 0.72 - ETA: 39s - loss: 0.6222 - acc: 0.72 - ETA: 38s - loss: 0.6219 - acc: 0.72 - ETA: 37s - loss: 0.6238 - acc: 0.72 - ETA: 37s - loss: 0.6240 - acc: 0.72 - ETA: 36s - loss: 0.6199 - acc: 0.73 - ETA: 35s - loss: 0.6167 - acc: 0.73 - ETA: 34s - loss: 0.6201 - acc: 0.72 - ETA: 34s - loss: 0.6206 - acc: 0.73 - ETA: 33s - loss: 0.6204 - acc: 0.73 - ETA: 32s - loss: 0.6173 - acc: 0.73 - ETA: 31s - loss: 0.6188 - acc: 0.73 - ETA: 31s - loss: 0.6207 - acc: 0.72 - ETA: 30s - loss: 0.6197 - acc: 0.72 - ETA: 29s - loss: 0.6201 - acc: 0.73 - ETA: 28s - loss: 0.6199 - acc: 0.73 - ETA: 27s - loss: 0.6187 - acc: 0.73 - ETA: 27s - loss: 0.6195 - acc: 0.73 - ETA: 26s - loss: 0.6208 - acc: 0.73 - ETA: 25s - loss: 0.6178 - acc: 0.73 - ETA: 24s - loss: 0.6168 - acc: 0.73 - ETA: 24s - loss: 0.6195 - acc: 0.73 - ETA: 23s - loss: 0.6178 - acc: 0.73 - ETA: 22s - loss: 0.6161 - acc: 0.73 - ETA: 21s - loss: 0.6160 - acc: 0.73 - ETA: 21s - loss: 0.6162 - acc: 0.73 - ETA: 20s - loss: 0.6149 - acc: 0.73 - ETA: 19s - loss: 0.6158 - acc: 0.73 - ETA: 18s - loss: 0.6150 - acc: 0.73 - ETA: 18s - loss: 0.6182 - acc: 0.73 - ETA: 17s - loss: 0.6195 - acc: 0.73 - ETA: 16s - loss: 0.6186 - acc: 0.73 - ETA: 15s - loss: 0.6217 - acc: 0.72 - ETA: 15s - loss: 0.6241 - acc: 0.72 - ETA: 14s - loss: 0.6256 - acc: 0.72 - ETA: 13s - loss: 0.6265 - acc: 0.72 - ETA: 12s - loss: 0.6282 - acc: 0.72 - ETA: 12s - loss: 0.6265 - acc: 0.72 - ETA: 11s - loss: 0.6235 - acc: 0.72 - ETA: 10s - loss: 0.6224 - acc: 0.72 - ETA: 9s - loss: 0.6187 - acc: 0.7310 - ETA: 9s - loss: 0.6238 - acc: 0.727 - ETA: 8s - loss: 0.6249 - acc: 0.725 - ETA: 7s - loss: 0.6244 - acc: 0.726 - ETA: 6s - loss: 0.6271 - acc: 0.724 - ETA: 6s - loss: 0.6270 - acc: 0.725 - ETA: 5s - loss: 0.6274 - acc: 0.725 - ETA: 4s - loss: 0.6253 - acc: 0.726 - ETA: 3s - loss: 0.6249 - acc: 0.726 - ETA: 3s - loss: 0.6226 - acc: 0.728 - ETA: 2s - loss: 0.6198 - acc: 0.728 - ETA: 1s - loss: 0.6227 - acc: 0.727 - ETA: 0s - loss: 0.6223 - acc: 0.7283Epoch 00004: val_loss improved from 0.83922 to 0.75999, saving model to saved_models/weights1.hdf5\n",
      "2000/2000 [==============================] - 80s - loss: 0.6231 - acc: 0.7280 - val_loss: 0.7600 - val_acc: 0.6600\n",
      "Epoch 6/20\n",
      "1980/2000 [============================>.] - ETA: 74s - loss: 0.7848 - acc: 0.70 - ETA: 73s - loss: 0.7195 - acc: 0.70 - ETA: 72s - loss: 0.7484 - acc: 0.68 - ETA: 72s - loss: 0.7451 - acc: 0.68 - ETA: 71s - loss: 0.6684 - acc: 0.72 - ETA: 70s - loss: 0.6470 - acc: 0.73 - ETA: 70s - loss: 0.6568 - acc: 0.72 - ETA: 69s - loss: 0.6612 - acc: 0.71 - ETA: 68s - loss: 0.6838 - acc: 0.70 - ETA: 68s - loss: 0.6639 - acc: 0.70 - ETA: 67s - loss: 0.6608 - acc: 0.71 - ETA: 66s - loss: 0.6459 - acc: 0.72 - ETA: 65s - loss: 0.6322 - acc: 0.72 - ETA: 65s - loss: 0.6509 - acc: 0.71 - ETA: 64s - loss: 0.6292 - acc: 0.73 - ETA: 63s - loss: 0.6126 - acc: 0.74 - ETA: 62s - loss: 0.6145 - acc: 0.73 - ETA: 62s - loss: 0.6114 - acc: 0.73 - ETA: 61s - loss: 0.6184 - acc: 0.73 - ETA: 60s - loss: 0.6115 - acc: 0.73 - ETA: 59s - loss: 0.6033 - acc: 0.74 - ETA: 59s - loss: 0.5922 - acc: 0.74 - ETA: 58s - loss: 0.5836 - acc: 0.75 - ETA: 58s - loss: 0.5996 - acc: 0.74 - ETA: 57s - loss: 0.6013 - acc: 0.74 - ETA: 56s - loss: 0.5921 - acc: 0.75 - ETA: 55s - loss: 0.5945 - acc: 0.74 - ETA: 55s - loss: 0.5948 - acc: 0.74 - ETA: 54s - loss: 0.5961 - acc: 0.74 - ETA: 53s - loss: 0.5964 - acc: 0.74 - ETA: 52s - loss: 0.5966 - acc: 0.74 - ETA: 51s - loss: 0.6096 - acc: 0.73 - ETA: 51s - loss: 0.6120 - acc: 0.73 - ETA: 50s - loss: 0.6106 - acc: 0.73 - ETA: 49s - loss: 0.6186 - acc: 0.73 - ETA: 48s - loss: 0.6183 - acc: 0.73 - ETA: 48s - loss: 0.6186 - acc: 0.73 - ETA: 47s - loss: 0.6164 - acc: 0.73 - ETA: 46s - loss: 0.6125 - acc: 0.73 - ETA: 45s - loss: 0.6176 - acc: 0.73 - ETA: 44s - loss: 0.6113 - acc: 0.74 - ETA: 44s - loss: 0.6097 - acc: 0.74 - ETA: 43s - loss: 0.6101 - acc: 0.74 - ETA: 42s - loss: 0.6071 - acc: 0.74 - ETA: 41s - loss: 0.6050 - acc: 0.74 - ETA: 41s - loss: 0.6130 - acc: 0.74 - ETA: 40s - loss: 0.6219 - acc: 0.73 - ETA: 39s - loss: 0.6160 - acc: 0.74 - ETA: 39s - loss: 0.6149 - acc: 0.74 - ETA: 39s - loss: 0.6120 - acc: 0.74 - ETA: 38s - loss: 0.6137 - acc: 0.74 - ETA: 37s - loss: 0.6138 - acc: 0.74 - ETA: 36s - loss: 0.6119 - acc: 0.74 - ETA: 35s - loss: 0.6091 - acc: 0.74 - ETA: 35s - loss: 0.6103 - acc: 0.74 - ETA: 34s - loss: 0.6109 - acc: 0.74 - ETA: 33s - loss: 0.6131 - acc: 0.74 - ETA: 32s - loss: 0.6147 - acc: 0.74 - ETA: 31s - loss: 0.6183 - acc: 0.73 - ETA: 31s - loss: 0.6179 - acc: 0.73 - ETA: 30s - loss: 0.6140 - acc: 0.73 - ETA: 29s - loss: 0.6083 - acc: 0.74 - ETA: 28s - loss: 0.6072 - acc: 0.74 - ETA: 27s - loss: 0.6006 - acc: 0.74 - ETA: 27s - loss: 0.6035 - acc: 0.74 - ETA: 26s - loss: 0.6044 - acc: 0.74 - ETA: 25s - loss: 0.6034 - acc: 0.74 - ETA: 24s - loss: 0.6005 - acc: 0.74 - ETA: 24s - loss: 0.5983 - acc: 0.74 - ETA: 23s - loss: 0.5972 - acc: 0.74 - ETA: 22s - loss: 0.6003 - acc: 0.74 - ETA: 21s - loss: 0.5982 - acc: 0.74 - ETA: 20s - loss: 0.5970 - acc: 0.74 - ETA: 20s - loss: 0.5985 - acc: 0.74 - ETA: 19s - loss: 0.5978 - acc: 0.74 - ETA: 18s - loss: 0.5978 - acc: 0.74 - ETA: 17s - loss: 0.5950 - acc: 0.74 - ETA: 16s - loss: 0.5939 - acc: 0.74 - ETA: 16s - loss: 0.5926 - acc: 0.74 - ETA: 15s - loss: 0.5921 - acc: 0.74 - ETA: 14s - loss: 0.5900 - acc: 0.75 - ETA: 13s - loss: 0.5900 - acc: 0.75 - ETA: 13s - loss: 0.5905 - acc: 0.74 - ETA: 12s - loss: 0.5896 - acc: 0.74 - ETA: 11s - loss: 0.5899 - acc: 0.74 - ETA: 10s - loss: 0.5904 - acc: 0.74 - ETA: 10s - loss: 0.5911 - acc: 0.74 - ETA: 9s - loss: 0.5890 - acc: 0.7472 - ETA: 8s - loss: 0.5903 - acc: 0.746 - ETA: 7s - loss: 0.5881 - acc: 0.747 - ETA: 6s - loss: 0.5867 - acc: 0.747 - ETA: 6s - loss: 0.5843 - acc: 0.748 - ETA: 5s - loss: 0.5841 - acc: 0.747 - ETA: 4s - loss: 0.5820 - acc: 0.748 - ETA: 3s - loss: 0.5821 - acc: 0.748 - ETA: 3s - loss: 0.5840 - acc: 0.747 - ETA: 2s - loss: 0.5831 - acc: 0.746 - ETA: 1s - loss: 0.5816 - acc: 0.747 - ETA: 0s - loss: 0.5821 - acc: 0.7475Epoch 00005: val_loss did not improve\n",
      "2000/2000 [==============================] - 82s - loss: 0.5853 - acc: 0.7475 - val_loss: 0.7635 - val_acc: 0.6800\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/2000 [============================>.] - ETA: 74s - loss: 0.5213 - acc: 0.75 - ETA: 73s - loss: 0.4764 - acc: 0.80 - ETA: 72s - loss: 0.5483 - acc: 0.73 - ETA: 71s - loss: 0.5697 - acc: 0.73 - ETA: 71s - loss: 0.5648 - acc: 0.74 - ETA: 70s - loss: 0.5664 - acc: 0.74 - ETA: 69s - loss: 0.5427 - acc: 0.76 - ETA: 69s - loss: 0.5547 - acc: 0.76 - ETA: 68s - loss: 0.5704 - acc: 0.76 - ETA: 67s - loss: 0.5935 - acc: 0.75 - ETA: 66s - loss: 0.5857 - acc: 0.76 - ETA: 66s - loss: 0.5793 - acc: 0.77 - ETA: 65s - loss: 0.5910 - acc: 0.76 - ETA: 64s - loss: 0.5809 - acc: 0.77 - ETA: 64s - loss: 0.5774 - acc: 0.77 - ETA: 63s - loss: 0.5515 - acc: 0.78 - ETA: 62s - loss: 0.5498 - acc: 0.78 - ETA: 61s - loss: 0.5639 - acc: 0.77 - ETA: 61s - loss: 0.5594 - acc: 0.77 - ETA: 60s - loss: 0.5508 - acc: 0.78 - ETA: 59s - loss: 0.5489 - acc: 0.78 - ETA: 58s - loss: 0.5440 - acc: 0.78 - ETA: 58s - loss: 0.5537 - acc: 0.78 - ETA: 57s - loss: 0.5477 - acc: 0.78 - ETA: 56s - loss: 0.5599 - acc: 0.77 - ETA: 55s - loss: 0.5562 - acc: 0.77 - ETA: 55s - loss: 0.5578 - acc: 0.77 - ETA: 54s - loss: 0.5486 - acc: 0.77 - ETA: 53s - loss: 0.5437 - acc: 0.77 - ETA: 52s - loss: 0.5532 - acc: 0.76 - ETA: 52s - loss: 0.5531 - acc: 0.76 - ETA: 51s - loss: 0.5480 - acc: 0.77 - ETA: 50s - loss: 0.5382 - acc: 0.77 - ETA: 49s - loss: 0.5384 - acc: 0.77 - ETA: 49s - loss: 0.5389 - acc: 0.77 - ETA: 48s - loss: 0.5346 - acc: 0.77 - ETA: 47s - loss: 0.5336 - acc: 0.77 - ETA: 46s - loss: 0.5350 - acc: 0.77 - ETA: 46s - loss: 0.5374 - acc: 0.76 - ETA: 45s - loss: 0.5392 - acc: 0.76 - ETA: 44s - loss: 0.5398 - acc: 0.76 - ETA: 43s - loss: 0.5396 - acc: 0.77 - ETA: 43s - loss: 0.5441 - acc: 0.76 - ETA: 42s - loss: 0.5438 - acc: 0.76 - ETA: 41s - loss: 0.5441 - acc: 0.76 - ETA: 40s - loss: 0.5450 - acc: 0.76 - ETA: 39s - loss: 0.5394 - acc: 0.76 - ETA: 39s - loss: 0.5446 - acc: 0.76 - ETA: 38s - loss: 0.5451 - acc: 0.76 - ETA: 37s - loss: 0.5445 - acc: 0.76 - ETA: 36s - loss: 0.5453 - acc: 0.76 - ETA: 36s - loss: 0.5450 - acc: 0.76 - ETA: 35s - loss: 0.5407 - acc: 0.76 - ETA: 34s - loss: 0.5426 - acc: 0.76 - ETA: 33s - loss: 0.5404 - acc: 0.76 - ETA: 33s - loss: 0.5371 - acc: 0.76 - ETA: 32s - loss: 0.5407 - acc: 0.76 - ETA: 31s - loss: 0.5373 - acc: 0.76 - ETA: 30s - loss: 0.5413 - acc: 0.76 - ETA: 30s - loss: 0.5424 - acc: 0.76 - ETA: 29s - loss: 0.5456 - acc: 0.76 - ETA: 28s - loss: 0.5488 - acc: 0.76 - ETA: 27s - loss: 0.5468 - acc: 0.76 - ETA: 27s - loss: 0.5441 - acc: 0.76 - ETA: 26s - loss: 0.5423 - acc: 0.76 - ETA: 25s - loss: 0.5415 - acc: 0.76 - ETA: 24s - loss: 0.5400 - acc: 0.76 - ETA: 24s - loss: 0.5384 - acc: 0.76 - ETA: 23s - loss: 0.5395 - acc: 0.76 - ETA: 22s - loss: 0.5453 - acc: 0.76 - ETA: 21s - loss: 0.5445 - acc: 0.76 - ETA: 21s - loss: 0.5444 - acc: 0.76 - ETA: 20s - loss: 0.5452 - acc: 0.76 - ETA: 19s - loss: 0.5435 - acc: 0.76 - ETA: 18s - loss: 0.5410 - acc: 0.76 - ETA: 18s - loss: 0.5428 - acc: 0.76 - ETA: 17s - loss: 0.5402 - acc: 0.76 - ETA: 16s - loss: 0.5418 - acc: 0.76 - ETA: 15s - loss: 0.5417 - acc: 0.76 - ETA: 15s - loss: 0.5384 - acc: 0.76 - ETA: 14s - loss: 0.5435 - acc: 0.76 - ETA: 13s - loss: 0.5443 - acc: 0.76 - ETA: 12s - loss: 0.5443 - acc: 0.76 - ETA: 12s - loss: 0.5428 - acc: 0.76 - ETA: 11s - loss: 0.5449 - acc: 0.76 - ETA: 10s - loss: 0.5429 - acc: 0.76 - ETA: 9s - loss: 0.5429 - acc: 0.7655 - ETA: 9s - loss: 0.5427 - acc: 0.765 - ETA: 8s - loss: 0.5440 - acc: 0.764 - ETA: 7s - loss: 0.5434 - acc: 0.765 - ETA: 6s - loss: 0.5415 - acc: 0.765 - ETA: 6s - loss: 0.5461 - acc: 0.765 - ETA: 5s - loss: 0.5452 - acc: 0.766 - ETA: 4s - loss: 0.5438 - acc: 0.767 - ETA: 3s - loss: 0.5432 - acc: 0.767 - ETA: 3s - loss: 0.5423 - acc: 0.767 - ETA: 2s - loss: 0.5460 - acc: 0.764 - ETA: 1s - loss: 0.5459 - acc: 0.764 - ETA: 0s - loss: 0.5444 - acc: 0.7652Epoch 00006: val_loss improved from 0.75999 to 0.71913, saving model to saved_models/weights1.hdf5\n",
      "2000/2000 [==============================] - 81s - loss: 0.5453 - acc: 0.7645 - val_loss: 0.7191 - val_acc: 0.6800\n",
      "Epoch 8/20\n",
      "1980/2000 [============================>.] - ETA: 74s - loss: 0.5579 - acc: 0.75 - ETA: 73s - loss: 0.4890 - acc: 0.77 - ETA: 73s - loss: 0.4717 - acc: 0.78 - ETA: 72s - loss: 0.4420 - acc: 0.81 - ETA: 72s - loss: 0.4525 - acc: 0.80 - ETA: 71s - loss: 0.5033 - acc: 0.78 - ETA: 70s - loss: 0.5297 - acc: 0.76 - ETA: 69s - loss: 0.5760 - acc: 0.73 - ETA: 69s - loss: 0.5773 - acc: 0.75 - ETA: 68s - loss: 0.5387 - acc: 0.77 - ETA: 67s - loss: 0.5424 - acc: 0.76 - ETA: 66s - loss: 0.5475 - acc: 0.75 - ETA: 66s - loss: 0.5482 - acc: 0.74 - ETA: 65s - loss: 0.5284 - acc: 0.76 - ETA: 64s - loss: 0.5142 - acc: 0.77 - ETA: 63s - loss: 0.5248 - acc: 0.76 - ETA: 63s - loss: 0.5265 - acc: 0.77 - ETA: 62s - loss: 0.5580 - acc: 0.77 - ETA: 61s - loss: 0.5627 - acc: 0.76 - ETA: 60s - loss: 0.5673 - acc: 0.76 - ETA: 59s - loss: 0.5577 - acc: 0.76 - ETA: 59s - loss: 0.5637 - acc: 0.76 - ETA: 58s - loss: 0.5671 - acc: 0.76 - ETA: 57s - loss: 0.5654 - acc: 0.76 - ETA: 56s - loss: 0.5551 - acc: 0.77 - ETA: 56s - loss: 0.5541 - acc: 0.76 - ETA: 55s - loss: 0.5493 - acc: 0.77 - ETA: 54s - loss: 0.5408 - acc: 0.77 - ETA: 53s - loss: 0.5416 - acc: 0.77 - ETA: 53s - loss: 0.5329 - acc: 0.78 - ETA: 52s - loss: 0.5299 - acc: 0.78 - ETA: 51s - loss: 0.5304 - acc: 0.78 - ETA: 50s - loss: 0.5295 - acc: 0.78 - ETA: 50s - loss: 0.5309 - acc: 0.78 - ETA: 49s - loss: 0.5242 - acc: 0.78 - ETA: 48s - loss: 0.5225 - acc: 0.78 - ETA: 47s - loss: 0.5172 - acc: 0.78 - ETA: 47s - loss: 0.5132 - acc: 0.78 - ETA: 46s - loss: 0.5122 - acc: 0.78 - ETA: 45s - loss: 0.5171 - acc: 0.78 - ETA: 44s - loss: 0.5190 - acc: 0.78 - ETA: 44s - loss: 0.5188 - acc: 0.78 - ETA: 43s - loss: 0.5214 - acc: 0.78 - ETA: 42s - loss: 0.5223 - acc: 0.78 - ETA: 41s - loss: 0.5196 - acc: 0.78 - ETA: 41s - loss: 0.5213 - acc: 0.78 - ETA: 40s - loss: 0.5202 - acc: 0.77 - ETA: 39s - loss: 0.5191 - acc: 0.78 - ETA: 38s - loss: 0.5179 - acc: 0.78 - ETA: 37s - loss: 0.5182 - acc: 0.78 - ETA: 37s - loss: 0.5153 - acc: 0.78 - ETA: 36s - loss: 0.5118 - acc: 0.78 - ETA: 35s - loss: 0.5095 - acc: 0.78 - ETA: 34s - loss: 0.5092 - acc: 0.78 - ETA: 34s - loss: 0.5129 - acc: 0.78 - ETA: 33s - loss: 0.5110 - acc: 0.78 - ETA: 32s - loss: 0.5090 - acc: 0.78 - ETA: 31s - loss: 0.5108 - acc: 0.78 - ETA: 31s - loss: 0.5123 - acc: 0.78 - ETA: 30s - loss: 0.5089 - acc: 0.78 - ETA: 30s - loss: 0.5072 - acc: 0.78 - ETA: 29s - loss: 0.5036 - acc: 0.79 - ETA: 28s - loss: 0.5044 - acc: 0.78 - ETA: 27s - loss: 0.5028 - acc: 0.79 - ETA: 26s - loss: 0.5012 - acc: 0.79 - ETA: 26s - loss: 0.4980 - acc: 0.79 - ETA: 25s - loss: 0.5059 - acc: 0.78 - ETA: 24s - loss: 0.5123 - acc: 0.78 - ETA: 23s - loss: 0.5129 - acc: 0.78 - ETA: 23s - loss: 0.5130 - acc: 0.78 - ETA: 22s - loss: 0.5122 - acc: 0.78 - ETA: 21s - loss: 0.5119 - acc: 0.78 - ETA: 20s - loss: 0.5135 - acc: 0.78 - ETA: 19s - loss: 0.5127 - acc: 0.78 - ETA: 19s - loss: 0.5130 - acc: 0.78 - ETA: 18s - loss: 0.5154 - acc: 0.78 - ETA: 17s - loss: 0.5151 - acc: 0.78 - ETA: 16s - loss: 0.5148 - acc: 0.78 - ETA: 16s - loss: 0.5119 - acc: 0.78 - ETA: 15s - loss: 0.5124 - acc: 0.78 - ETA: 14s - loss: 0.5158 - acc: 0.78 - ETA: 13s - loss: 0.5174 - acc: 0.77 - ETA: 13s - loss: 0.5153 - acc: 0.78 - ETA: 12s - loss: 0.5125 - acc: 0.78 - ETA: 11s - loss: 0.5108 - acc: 0.78 - ETA: 10s - loss: 0.5092 - acc: 0.78 - ETA: 9s - loss: 0.5082 - acc: 0.7828 - ETA: 9s - loss: 0.5085 - acc: 0.782 - ETA: 8s - loss: 0.5072 - acc: 0.783 - ETA: 7s - loss: 0.5085 - acc: 0.782 - ETA: 6s - loss: 0.5081 - acc: 0.782 - ETA: 6s - loss: 0.5105 - acc: 0.779 - ETA: 5s - loss: 0.5141 - acc: 0.779 - ETA: 4s - loss: 0.5142 - acc: 0.779 - ETA: 3s - loss: 0.5182 - acc: 0.776 - ETA: 3s - loss: 0.5171 - acc: 0.777 - ETA: 2s - loss: 0.5149 - acc: 0.778 - ETA: 1s - loss: 0.5162 - acc: 0.778 - ETA: 0s - loss: 0.5165 - acc: 0.7778Epoch 00007: val_loss did not improve\n",
      "2000/2000 [==============================] - 81s - loss: 0.5144 - acc: 0.7795 - val_loss: 0.7645 - val_acc: 0.7133\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/2000 [============================>.] - ETA: 75s - loss: 0.5765 - acc: 0.80 - ETA: 74s - loss: 0.5486 - acc: 0.77 - ETA: 73s - loss: 0.4300 - acc: 0.81 - ETA: 73s - loss: 0.4412 - acc: 0.82 - ETA: 72s - loss: 0.4062 - acc: 0.82 - ETA: 71s - loss: 0.4157 - acc: 0.81 - ETA: 70s - loss: 0.4189 - acc: 0.80 - ETA: 69s - loss: 0.4348 - acc: 0.80 - ETA: 69s - loss: 0.4302 - acc: 0.80 - ETA: 68s - loss: 0.4300 - acc: 0.80 - ETA: 67s - loss: 0.4354 - acc: 0.80 - ETA: 66s - loss: 0.4377 - acc: 0.80 - ETA: 66s - loss: 0.4325 - acc: 0.80 - ETA: 65s - loss: 0.4157 - acc: 0.81 - ETA: 64s - loss: 0.4569 - acc: 0.80 - ETA: 63s - loss: 0.4908 - acc: 0.78 - ETA: 62s - loss: 0.4913 - acc: 0.77 - ETA: 62s - loss: 0.4926 - acc: 0.76 - ETA: 61s - loss: 0.4831 - acc: 0.77 - ETA: 60s - loss: 0.4750 - acc: 0.77 - ETA: 59s - loss: 0.4735 - acc: 0.77 - ETA: 59s - loss: 0.4651 - acc: 0.77 - ETA: 58s - loss: 0.4792 - acc: 0.77 - ETA: 57s - loss: 0.4776 - acc: 0.77 - ETA: 56s - loss: 0.4738 - acc: 0.78 - ETA: 56s - loss: 0.4716 - acc: 0.78 - ETA: 55s - loss: 0.4761 - acc: 0.78 - ETA: 54s - loss: 0.4801 - acc: 0.78 - ETA: 53s - loss: 0.4863 - acc: 0.78 - ETA: 53s - loss: 0.4854 - acc: 0.78 - ETA: 52s - loss: 0.4848 - acc: 0.78 - ETA: 51s - loss: 0.4783 - acc: 0.79 - ETA: 50s - loss: 0.4790 - acc: 0.79 - ETA: 50s - loss: 0.4780 - acc: 0.79 - ETA: 49s - loss: 0.4802 - acc: 0.79 - ETA: 48s - loss: 0.4768 - acc: 0.79 - ETA: 47s - loss: 0.4784 - acc: 0.79 - ETA: 47s - loss: 0.4760 - acc: 0.79 - ETA: 46s - loss: 0.4780 - acc: 0.79 - ETA: 45s - loss: 0.4835 - acc: 0.79 - ETA: 44s - loss: 0.4812 - acc: 0.79 - ETA: 44s - loss: 0.4808 - acc: 0.79 - ETA: 43s - loss: 0.4762 - acc: 0.80 - ETA: 42s - loss: 0.4760 - acc: 0.80 - ETA: 41s - loss: 0.4740 - acc: 0.80 - ETA: 40s - loss: 0.4753 - acc: 0.80 - ETA: 40s - loss: 0.4800 - acc: 0.80 - ETA: 39s - loss: 0.4803 - acc: 0.79 - ETA: 38s - loss: 0.4753 - acc: 0.80 - ETA: 37s - loss: 0.4789 - acc: 0.79 - ETA: 37s - loss: 0.4772 - acc: 0.79 - ETA: 36s - loss: 0.4742 - acc: 0.79 - ETA: 36s - loss: 0.4726 - acc: 0.79 - ETA: 35s - loss: 0.4758 - acc: 0.79 - ETA: 34s - loss: 0.4752 - acc: 0.80 - ETA: 33s - loss: 0.4736 - acc: 0.80 - ETA: 33s - loss: 0.4728 - acc: 0.80 - ETA: 32s - loss: 0.4752 - acc: 0.80 - ETA: 31s - loss: 0.4715 - acc: 0.80 - ETA: 30s - loss: 0.4700 - acc: 0.80 - ETA: 29s - loss: 0.4712 - acc: 0.80 - ETA: 29s - loss: 0.4695 - acc: 0.80 - ETA: 28s - loss: 0.4669 - acc: 0.80 - ETA: 27s - loss: 0.4631 - acc: 0.80 - ETA: 26s - loss: 0.4622 - acc: 0.80 - ETA: 26s - loss: 0.4615 - acc: 0.80 - ETA: 25s - loss: 0.4626 - acc: 0.80 - ETA: 24s - loss: 0.4656 - acc: 0.80 - ETA: 23s - loss: 0.4660 - acc: 0.80 - ETA: 23s - loss: 0.4678 - acc: 0.80 - ETA: 22s - loss: 0.4670 - acc: 0.80 - ETA: 21s - loss: 0.4631 - acc: 0.80 - ETA: 20s - loss: 0.4616 - acc: 0.81 - ETA: 19s - loss: 0.4610 - acc: 0.81 - ETA: 19s - loss: 0.4628 - acc: 0.80 - ETA: 18s - loss: 0.4629 - acc: 0.80 - ETA: 17s - loss: 0.4616 - acc: 0.80 - ETA: 16s - loss: 0.4604 - acc: 0.81 - ETA: 16s - loss: 0.4591 - acc: 0.81 - ETA: 15s - loss: 0.4604 - acc: 0.81 - ETA: 14s - loss: 0.4595 - acc: 0.80 - ETA: 13s - loss: 0.4582 - acc: 0.81 - ETA: 13s - loss: 0.4637 - acc: 0.80 - ETA: 12s - loss: 0.4654 - acc: 0.80 - ETA: 11s - loss: 0.4657 - acc: 0.81 - ETA: 10s - loss: 0.4634 - acc: 0.81 - ETA: 9s - loss: 0.4641 - acc: 0.8098 - ETA: 9s - loss: 0.4638 - acc: 0.809 - ETA: 8s - loss: 0.4635 - acc: 0.809 - ETA: 7s - loss: 0.4651 - acc: 0.808 - ETA: 6s - loss: 0.4663 - acc: 0.806 - ETA: 6s - loss: 0.4675 - acc: 0.806 - ETA: 5s - loss: 0.4679 - acc: 0.806 - ETA: 4s - loss: 0.4672 - acc: 0.806 - ETA: 3s - loss: 0.4692 - acc: 0.804 - ETA: 3s - loss: 0.4701 - acc: 0.803 - ETA: 2s - loss: 0.4691 - acc: 0.802 - ETA: 1s - loss: 0.4694 - acc: 0.803 - ETA: 0s - loss: 0.4680 - acc: 0.8040Epoch 00008: val_loss did not improve\n",
      "2000/2000 [==============================] - 81s - loss: 0.4705 - acc: 0.8020 - val_loss: 0.8404 - val_acc: 0.6400\n",
      "Epoch 10/20\n",
      "1980/2000 [============================>.] - ETA: 76s - loss: 0.3286 - acc: 0.90 - ETA: 75s - loss: 0.2660 - acc: 0.92 - ETA: 74s - loss: 0.3849 - acc: 0.86 - ETA: 73s - loss: 0.4073 - acc: 0.83 - ETA: 72s - loss: 0.4362 - acc: 0.83 - ETA: 71s - loss: 0.4265 - acc: 0.83 - ETA: 70s - loss: 0.4443 - acc: 0.81 - ETA: 69s - loss: 0.4530 - acc: 0.81 - ETA: 69s - loss: 0.4330 - acc: 0.82 - ETA: 68s - loss: 0.4223 - acc: 0.82 - ETA: 67s - loss: 0.4250 - acc: 0.82 - ETA: 67s - loss: 0.4456 - acc: 0.81 - ETA: 66s - loss: 0.4527 - acc: 0.81 - ETA: 65s - loss: 0.4558 - acc: 0.82 - ETA: 64s - loss: 0.4578 - acc: 0.82 - ETA: 63s - loss: 0.4680 - acc: 0.82 - ETA: 63s - loss: 0.4725 - acc: 0.81 - ETA: 62s - loss: 0.4661 - acc: 0.81 - ETA: 61s - loss: 0.4728 - acc: 0.81 - ETA: 60s - loss: 0.4676 - acc: 0.82 - ETA: 60s - loss: 0.4623 - acc: 0.82 - ETA: 59s - loss: 0.4584 - acc: 0.82 - ETA: 58s - loss: 0.4518 - acc: 0.82 - ETA: 57s - loss: 0.4421 - acc: 0.82 - ETA: 57s - loss: 0.4412 - acc: 0.83 - ETA: 56s - loss: 0.4489 - acc: 0.82 - ETA: 55s - loss: 0.4508 - acc: 0.82 - ETA: 54s - loss: 0.4515 - acc: 0.82 - ETA: 53s - loss: 0.4431 - acc: 0.83 - ETA: 53s - loss: 0.4422 - acc: 0.83 - ETA: 52s - loss: 0.4343 - acc: 0.83 - ETA: 51s - loss: 0.4384 - acc: 0.83 - ETA: 50s - loss: 0.4382 - acc: 0.83 - ETA: 50s - loss: 0.4395 - acc: 0.83 - ETA: 49s - loss: 0.4381 - acc: 0.83 - ETA: 48s - loss: 0.4394 - acc: 0.83 - ETA: 48s - loss: 0.4383 - acc: 0.83 - ETA: 47s - loss: 0.4400 - acc: 0.83 - ETA: 46s - loss: 0.4347 - acc: 0.83 - ETA: 45s - loss: 0.4380 - acc: 0.83 - ETA: 45s - loss: 0.4417 - acc: 0.83 - ETA: 44s - loss: 0.4388 - acc: 0.83 - ETA: 43s - loss: 0.4394 - acc: 0.83 - ETA: 42s - loss: 0.4396 - acc: 0.83 - ETA: 41s - loss: 0.4340 - acc: 0.83 - ETA: 41s - loss: 0.4361 - acc: 0.83 - ETA: 40s - loss: 0.4361 - acc: 0.83 - ETA: 39s - loss: 0.4390 - acc: 0.83 - ETA: 38s - loss: 0.4444 - acc: 0.82 - ETA: 38s - loss: 0.4462 - acc: 0.82 - ETA: 37s - loss: 0.4476 - acc: 0.82 - ETA: 36s - loss: 0.4497 - acc: 0.82 - ETA: 35s - loss: 0.4494 - acc: 0.82 - ETA: 35s - loss: 0.4510 - acc: 0.82 - ETA: 34s - loss: 0.4484 - acc: 0.82 - ETA: 33s - loss: 0.4501 - acc: 0.82 - ETA: 32s - loss: 0.4506 - acc: 0.82 - ETA: 32s - loss: 0.4470 - acc: 0.82 - ETA: 31s - loss: 0.4442 - acc: 0.82 - ETA: 30s - loss: 0.4476 - acc: 0.82 - ETA: 29s - loss: 0.4478 - acc: 0.82 - ETA: 28s - loss: 0.4475 - acc: 0.82 - ETA: 28s - loss: 0.4477 - acc: 0.82 - ETA: 27s - loss: 0.4474 - acc: 0.82 - ETA: 26s - loss: 0.4481 - acc: 0.82 - ETA: 25s - loss: 0.4506 - acc: 0.82 - ETA: 25s - loss: 0.4494 - acc: 0.82 - ETA: 24s - loss: 0.4505 - acc: 0.82 - ETA: 23s - loss: 0.4497 - acc: 0.82 - ETA: 22s - loss: 0.4487 - acc: 0.81 - ETA: 22s - loss: 0.4479 - acc: 0.81 - ETA: 21s - loss: 0.4475 - acc: 0.81 - ETA: 20s - loss: 0.4443 - acc: 0.82 - ETA: 19s - loss: 0.4434 - acc: 0.82 - ETA: 19s - loss: 0.4464 - acc: 0.82 - ETA: 18s - loss: 0.4459 - acc: 0.81 - ETA: 17s - loss: 0.4465 - acc: 0.81 - ETA: 16s - loss: 0.4447 - acc: 0.81 - ETA: 15s - loss: 0.4432 - acc: 0.81 - ETA: 15s - loss: 0.4497 - acc: 0.81 - ETA: 14s - loss: 0.4489 - acc: 0.81 - ETA: 13s - loss: 0.4501 - acc: 0.81 - ETA: 13s - loss: 0.4476 - acc: 0.81 - ETA: 12s - loss: 0.4461 - acc: 0.81 - ETA: 11s - loss: 0.4484 - acc: 0.81 - ETA: 11s - loss: 0.4492 - acc: 0.81 - ETA: 10s - loss: 0.4481 - acc: 0.81 - ETA: 9s - loss: 0.4462 - acc: 0.8176 - ETA: 9s - loss: 0.4471 - acc: 0.816 - ETA: 8s - loss: 0.4457 - acc: 0.817 - ETA: 7s - loss: 0.4453 - acc: 0.818 - ETA: 6s - loss: 0.4464 - acc: 0.818 - ETA: 5s - loss: 0.4460 - acc: 0.819 - ETA: 4s - loss: 0.4433 - acc: 0.820 - ETA: 4s - loss: 0.4458 - acc: 0.819 - ETA: 3s - loss: 0.4456 - acc: 0.818 - ETA: 2s - loss: 0.4435 - acc: 0.820 - ETA: 1s - loss: 0.4451 - acc: 0.819 - ETA: 0s - loss: 0.4451 - acc: 0.8197Epoch 00009: val_loss did not improve\n",
      "2000/2000 [==============================] - 86s - loss: 0.4496 - acc: 0.8175 - val_loss: 0.7487 - val_acc: 0.6800\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/2000 [============================>.] - ETA: 74s - loss: 0.4890 - acc: 0.80 - ETA: 74s - loss: 0.3017 - acc: 0.90 - ETA: 73s - loss: 0.3984 - acc: 0.83 - ETA: 72s - loss: 0.3708 - acc: 0.85 - ETA: 71s - loss: 0.3423 - acc: 0.86 - ETA: 70s - loss: 0.3279 - acc: 0.87 - ETA: 69s - loss: 0.3644 - acc: 0.86 - ETA: 69s - loss: 0.3924 - acc: 0.86 - ETA: 68s - loss: 0.3815 - acc: 0.85 - ETA: 67s - loss: 0.4291 - acc: 0.83 - ETA: 66s - loss: 0.4340 - acc: 0.82 - ETA: 66s - loss: 0.4191 - acc: 0.84 - ETA: 65s - loss: 0.4071 - acc: 0.84 - ETA: 64s - loss: 0.4044 - acc: 0.84 - ETA: 63s - loss: 0.4067 - acc: 0.84 - ETA: 63s - loss: 0.4156 - acc: 0.83 - ETA: 62s - loss: 0.4147 - acc: 0.83 - ETA: 61s - loss: 0.4181 - acc: 0.83 - ETA: 60s - loss: 0.4127 - acc: 0.83 - ETA: 60s - loss: 0.4057 - acc: 0.84 - ETA: 59s - loss: 0.3980 - acc: 0.84 - ETA: 58s - loss: 0.4028 - acc: 0.84 - ETA: 57s - loss: 0.4067 - acc: 0.84 - ETA: 57s - loss: 0.3984 - acc: 0.84 - ETA: 56s - loss: 0.3956 - acc: 0.84 - ETA: 55s - loss: 0.3887 - acc: 0.84 - ETA: 55s - loss: 0.3832 - acc: 0.85 - ETA: 54s - loss: 0.3856 - acc: 0.84 - ETA: 53s - loss: 0.3866 - acc: 0.84 - ETA: 52s - loss: 0.3942 - acc: 0.84 - ETA: 51s - loss: 0.3887 - acc: 0.84 - ETA: 51s - loss: 0.3844 - acc: 0.84 - ETA: 50s - loss: 0.3880 - acc: 0.84 - ETA: 49s - loss: 0.3841 - acc: 0.84 - ETA: 49s - loss: 0.3798 - acc: 0.84 - ETA: 48s - loss: 0.3750 - acc: 0.85 - ETA: 47s - loss: 0.3759 - acc: 0.85 - ETA: 46s - loss: 0.3708 - acc: 0.85 - ETA: 45s - loss: 0.3804 - acc: 0.84 - ETA: 45s - loss: 0.3896 - acc: 0.84 - ETA: 44s - loss: 0.3912 - acc: 0.84 - ETA: 43s - loss: 0.3886 - acc: 0.84 - ETA: 42s - loss: 0.3914 - acc: 0.83 - ETA: 42s - loss: 0.3921 - acc: 0.83 - ETA: 41s - loss: 0.4006 - acc: 0.83 - ETA: 40s - loss: 0.4010 - acc: 0.83 - ETA: 39s - loss: 0.4033 - acc: 0.83 - ETA: 39s - loss: 0.4068 - acc: 0.83 - ETA: 38s - loss: 0.4085 - acc: 0.83 - ETA: 37s - loss: 0.4145 - acc: 0.82 - ETA: 36s - loss: 0.4106 - acc: 0.83 - ETA: 36s - loss: 0.4076 - acc: 0.83 - ETA: 35s - loss: 0.4073 - acc: 0.83 - ETA: 34s - loss: 0.4047 - acc: 0.83 - ETA: 33s - loss: 0.4019 - acc: 0.83 - ETA: 33s - loss: 0.3970 - acc: 0.83 - ETA: 32s - loss: 0.4067 - acc: 0.83 - ETA: 31s - loss: 0.4074 - acc: 0.83 - ETA: 30s - loss: 0.4057 - acc: 0.83 - ETA: 30s - loss: 0.4063 - acc: 0.83 - ETA: 29s - loss: 0.4056 - acc: 0.83 - ETA: 28s - loss: 0.4063 - acc: 0.83 - ETA: 27s - loss: 0.4051 - acc: 0.83 - ETA: 27s - loss: 0.4038 - acc: 0.83 - ETA: 26s - loss: 0.4050 - acc: 0.83 - ETA: 25s - loss: 0.4043 - acc: 0.83 - ETA: 24s - loss: 0.4027 - acc: 0.83 - ETA: 24s - loss: 0.4029 - acc: 0.83 - ETA: 23s - loss: 0.4017 - acc: 0.83 - ETA: 22s - loss: 0.3993 - acc: 0.83 - ETA: 21s - loss: 0.4000 - acc: 0.83 - ETA: 21s - loss: 0.3978 - acc: 0.83 - ETA: 20s - loss: 0.3982 - acc: 0.83 - ETA: 19s - loss: 0.3955 - acc: 0.84 - ETA: 18s - loss: 0.3954 - acc: 0.84 - ETA: 18s - loss: 0.3948 - acc: 0.84 - ETA: 17s - loss: 0.3928 - acc: 0.84 - ETA: 16s - loss: 0.3916 - acc: 0.84 - ETA: 15s - loss: 0.3912 - acc: 0.84 - ETA: 15s - loss: 0.3898 - acc: 0.84 - ETA: 14s - loss: 0.3873 - acc: 0.84 - ETA: 13s - loss: 0.3850 - acc: 0.84 - ETA: 12s - loss: 0.3850 - acc: 0.84 - ETA: 12s - loss: 0.3836 - acc: 0.84 - ETA: 11s - loss: 0.3876 - acc: 0.84 - ETA: 10s - loss: 0.3892 - acc: 0.84 - ETA: 9s - loss: 0.3886 - acc: 0.8420 - ETA: 9s - loss: 0.3912 - acc: 0.841 - ETA: 8s - loss: 0.3893 - acc: 0.842 - ETA: 7s - loss: 0.3904 - acc: 0.842 - ETA: 6s - loss: 0.3927 - acc: 0.840 - ETA: 6s - loss: 0.3907 - acc: 0.841 - ETA: 5s - loss: 0.3884 - acc: 0.843 - ETA: 4s - loss: 0.3929 - acc: 0.843 - ETA: 3s - loss: 0.3912 - acc: 0.843 - ETA: 3s - loss: 0.3890 - acc: 0.844 - ETA: 2s - loss: 0.3884 - acc: 0.844 - ETA: 1s - loss: 0.3868 - acc: 0.845 - ETA: 0s - loss: 0.3893 - acc: 0.8460Epoch 00010: val_loss did not improve\n",
      "2000/2000 [==============================] - 80s - loss: 0.3895 - acc: 0.8455 - val_loss: 0.7794 - val_acc: 0.6800\n",
      "Epoch 12/20\n",
      "1980/2000 [============================>.] - ETA: 76s - loss: 0.3800 - acc: 0.85 - ETA: 74s - loss: 0.4481 - acc: 0.85 - ETA: 74s - loss: 0.3584 - acc: 0.88 - ETA: 73s - loss: 0.3178 - acc: 0.87 - ETA: 71s - loss: 0.3509 - acc: 0.85 - ETA: 71s - loss: 0.3892 - acc: 0.84 - ETA: 70s - loss: 0.4187 - acc: 0.84 - ETA: 69s - loss: 0.4074 - acc: 0.83 - ETA: 68s - loss: 0.3979 - acc: 0.84 - ETA: 68s - loss: 0.3778 - acc: 0.85 - ETA: 67s - loss: 0.3815 - acc: 0.85 - ETA: 66s - loss: 0.3691 - acc: 0.85 - ETA: 65s - loss: 0.3741 - acc: 0.85 - ETA: 64s - loss: 0.3637 - acc: 0.85 - ETA: 64s - loss: 0.3530 - acc: 0.86 - ETA: 63s - loss: 0.3406 - acc: 0.86 - ETA: 62s - loss: 0.3371 - acc: 0.86 - ETA: 62s - loss: 0.3427 - acc: 0.86 - ETA: 61s - loss: 0.3356 - acc: 0.86 - ETA: 60s - loss: 0.3279 - acc: 0.86 - ETA: 59s - loss: 0.3219 - acc: 0.87 - ETA: 58s - loss: 0.3216 - acc: 0.87 - ETA: 58s - loss: 0.3228 - acc: 0.86 - ETA: 57s - loss: 0.3210 - acc: 0.86 - ETA: 56s - loss: 0.3242 - acc: 0.86 - ETA: 56s - loss: 0.3237 - acc: 0.86 - ETA: 55s - loss: 0.3197 - acc: 0.86 - ETA: 54s - loss: 0.3115 - acc: 0.87 - ETA: 53s - loss: 0.3297 - acc: 0.86 - ETA: 52s - loss: 0.3489 - acc: 0.86 - ETA: 52s - loss: 0.3541 - acc: 0.85 - ETA: 51s - loss: 0.3557 - acc: 0.85 - ETA: 50s - loss: 0.3496 - acc: 0.85 - ETA: 49s - loss: 0.3464 - acc: 0.86 - ETA: 49s - loss: 0.3475 - acc: 0.86 - ETA: 48s - loss: 0.3417 - acc: 0.86 - ETA: 47s - loss: 0.3438 - acc: 0.86 - ETA: 46s - loss: 0.3429 - acc: 0.86 - ETA: 46s - loss: 0.3431 - acc: 0.86 - ETA: 45s - loss: 0.3479 - acc: 0.85 - ETA: 44s - loss: 0.3496 - acc: 0.85 - ETA: 44s - loss: 0.3499 - acc: 0.85 - ETA: 43s - loss: 0.3455 - acc: 0.86 - ETA: 42s - loss: 0.3475 - acc: 0.86 - ETA: 41s - loss: 0.3491 - acc: 0.85 - ETA: 40s - loss: 0.3526 - acc: 0.85 - ETA: 40s - loss: 0.3534 - acc: 0.85 - ETA: 39s - loss: 0.3500 - acc: 0.85 - ETA: 38s - loss: 0.3535 - acc: 0.85 - ETA: 38s - loss: 0.3520 - acc: 0.85 - ETA: 37s - loss: 0.3508 - acc: 0.85 - ETA: 36s - loss: 0.3488 - acc: 0.85 - ETA: 35s - loss: 0.3476 - acc: 0.85 - ETA: 34s - loss: 0.3519 - acc: 0.85 - ETA: 34s - loss: 0.3497 - acc: 0.85 - ETA: 33s - loss: 0.3504 - acc: 0.85 - ETA: 32s - loss: 0.3517 - acc: 0.85 - ETA: 31s - loss: 0.3488 - acc: 0.85 - ETA: 31s - loss: 0.3505 - acc: 0.85 - ETA: 30s - loss: 0.3495 - acc: 0.85 - ETA: 29s - loss: 0.3524 - acc: 0.85 - ETA: 28s - loss: 0.3618 - acc: 0.85 - ETA: 28s - loss: 0.3603 - acc: 0.85 - ETA: 27s - loss: 0.3571 - acc: 0.85 - ETA: 26s - loss: 0.3582 - acc: 0.85 - ETA: 25s - loss: 0.3547 - acc: 0.85 - ETA: 25s - loss: 0.3544 - acc: 0.85 - ETA: 24s - loss: 0.3537 - acc: 0.85 - ETA: 23s - loss: 0.3562 - acc: 0.85 - ETA: 22s - loss: 0.3550 - acc: 0.85 - ETA: 22s - loss: 0.3533 - acc: 0.85 - ETA: 21s - loss: 0.3523 - acc: 0.85 - ETA: 20s - loss: 0.3531 - acc: 0.85 - ETA: 19s - loss: 0.3598 - acc: 0.85 - ETA: 19s - loss: 0.3606 - acc: 0.85 - ETA: 18s - loss: 0.3637 - acc: 0.85 - ETA: 17s - loss: 0.3612 - acc: 0.85 - ETA: 16s - loss: 0.3582 - acc: 0.85 - ETA: 15s - loss: 0.3588 - acc: 0.85 - ETA: 15s - loss: 0.3576 - acc: 0.86 - ETA: 14s - loss: 0.3569 - acc: 0.86 - ETA: 13s - loss: 0.3554 - acc: 0.86 - ETA: 12s - loss: 0.3580 - acc: 0.85 - ETA: 12s - loss: 0.3590 - acc: 0.85 - ETA: 11s - loss: 0.3582 - acc: 0.85 - ETA: 10s - loss: 0.3585 - acc: 0.85 - ETA: 9s - loss: 0.3586 - acc: 0.8580 - ETA: 9s - loss: 0.3571 - acc: 0.858 - ETA: 8s - loss: 0.3574 - acc: 0.856 - ETA: 7s - loss: 0.3590 - acc: 0.856 - ETA: 6s - loss: 0.3625 - acc: 0.854 - ETA: 6s - loss: 0.3642 - acc: 0.854 - ETA: 5s - loss: 0.3627 - acc: 0.855 - ETA: 4s - loss: 0.3609 - acc: 0.855 - ETA: 3s - loss: 0.3609 - acc: 0.854 - ETA: 3s - loss: 0.3602 - acc: 0.854 - ETA: 2s - loss: 0.3595 - acc: 0.855 - ETA: 1s - loss: 0.3603 - acc: 0.855 - ETA: 0s - loss: 0.3614 - acc: 0.8551Epoch 00011: val_loss did not improve\n",
      "2000/2000 [==============================] - 81s - loss: 0.3629 - acc: 0.8550 - val_loss: 0.8085 - val_acc: 0.6867\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/2000 [============================>.] - ETA: 74s - loss: 0.3592 - acc: 0.85 - ETA: 74s - loss: 0.3111 - acc: 0.87 - ETA: 68s - loss: 0.3477 - acc: 0.85 - ETA: 69s - loss: 0.3005 - acc: 0.88 - ETA: 69s - loss: 0.3135 - acc: 0.89 - ETA: 68s - loss: 0.3359 - acc: 0.87 - ETA: 68s - loss: 0.3186 - acc: 0.87 - ETA: 68s - loss: 0.3794 - acc: 0.86 - ETA: 67s - loss: 0.3808 - acc: 0.87 - ETA: 67s - loss: 0.3766 - acc: 0.87 - ETA: 66s - loss: 0.3704 - acc: 0.86 - ETA: 65s - loss: 0.3511 - acc: 0.87 - ETA: 65s - loss: 0.3585 - acc: 0.87 - ETA: 64s - loss: 0.3479 - acc: 0.87 - ETA: 63s - loss: 0.3373 - acc: 0.88 - ETA: 63s - loss: 0.3244 - acc: 0.88 - ETA: 62s - loss: 0.3402 - acc: 0.87 - ETA: 61s - loss: 0.3298 - acc: 0.88 - ETA: 60s - loss: 0.3330 - acc: 0.88 - ETA: 60s - loss: 0.3335 - acc: 0.88 - ETA: 59s - loss: 0.3425 - acc: 0.87 - ETA: 58s - loss: 0.3393 - acc: 0.87 - ETA: 57s - loss: 0.3451 - acc: 0.87 - ETA: 57s - loss: 0.3417 - acc: 0.87 - ETA: 57s - loss: 0.3603 - acc: 0.86 - ETA: 56s - loss: 0.3786 - acc: 0.86 - ETA: 55s - loss: 0.3720 - acc: 0.86 - ETA: 54s - loss: 0.3633 - acc: 0.87 - ETA: 54s - loss: 0.3730 - acc: 0.86 - ETA: 53s - loss: 0.3685 - acc: 0.87 - ETA: 52s - loss: 0.3612 - acc: 0.87 - ETA: 51s - loss: 0.3637 - acc: 0.86 - ETA: 51s - loss: 0.3602 - acc: 0.87 - ETA: 50s - loss: 0.3624 - acc: 0.86 - ETA: 49s - loss: 0.3610 - acc: 0.86 - ETA: 48s - loss: 0.3571 - acc: 0.86 - ETA: 47s - loss: 0.3537 - acc: 0.87 - ETA: 47s - loss: 0.3518 - acc: 0.87 - ETA: 46s - loss: 0.3510 - acc: 0.87 - ETA: 45s - loss: 0.3452 - acc: 0.87 - ETA: 44s - loss: 0.3426 - acc: 0.87 - ETA: 44s - loss: 0.3437 - acc: 0.87 - ETA: 43s - loss: 0.3463 - acc: 0.87 - ETA: 42s - loss: 0.3459 - acc: 0.87 - ETA: 42s - loss: 0.3428 - acc: 0.87 - ETA: 42s - loss: 0.3412 - acc: 0.87 - ETA: 41s - loss: 0.3455 - acc: 0.87 - ETA: 40s - loss: 0.3511 - acc: 0.86 - ETA: 39s - loss: 0.3513 - acc: 0.86 - ETA: 38s - loss: 0.3495 - acc: 0.86 - ETA: 38s - loss: 0.3483 - acc: 0.86 - ETA: 37s - loss: 0.3488 - acc: 0.86 - ETA: 36s - loss: 0.3487 - acc: 0.86 - ETA: 35s - loss: 0.3463 - acc: 0.86 - ETA: 34s - loss: 0.3460 - acc: 0.87 - ETA: 34s - loss: 0.3438 - acc: 0.87 - ETA: 33s - loss: 0.3428 - acc: 0.87 - ETA: 32s - loss: 0.3389 - acc: 0.87 - ETA: 31s - loss: 0.3384 - acc: 0.87 - ETA: 30s - loss: 0.3377 - acc: 0.87 - ETA: 30s - loss: 0.3405 - acc: 0.87 - ETA: 29s - loss: 0.3380 - acc: 0.87 - ETA: 28s - loss: 0.3369 - acc: 0.87 - ETA: 27s - loss: 0.3382 - acc: 0.87 - ETA: 27s - loss: 0.3399 - acc: 0.87 - ETA: 26s - loss: 0.3396 - acc: 0.87 - ETA: 25s - loss: 0.3368 - acc: 0.87 - ETA: 24s - loss: 0.3345 - acc: 0.87 - ETA: 23s - loss: 0.3349 - acc: 0.87 - ETA: 23s - loss: 0.3341 - acc: 0.87 - ETA: 22s - loss: 0.3350 - acc: 0.87 - ETA: 21s - loss: 0.3355 - acc: 0.87 - ETA: 20s - loss: 0.3356 - acc: 0.87 - ETA: 20s - loss: 0.3362 - acc: 0.87 - ETA: 19s - loss: 0.3360 - acc: 0.87 - ETA: 18s - loss: 0.3386 - acc: 0.87 - ETA: 17s - loss: 0.3380 - acc: 0.87 - ETA: 16s - loss: 0.3384 - acc: 0.87 - ETA: 16s - loss: 0.3386 - acc: 0.87 - ETA: 15s - loss: 0.3396 - acc: 0.87 - ETA: 14s - loss: 0.3401 - acc: 0.87 - ETA: 13s - loss: 0.3398 - acc: 0.87 - ETA: 13s - loss: 0.3406 - acc: 0.86 - ETA: 12s - loss: 0.3401 - acc: 0.86 - ETA: 11s - loss: 0.3382 - acc: 0.87 - ETA: 10s - loss: 0.3393 - acc: 0.86 - ETA: 9s - loss: 0.3393 - acc: 0.8701 - ETA: 9s - loss: 0.3380 - acc: 0.870 - ETA: 8s - loss: 0.3382 - acc: 0.870 - ETA: 7s - loss: 0.3389 - acc: 0.870 - ETA: 6s - loss: 0.3388 - acc: 0.870 - ETA: 6s - loss: 0.3373 - acc: 0.871 - ETA: 5s - loss: 0.3376 - acc: 0.871 - ETA: 4s - loss: 0.3458 - acc: 0.867 - ETA: 3s - loss: 0.3463 - acc: 0.867 - ETA: 3s - loss: 0.3472 - acc: 0.866 - ETA: 2s - loss: 0.3462 - acc: 0.867 - ETA: 1s - loss: 0.3449 - acc: 0.867 - ETA: 0s - loss: 0.3434 - acc: 0.8682Epoch 00012: val_loss did not improve\n",
      "2000/2000 [==============================] - 81s - loss: 0.3414 - acc: 0.8695 - val_loss: 1.0951 - val_acc: 0.7133\n",
      "Epoch 14/20\n",
      "1220/2000 [=================>............] - ETA: 74s - loss: 0.5031 - acc: 0.80 - ETA: 74s - loss: 0.3370 - acc: 0.87 - ETA: 73s - loss: 0.2916 - acc: 0.88 - ETA: 72s - loss: 0.2924 - acc: 0.86 - ETA: 72s - loss: 0.3160 - acc: 0.85 - ETA: 71s - loss: 0.3354 - acc: 0.83 - ETA: 70s - loss: 0.3519 - acc: 0.84 - ETA: 69s - loss: 0.3339 - acc: 0.86 - ETA: 69s - loss: 0.3336 - acc: 0.86 - ETA: 68s - loss: 0.3416 - acc: 0.86 - ETA: 67s - loss: 0.3456 - acc: 0.85 - ETA: 66s - loss: 0.3428 - acc: 0.86 - ETA: 70s - loss: 0.3265 - acc: 0.86 - ETA: 68s - loss: 0.3224 - acc: 0.86 - ETA: 67s - loss: 0.3100 - acc: 0.87 - ETA: 66s - loss: 0.3131 - acc: 0.86 - ETA: 65s - loss: 0.3073 - acc: 0.86 - ETA: 65s - loss: 0.3107 - acc: 0.86 - ETA: 64s - loss: 0.3108 - acc: 0.86 - ETA: 63s - loss: 0.3030 - acc: 0.87 - ETA: 62s - loss: 0.2975 - acc: 0.87 - ETA: 61s - loss: 0.2923 - acc: 0.87 - ETA: 60s - loss: 0.2954 - acc: 0.87 - ETA: 59s - loss: 0.3012 - acc: 0.86 - ETA: 58s - loss: 0.2970 - acc: 0.86 - ETA: 57s - loss: 0.3050 - acc: 0.86 - ETA: 57s - loss: 0.3088 - acc: 0.85 - ETA: 56s - loss: 0.3056 - acc: 0.86 - ETA: 55s - loss: 0.3084 - acc: 0.85 - ETA: 54s - loss: 0.3099 - acc: 0.85 - ETA: 53s - loss: 0.3037 - acc: 0.86 - ETA: 52s - loss: 0.3076 - acc: 0.86 - ETA: 52s - loss: 0.3209 - acc: 0.85 - ETA: 51s - loss: 0.3212 - acc: 0.85 - ETA: 50s - loss: 0.3222 - acc: 0.85 - ETA: 49s - loss: 0.3207 - acc: 0.85 - ETA: 48s - loss: 0.3224 - acc: 0.85 - ETA: 48s - loss: 0.3238 - acc: 0.85 - ETA: 47s - loss: 0.3244 - acc: 0.85 - ETA: 46s - loss: 0.3214 - acc: 0.85 - ETA: 45s - loss: 0.3200 - acc: 0.85 - ETA: 44s - loss: 0.3210 - acc: 0.85 - ETA: 44s - loss: 0.3211 - acc: 0.85 - ETA: 43s - loss: 0.3263 - acc: 0.85 - ETA: 42s - loss: 0.3231 - acc: 0.86 - ETA: 41s - loss: 0.3243 - acc: 0.86 - ETA: 40s - loss: 0.3215 - acc: 0.86 - ETA: 40s - loss: 0.3179 - acc: 0.86 - ETA: 39s - loss: 0.3175 - acc: 0.86 - ETA: 38s - loss: 0.3201 - acc: 0.86 - ETA: 37s - loss: 0.3184 - acc: 0.86 - ETA: 37s - loss: 0.3157 - acc: 0.86 - ETA: 36s - loss: 0.3194 - acc: 0.86 - ETA: 35s - loss: 0.3219 - acc: 0.86 - ETA: 34s - loss: 0.3209 - acc: 0.86 - ETA: 33s - loss: 0.3197 - acc: 0.86 - ETA: 33s - loss: 0.3211 - acc: 0.86 - ETA: 32s - loss: 0.3200 - acc: 0.86 - ETA: 31s - loss: 0.3171 - acc: 0.86 - ETA: 30s - loss: 0.3164 - acc: 0.86 - ETA: 30s - loss: 0.3137 - acc: 0.8697"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-2047d5592718>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             verbose=1)\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DLND_L2\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 867\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DLND_L2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1598\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DLND_L2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DLND_L2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2273\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2274\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DLND_L2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DLND_L2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DLND_L2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DLND_L2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DLND_L2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "epochs = 20\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights1.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_labels,\n",
    "            validation_data=(valid_tensors, valid_labels),\n",
    "            epochs=epochs,\n",
    "            batch_size=20,\n",
    "            callbacks=[checkpointer],\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create output .csv for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the model weights with the best validation loss.\n",
    "\n",
    "model.load_weights('saved_models/weights1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 600/600 [00:13<00:00, 44.28it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "y_pred = pd.DataFrame(columns=[\"task_1\", \"task_2\"])\n",
    "\n",
    "for ii in tqdm(range(len(test_labels))):\n",
    "    #path = test_files[ii]\n",
    "    prediction = np.argmax(model.predict(np.expand_dims(test_tensors[ii], axis=0)))\n",
    "    if prediction == 0:\n",
    "        y_pred.loc[ii] = [1, 0]\n",
    "    if prediction == 2:\n",
    "        y_pred.loc[ii] = [0, 1]\n",
    "    else:\n",
    "        y_pred.loc[ii] = [0, 0]\n",
    "\n",
    "y_pred.to_csv(\"predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Internal testing vs. ground truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def plot_roc_auc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    This function plots the ROC curves and provides the scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize dictionaries and array\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = np.zeros(3)\n",
    "    \n",
    "    # prepare for figure\n",
    "    plt.figure()\n",
    "    colors = ['aqua', 'cornflowerblue']\n",
    "\n",
    "    # for both classification tasks (categories 1 and 2)\n",
    "    for i in range(2):\n",
    "        # obtain ROC curve\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true[:,i], y_pred[:,i])\n",
    "        # obtain ROC AUC\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        # plot ROC curve\n",
    "        plt.plot(fpr[i], tpr[i], color=colors[i], lw=2,\n",
    "                 label='ROC curve for task {d} (area = {f:.2f})'.format(d=i+1, f=roc_auc[i]))\n",
    "    # get score for category 3\n",
    "    roc_auc[2] = np.average(roc_auc[:2])\n",
    "    \n",
    "    # format figure\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curves')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    # print scores\n",
    "    for i in range(3):\n",
    "        print('Category {d} Score: {f:.3f}'. format(d=i+1, f=roc_auc[i]))\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, thresh, classes):\n",
    "    \"\"\"\n",
    "    This function plots the (normalized) confusion matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    # obtain class predictions from probabilities\n",
    "    y_pred = (y_pred>=thresh)*1\n",
    "    # obtain (unnormalized) confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # normalize confusion matrix\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], '.2f'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUDjnnEvJC4VzzrmUvFA455xL6f9vtxQD75o2XwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13959239438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 1 Score: 0.500\n",
      "Category 2 Score: 0.745\n",
      "Category 3 Score: 0.622\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVMAAAEmCAYAAADfpHMGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xm8VVX9//HX+94LKjGZIMpFRQVFMEdAM00qZ1EyJ9Qy\nhzT9Sv7Sn6WZZQ6l1rdvaaiI5pQ5l4GKYvUL+zogoOKAI6gkFxxwwBkZPr8/9r54uHLvOZezzz3n\n3Pt+9tgPz9l77bU/B/PDWmuvvbYiAjMzK05NuQMwM2sPnEzNzDLgZGpmlgEnUzOzDDiZmpllwMnU\nzCwDTqZWUpLWknSnpEWSbiuiniMk3ZdlbOUiaRdJz5c7DsuWPM/UACQdDpwKDALeB2YCv4yIB4qs\n9zvAD4CdImJp0YFWOEkBDIyI2eWOxdqWW6aGpFOB3wO/AvoAGwKXAvtnUP1GwAsdIZEWQlJduWOw\nEokIbx14A3oAHwAHt1BmDZJkOz/dfg+skR4bAcwD/i/wBrAAODo9dg7wKbAkvcaxwC+AG3Lq7g8E\nUJd+Pwp4iaR1/DJwRM7+B3LO2wmYDixK/7lTzrEpwHnAg2k99wG9mvltjfH/OCf+bwL7AC8AbwNn\n5pQfDjwMvJuWHQt0To/9O/0tH6a/99Cc+k8HXgP+1LgvPWfT9Brbpd/7Am8CI8r9/w1vrdvcMrUv\nA2sCd7RQ5qfAjsA2wNYkCeWsnOPrkSTlepKEeamktSPibJLW7i0R0TUi/thSIJK+AFwC7B0R3UgS\n5sxVlPsicHdadh3gf4C7Ja2TU+xw4GhgXaAzcFoLl16P5M+gHvg5cCXwbWB7YBfgZ5I2TssuA04B\nepH82X0D+C+AiPhqWmbr9PfeklP/F0la6cfnXjgi5pAk2hskdQGuAa6LiCktxGsVyMnU1gEWRsvd\n8COAcyPijYh4k6TF+Z2c40vS40siYhJJq2zz1YxnObClpLUiYkFEzFpFmX2BFyPiTxGxNCJuAp4D\n9sspc01EvBARHwO3kvxF0JwlJOPDS4CbSRLlxRHxfnr9Z0j+EiEiHo2Iqel1XwGuAHYt4DedHRGL\n03hWEhFXArOBR4D1Sf7ysirjZGpvAb3yjOX1BebmfJ+b7ltRR5Nk/BHQtbWBRMSHJF3jE4AFku6W\nNKiAeBpjqs/5/lor4nkrIpalnxuT3es5xz9uPF/SZpLukvSapPdIWt69Wqgb4M2I+CRPmSuBLYE/\nRMTiPGWtAjmZ2sPAYpJxwubMJ+miNtow3bc6PgS65HxfL/dgREyOiN1JWmjPkSSZfPE0xtSwmjG1\nxuUkcQ2MiO7AmYDynNPilBlJXUnGof8I/CIdxrAq42TawUXEIpJxwkslfVNSF0mdJO0t6ddpsZuA\nsyT1ltQrLX/Dal5yJvBVSRtK6gH8pPGApD6SRqVjp4tJhguWr6KOScBmkg6XVCfpUGAwcNdqxtQa\n3YD3gA/SVvOJTY6/DmzSyjovBmZExPdIxoLHFR2ltTknUyMifksyx/QskjvJrwJjgL+lRc4HZgBP\nAk8Bj6X7VudafwduSet6lJUTYE0ax3ySO9y78vlkRUS8BYwkmUHwFsmd+JERsXB1Ymql00hubr1P\n0mq+pcnxXwDXSXpX0iH5KpM0CtiLz37nqcB2ko7ILGJrE560b2aWAbdMzcwy4GRqZh2OpKslvSHp\n6WaOS9IlkmZLelLSdvnqdDI1s47oWpKx6ubsDQxMt+NJZnG0yMnUzDqciPg3yU3O5owCro/EVKCn\npPVbqtOLLhRBdWuFOncrdxiW2naLDcsdguWYO/cVFi5cmG8ObqvUdt8oYunnHiL7nPj4zVlA7oMS\n4yNifCsuVU8yq6XRvHTfguZOcDItgjp3Y43N885+sTby4CNjyx2C5fjKDkMzrzOWflzQf3OfzLz0\nk4jIPoAWOJmaWfWQoKa2La7UAGyQ870feZ6w85ipmVUX1eTfijcRODK9q78jsCgimu3ig1umZlZt\nVPwwrKSbSNaV7SVpHnA20AkgIsaRPLK8D8lqXh+RLOfYIidTM6siyqTlGRGH5TkewEmtqdPJ1Myq\nh2irMdNWczI1syqiTLr5peBkambVJZsbTJlzMjWz6uKWqZlZkdpunmmrOZmaWXVxN9/MrFjZTI0q\nBSdTM6seAmrdzTczK55vQJmZFcvdfDOzbLhlamZWJE+NMjPLiLv5ZmYZcDffzKxY7uabmRVPuJtv\nZlY8T40yM8uGx0zNzDLgMVMzsyLJ3Xwzs2y4m29mVjw5mZqZFSfp5TuZmpkVSW6ZmpllwcnUzCwD\nNTW+m29mVhylWwVyMjWzqiGPmZqZZcPJ1MwsAx4zNTMrVgWPmVZmijcza4akvFsBdewl6XlJsyWd\nsYrjPSTdKekJSbMkHZ2vTrdMzaxqCBXdzZdUC1wK7A7MA6ZLmhgRz+QUOwl4JiL2k9QbeF7SnyPi\n0+bqdcvUzKqLCthaNhyYHREvpcnxZmBUkzIBdFPSzO0KvA0sbalSt0zNrHook7v59cCrOd/nATs0\nKTMWmAjMB7oBh0bE8pYqdcvUzKpKgWOmvSTNyNmOb+Vl9gRmAn2BbYCxkrq3dIJbpmZWNVoxZrow\nIoY2c6wB2CDne790X66jgQsjIoDZkl4GBgHTmrugW6bt2Lizj2DuPy9gxm1nNlvmtz8+iKcnnM20\nW37CNoP6rdi/+05b8MQdP+PpCWdz2tG7t0W4HcJ9k+9lqyGbM2TQAH7z6ws/dzwiOPWHJzNk0ACG\nbbsVjz/2WMHndhjFj5lOBwZK2lhSZ2A0SZc+13+AbwBI6gNsDrzUUqVOpu3Yn+6cyqiTLm32+J47\nD2bTDXuz5ahzGHP+TVxy5mgAamrE7884hFFjLmPbA8/n4L22Z9Am67VV2O3WsmXL+OHJJzHhznt4\n/MlnuO3mm3j2mWdWKjP53nuYM/tFnn72RcZePp6Tx5xY8LkdgoqfGhURS4ExwGTgWeDWiJgl6QRJ\nJ6TFzgN2kvQU8E/g9IhY2FK97ua3Yw8+NocN1/9is8dH7roVN96V9FqmPfUKPbqtxXq9urNR33WY\n8+pCXml4C4DbJj/GyBFb8dxLr7VJ3O3V9GnT2HTTAWy8ySYAHHzoaO66cwJbDB68osxdEydw+LeP\nRBI77Lgjixa9y4IFC5j7yit5z+0osnicNCImAZOa7BuX83k+sEdr6nTLtAPru25P5r32zorvDa+/\nS991e9J33R7Mez13/zvU9+5RjhDblfnzG+jX77Ohuvr6fjQ0NOQtM7+hoaBzOwrVKO9WDhWbTCX1\nl/R0BvUMlXRJFjGZWfll8QRUKbT7bn5EzABmlDuOSjT/jXfpt97aK77X9+nJ/DfepVNdLf365O5f\nm4Y3F5UjxHalb9965s37bHpjQ8M86uvr85bpW1/PkiVL8p7bEZQzWeZTsS3TVJ2kP0t6VtLtkrpI\n2l7S/ZIelTRZ0voAkqZIukjSNEkvSNol3T9C0l3p596S/p4+a3uVpLmSeqWt4GclXZkeu0/SWuX8\n4W3h7vuf4vCRwwEY/qX+vPfBx7y28D1mzJrLgA17s1HfdehUV8vBe27H3VOeLHO01W/osGHMnv0i\nr7z8Mp9++im33XIz+47cf6Uy++63PzfecD0RwSNTp9K9ew/WX3/9gs7tKGpqavJu5VDpLdPNgWMj\n4kFJV5M8L3sAMCoi3pR0KPBL4Ji0fF1EDJe0D3A2sFuT+s4G/l9EXCBpL+DYnGMDgcMi4jhJtwIH\nAjeU7qeV3nUXHMUu2w+kV8+uzL73PM4bN4lOdbUAXHX7A9z7wCz23HkIsyaezUefLOH7v0h+7rJl\nyznlolu587KTqK0R102YyrO++VS0uro6fnfxWPbbd0+WLVvGd486hsFDhnDlFcl9j+O+fwJ77b0P\nk++ZxJBBA+iyVheuuOqaFs/tkCqzYYqSOamVR1J/4N8RsWH6/evAmSTP1TbO96oFFkTEHpKmAD9N\nE28f4MGIGCBpBHBaRIyUNBM4ICJeTut8G9iM5Nnbv0fEwHT/6UCniDh/FXEdDyRPU3Tquv2aQ75b\nip9vq+Gd6WPLHYLl+MoOQ3n00RmZpr41+gyM+iMuzlvu5d/t+2gLk/ZLotJbpk0z/fvArIj4cjPl\nF6f/XEbrf9vinM/LgFV28yNiPDAeoKbLupX5N5FZe5XNs/klUeljphtKakychwNTgd6N+yR1ktSa\nvs6DwCHpuXsAa7dc3MwqSfI4af6tHCo9mT4PnCTpWZLE9wfgIOAiSU+QLESwUyvqOwfYI51ydTDw\nGklr18yqhJR/K4eK7eZHxCskCws0NRP46irKj8j5vBDon36eAkxJDy0C9oyIpWnrdlhELAZeAbbM\nOf+/i/8FZlYKldrNr9hkWiIbArdKqgE+BY4rczxm1hplbHnm06GSaUS8CGxb7jjMbPUIqK2tzGza\noZKpmVU/d/PNzIrlbr6ZWfGyeDtpqTiZmllVccvUzCwDHjM1MyuWx0zNzIonKNvjovk4mZpZVXE3\n38wsAxWaS51Mzax6SO7mm5lloHLfAeVkamZVpUJzqZOpmVUXt0zNzIrkMVMzs4y4ZWpmloEKzaVO\npmZWXdwyNTMrklS+t4/m42RqZlWlQhumzb/qWVL3lra2DNLMrFGNlHfLR9Jekp6XNFvSGc2UGSFp\npqRZku7PV2dLLdNZQJAs1NKo8XuQvOnTzKzNZDE1SlItcCmwOzAPmC5pYkQ8k1OmJ3AZsFdE/EfS\nuvnqbTaZRsQGRUVsZlYCGQyZDgdmR8RLAJJuBkYBz+SUORz4a0T8ByAi3sgbVyFXljRa0pnp536S\ntm9l8GZmmZCUd8ujHng15/u8dF+uzYC1JU2R9KikI/NVmvcGlKSxQCfgq8CvgI+AccCwfOeamWWt\nwBtQvSTNyPk+PiLGt+IydcD2wDeAtYCHJU2NiBdaOiGfnSJiO0mPA0TE25I6tyIoM7NMCKgtLJsu\njIihzRxrAHKHMful+3LNA96KiA+BDyX9G9gaaDaZFtLNXyKphuSmE5LWAZYXcJ6ZWbYK6OIX0M2f\nDgyUtHHaMBwNTGxSZgKws6Q6SV2AHYBnW6q0kJbppcBfgN6SzgEOAc4p4Dwzs8wVO880IpZKGgNM\nBmqBqyNilqQT0uPjIuJZSfcCT5I0Hq+KiKdbqjdvMo2I6yU9CuyW7jo4X6VmZqUgoDaD2/kRMQmY\n1GTfuCbffwP8ptA6C30CqhZYQtLVL2gGgJlZKVTqs/l5E6OknwI3AX1JBmpvlPSTUgdmZtaUVNhW\nDoW0TI8Eto2IjwAk/RJ4HLiglIGZma1KIY+LlkMhyXRBk3J16T4zszZXdclU0u9IxkjfBmZJmpx+\n34NkaoGZWZsSmTxOWhIttUwb79jPAu7O2T+1dOGYmbWgsHmkZdHSQid/bMtAzMwKUaG5tKBn8zcF\nfgkMBtZs3B8Rm5UwLjOzz8lqnmkpFDJn9FrgGpLfsTdwK3BLCWMyM2tWBo+TlkQhybRLREwGiIg5\nEXEWSVI1M2tzKmArh0KmRi1OFzqZkz672gB0K21YZmafJ1VuN7+QZHoK8AXgZJKx0x7AMaUMysys\nOVV3N79RRDySfnwf+E5pwzEza1mF5tIWJ+3fQbqG6apExLdKEpGZWTNEYW8fLYeWWqZj2ywKM7NC\nZPB20lJpadL+P9syEDOzQlTqGqCFrmdqZlZ2oopvQJmZVZIK7eUXnkwlrRERi0sZjJlZSyp5nmkh\nK+0Pl/QU8GL6fWtJfyh5ZGZmq1Cj/FtZ4iqgzCXASOAtgIh4AvhaKYMyM2tONb+2pCYi5jYZ9F1W\nonjMzJoloK6Kb0C9Kmk4EJJqgR8AL5Q2LDOzVavQXFpQMj2RpKu/IfA68I90n5lZm5Kq8wkoACLi\nDWB0G8RiZpZXhebSglbav5JVPKMfEceXJCIzs2YIqKvQqVGFdPP/kfN5TeAA4NXShGNm1rKqbZlG\nxEqvKJH0J+CBkkVkZtacMs4jzWd1HifdGOiTdSBmZvkIqK3QpmkhY6bv8NmYaQ3wNnBGKYMyM2tO\nVbZMlczU35rkvU8AyyOi2QWjzcxKrVJXjWrxcdI0cU6KiGXp5kRqZmUjqvvZ/JmSti15JGZm+aSr\nRuXb8lYj7SXpeUmzJTU7bClpmKSlkg7KV2dL74Cqi4ilwLbAdElzgA+Tn0NExHZ5IzYzy1Bjy7So\nOpLH4i8FdgfmkeS3iRHxzCrKXQTcV0i9LY2ZTgO2A/ZfrYjNzEoggyHT4cDsiHgpqU83A6OAZ5qU\n+wHwF2BYIZW2lEwFEBFzWh2qmVlJiBoKyqa9JM3I+T4+Isann+tZ+cGjecAOK11Fqid5QOlrZJBM\ne0s6tbmDEfE/hVzAzCwryUr7BRVdGBFDi7jU74HTI2J5obMHWkqmtUBXKOyvATOztpDBqlENwAY5\n3/vx2fTPRkOBm9NE2gvYR9LSiPhbc5W2lEwXRMS5qxmsmVnmkreTFl3NdGCgpI1Jkuho4PDcAhGx\n8YprStcCd7WUSKGAMVMzs0pS7Av1ImKppDHAZJIe+NURMUvSCenxcatTb0vJ9BurU6GZWamIwibH\n5xMRk4BJTfatMolGxFGF1NlsMo2It1sTnJlZyalyHyddnVWjzMzKpjJTqZOpmVWRql6Cz8ysklRo\nLnUyNbNqIo+ZmpkVy918M7OMVGYqdTI1s2riqVFmZsXLatJ+KTiZmllVyWChk5JwMjWzqlKhudTJ\n1MyqR9LNr8xs6mRqZlXFLVMzs6LJY6ZmZsVyN9/MLAtyN9/MLBOV2s2v1PmvloFxZx/B3H9ewIzb\nzmy2zG9/fBBPTzibabf8hG0G9Vuxf/edtuCJO37G0xPO5rSjd2+LcDuE+ybfy1ZDNmfIoAH85tcX\nfu54RHDqD09myKABDNt2Kx5/7LGCz+0IBNQo/1YOTqbt2J/unMqoky5t9vieOw9m0w17s+Wocxhz\n/k1ccuZoAGpqxO/POIRRYy5j2wPP5+C9tmfQJuu1Vdjt1rJly/jhyScx4c57ePzJZ7jt5pt49pln\nVioz+d57mDP7RZ5+9kXGXj6ek8ecWPC5HYUK+F85OJm2Yw8+Noe3F33U7PGRu27FjXdNA2DaU6/Q\no9tarNerO8O27M+cVxfySsNbLFm6jNsmP8bIEVu1Vdjt1vRp09h00wFsvMkmdO7cmYMPHc1dd05Y\nqcxdEydw+LePRBI77Lgjixa9y4IFCwo6t6OQ8m/l4GTagfVdtyfzXntnxfeG19+l77o96btuD+a9\nnrv/Hep79yhHiO3K/PkN9Ov32eva6+v70dDQkLfM/IaGgs7tCBqX4Mu3lUNFJlNJIyTdlX7eX9IZ\nbXjtbSTt01bXM7PWKKSTX55kWvF38yNiIjCxDS+5DTCUJq+BbY/mv/Eu/dZbe8X3+j49mf/Gu3Sq\nq6Vfn9z9a9Pw5qJyhNiu9O1bz7x5r6743tAwj/r6+rxl+tbXs2TJkrzndggVPDWqZC1TSf0lPSfp\nWkkvSPqzpN0kPSjpRUnD0+1hSY9LekjS5quo5yhJY9PPm0qaKukpSedL+iDdP0LSFEm3p9f8s9JF\nDyX9XNJ0SU9LGp+zf4qkiyRNS+PbRVJn4FzgUEkzJR1aqj+fSnD3/U9x+MjhAAz/Un/e++BjXlv4\nHjNmzWXAhr3ZqO86dKqr5eA9t+PuKU+WOdrqN3TYMGbPfpFXXn6ZTz/9lNtuuZl9R+6/Upl999uf\nG2+4nojgkalT6d69B+uvv35B53YUKmArh1K3TAcABwPHANOBw4Gdgf2BM4EjgV0iYqmk3YBfAQe2\nUN/FwMURcZOkE5oc2xYYAswHHgS+AjwAjI2IcwEk/QkYCdyZnlMXEcPTbv3ZEbGbpJ8DQyNizKoC\nkHQ8cDwAnboW/AdRDtddcBS7bD+QXj27Mvve8zhv3CQ61dUCcNXtD3DvA7PYc+chzJp4Nh99soTv\n/+IGAJYtW84pF93KnZedRG2NuG7CVJ596bVy/pR2oa6ujt9dPJb99t2TZcuW8d2jjmHwkCFcecU4\nAI77/gnstfc+TL5nEkMGDaDLWl244qprWjy3o6nk15YoIkpTsdQf+HtEDEy/Xw9Mjog/S9oE+Cuw\nH3AJMBAIoFNEDJI0AjgtIkZKOoo0uUl6C+iTJt/uwPyI6JqW/2lE7J5e63LgwYi4QdKBwI+BLsAX\ngT9ExIWSpqTnPCipT1p+QO718v3Gmi7rxhqbH5LFH5dl4J3pY8sdguX4yg5DefTRGZlmvi2+tG1c\n87d/5S335QFrPxoRQ7O8dj6lvgG1OOfz8pzvy0laxecB/4qILUkS65oZXWsZUCdpTeAy4KCI+BJw\nZZNrLM4tX8S1zayNVOoNqHLfze8BNM7vOKqA8lP5bBhgdAHlGxPnQkldgYMKOOd9oFsB5cysDPwE\n1Kr9GrhA0uMU1jL8IXCqpCdJxmNbvMUcEe+StEafBiaTjNvm8y9gcEe4AWVWlSr0DlTJxkxLQVIX\n4OOICEmjgcMiYlS54vGYaWXxmGllKcWY6eAvbRvXT7w/b7lhm/Ro8zHTahsn3B4Ym05vepdkloCZ\ndRQdcZ5pKUTE/0bE1hGxVUR8NSJmlzsmM2tbWTybL2kvSc9Lmr2qJywlHSHpyXRO+0OSts5XZ7W1\nTM2sQyv+br2kWuBSYHdgHjBd0sSIyF2G62Vg14h4R9LewHhgh5bqraqWqZlZBi3T4cDsiHgpIj4F\nbgZWuvcSEQ9FRONqP1OBfuThZGpmVUMUnEx7SZqRsx2fU0098GrO93npvuYcC9yTLzZ3882sqhTY\nzV+Yxd18SV8jSaY75yvrZGpmVSWDu/kNwAY53/vx2cNDOdfRVsBVwN4R8Va+St3NN7OqksGc/enA\nQEkbpyvFjabJMp+SNiRZP+Q7EfFCIXG5ZWpm1UOgIpum6UJJY0ieiqwFro6IWY0r0UXEOODnwDrA\nZen1luYbNnAyNbOq0XgDqlgRMYkmC8CnSbTx8/eA77WmTidTM6sqFfoAlJOpmVWZCs2mTqZmVlVq\nKvThfCdTM6sqlZlKnUzNrNpUaDZ1MjWzqiG5m29mlonKTKVOpmZWbSo0mzqZmlkVKd/bR/NxMjWz\nqiHK9/bRfJxMzay6OJmamRXP3Xwzswy4m29mVqwKftWzk6mZVZnKzKZOpmZWNbJaz7QUnEzNrKp4\nzNTMLAO+m29mloXKzKVOpmZWXSo0lzqZmln18BJ8ZmZZqcxc6mRqZtWlQnOpk6mZVRO5m29mVqxK\nnrRfU+4AzMzaA7dMzayqVGrL1MnUzKqHp0aZmRVP+G6+mVk2KjSbOpmaWVWp1IVOfDffzKpKjfJv\n+UjaS9LzkmZLOmMVxyXpkvT4k5K2yxvX6v0cM7MyUQFbS6dLtcClwN7AYOAwSYObFNsbGJhuxwOX\n5wvLydTMqooK+F8ew4HZEfFSRHwK3AyMalJmFHB9JKYCPSWt31KlHjMtQnz85sJPZl46t9xxZKAX\nsLDcQRRrrU6XljuErLSLfx/ARllX+Phjj07u0lm9Cii6pqQZOd/HR8T49HM98GrOsXnADk3OX1WZ\nemBBcxd0Mi1CRPQudwxZkDQjIoaWOw5L+N9H8yJir3LH0Bx3882so2kANsj53i/d19oyK3EyNbOO\nZjowUNLGkjoDo4GJTcpMBI5M7+rvCCyKiGa7+OBuviXG5y9ibcj/PkooIpZKGgNMBmqBqyNilqQT\n0uPjgEnAPsBs4CPg6Hz1KiJKF7WZWQfhbr6ZWQacTM3MMuBkamaWASdTM7MMOJlas6QKXYXXViJp\nTUn16ecNJHUvd0wdkadGWbMiIiR9AxgB/BOYFRFvljcqy5X+hTcY2F1SDbAjcALwXlkD64DcMrXP\naWyRShoK/BpYD/gu8L3GFpBVhkjmNr4KfAn4EfCPxsnl7lm0LSdT+5y0RbodcAkwJiKOA24F1iZ5\nKmSDFiuwNtGYLNPewv3A7cAmknZN94ck9z7biJOprdCkJbOEZC3HowAi4h6Srn5f4GhJa7R5gLaC\nJKXJcqikYcCEiDgeeA04QtIQSZsCBzihtg0nU1sh/Y9zF0lHRMRTwO7AdpLOSo9PJnnM7taIWFzO\nWDu6nPHsu0jGSO+TtBVwMckjkL8GHgQWRsTS8kXacfhxUstt5XwZOAPYDzgpIi6XtDXJquRTIuKs\nsgZqK6SJ89vAxIh4IH2u/P8CB0bEk5K2AdaIiEfKGmgH4ua/rWiRAlcDRwJ/Ay5Mc+w4SScD4yVd\nC8wJ/w1cNukrNwT8FBgETJZUk/57CpIW6rci4qGyBtoBOZl2UOkrGA6NiN+nu/oD90TEw8DDkp4D\n/iVpSUT8UdLXI8LTbcqksfcA1EbEp5K+R3KDcH9gFvBaRFyRTo/qXM5YOyqPmXZc3UlaNY3vtXkV\nWF/SWmlL52HgOuBXkr7pRFo+OcMwewCXSzqO5NUmJwG9gR83TlmLiMsjYoqnRbU9j5l2YJLWBK4A\n3o6IUyRdnx66GFgX+A7wOLA5cJy79+UjaXfg9yTjoj8D5gJ/AGYCN5H8ZXiabwyWj1umHUxuiyUi\nPgF+B6wj6WcRcSTJqxlOAM4H/huYA6xJ3hfoWimkK733JHn18IEkU9a6AS8BPyB9VTHwRyfS8nLL\ntANKp9RsTDJt5m+ShpDc0HgmIs5Py3QHvgJcCHw7nSplbSRnjLTx+zrAGsCNwDeB5cA04N/A6RHx\nTlkCtRXcMu0gch4R3QH4I8lreH8i6fyImEXSEt1WUuMNqWXAJsB3nEjbXjpGurOkUyQNBD4k6R2s\nCywlGSt9BfidE2llcMu0A0mflDkU+HdETJS0EfBXYFJE/CxtodZFxBNlDdSQtDNwOfAc0Am4OSJu\nlnQhMJLk3UU/iYi/lTFMy+GpUR3LDiRTaeZLWiMi5ko6APi7pM4RcTp8votpbUvSlsA5JL2Cmek0\nqK+nnYuzSGZZLI+I5/3vqnI4mbZjOVNqNiGZhzhW0gLg+8AjkqZFxH/SKTcrFi/xf5xtr0lS7A9s\nCRwAzIyIqyQtJ2mR1kXEDY3n+d9V5XA3v52TtDdwHnAPsB0wimR+4u7Ab4EHImJJ+SK0RpJ2A74Q\nERMkjQKDmdS2AAAG90lEQVSOJ1nAZHx6/DhgqsewK5Nbpu2YpMHAL4GDgG+RTPReMyIuTp+UOSs9\n5hsYZZLTe9iGZM3YIyQdkCbU5cAx6RDM2Ii4sszhWgucTNsZSbURsSz9uhi4imTS/SHAYRHxgaSd\nIuJ3kv7iO8HllSbS3UgenjiJ5IbTnyQdGxG3pcvnHSdpAjDP3frK5W5+OyGpW0S8n37ehWQe6WJg\nLLAQGJYm0q8CpwPfa1yR3dqWpPWAXSPilvT7GKBnzhzfPYA7gEMi4m5JfSLi9fJFbIXwPNN2QFIX\n4G5JB0oaBIwH9gCGAv8hmZd4oKRDSB4VHe9EWlabAU+lE/EheV/T9o0HI+I+4E7gKkm7OZFWB7dM\n24l0itMZJJO7z4qIh9KV1kcCXyZ5JHQ28M+IuMdTatqepL7AiIi4UdJapM/Wp7MspgALSG46Nc4H\nnk8yEnBuuWK2wnnMtJ2IiDskvQ/8Bfg68BBJq/RlYIOIOK2xrBNp2QwieSnhFyLiSkn3AHsqebPB\nCEm3AONIZl0cRpJUty1jvNYKTqbtSET8Q9JRwG8kzYmImyQtAnaV1Ad4I1LljbTDephkrYOT0nVi\nr5X0Kcl7moiIQ9PFn3uQJNQfkCRVqwJOpu1M2kJdClwnaTTwCXCux93Kp7EnEBEfS7qf5F7FSen+\na5SskH+YpF7ptLVaYEeSBWZmlTV4K5iTaTsUEXemjyCeS7IO6cPu2pdHzjzSoSQ3ApdExL3pwjP/\nJWl5RFyXJtA5kLy6WdJFfpiiujiZtlMR8VdJUyLi7fS7E2kZpIl0X5K3hV4FHCnpR+lNwOUkq+TX\nRsTVsFIr1om0yjiZtmONidTKR9LmJD2E/YBdSFaAulLSyWkPohZ4o7G8/9KrXp4aZZaxnK79GiTr\nji4GtiCZCvUV4ESSR3m/ExETyxepZcktU7OMpYn0AOAYkqlptwFfAG5Mn0J7Fbgd+KCMYVrG3DI1\ny0hOi7QncC1wC9CV5Jn7F4HXSVbHPxE4KCIe943B9sMtU7OMpIl0B5I5oo9GxE0Akt4BfkLSOp0J\nnBIRjzeeU654LVtOpmZFymmR7gRcQ/LY7rqSHiBZL/Z2SZ1IXtF8R0S85RZp++NuvlkG0hbp+cCp\nEfGUpPOAniRjow9FxBJJ9RHRUNZArWS8apRZNnoAXyN5gwEk06HeJlnweWcAJ9L2zcnULAPpsnkH\nAsdKOjyddH8e8Bo580it/XI33yxDkvYhSaJ/iIhryxyOtSEnU7OMSdqfZHWo3YDXc14jY+2Yk6lZ\nCUjqHRFvljsOaztOpmZmGfANKDOzDDiZmpllwMnUzCwDTqZmZhlwMrXVJmmZpJmSnpZ0m6QuRdQ1\nQtJd6ef9JZ3RQtmekv5rNa7xC0mnFbq/SZlrJR3Uimv1l/R0a2O06uVkasX4OCK2iYgtgU+BE3IP\nKtHq/49FxMSIuLCFIj2BVidTs1JyMrWs/C8wIG2RPS/peuBpYANJe0h6WNJjaQu2K4CkvSQ9J+kx\n4FuNFUk6StLY9HMfSXdIeiLddiKZEL9p2ir+TVruR5KmS3pS0jk5df1U0gvpCk6b5/sRko5L63lC\n0l+atLZ3kzQjrW9kWr5W0m9yrv39Yv8grTo5mVrRJNUBewNPpbsGApdFxBDgQ5JXdOwWEdsBM4BT\nJa0JXEnybqTtgfWaqf4S4P6I2JpkndBZwBnAnLRV/CNJe6TXHA5sA2wv6auStgdGp/v2AYYV8HP+\nGhHD0us9Cxybc6x/eo19gXHpbzgWWBQRw9L6j5O0cQHXsXbG65laMdaSNDP9/L/AH4G+wNyImJru\n3xEYDDyYvN2YzsDDwCDg5Yh4EUDSDcDxq7jG14EjAdLHMhdJWrtJmT3S7fH0e1eS5NqNZP3Qj9Jr\nFPK+pS0lnU8ylNAVmJxz7NaIWA68KOml9DfsAWyVM57aI732CwVcy9oRJ1MrxscRsU3ujjRhfpi7\nC/h7RBzWpNxK5xVJwAURcUWTa/xwNeq6FvhmRDwh6ShgRM6xpo8LRnrtH0REbtJFUv/VuLZVMXfz\nrdSmAl+RNABA0hckbQY8B/SXtGla7rBmzv8nyTuTGscnewDvk7Q6G00GjskZi62XtC7wb+CbktaS\n1I1kSCGfbsCCdGX8I5ocO1hSTRrzJsDz6bVPTMsjaTNJXyjgOtbOuGVqJRURb6YtvJvSVx8DnBUR\nL0g6Hrhb0kckwwTdVlHF/wHGSzoWWAacGBEPS3ownXp0TzpuugXwcNoy/gD4dkQ8JukW4AmSNUWn\nFxDyz4BHgDfTf+bG9B9gGtAdOCEiPpF0FclY6mNKLv4m8M3C/nSsPfFCJ2ZmGXA338wsA06mZmYZ\ncDI1M8uAk6mZWQacTM3MMuBkamaWASdTM7MM/H/S4qpf8HLycAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13958a0be48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds_path = sys.argv[1]\n",
    "thresh = 0.5\n",
    "\n",
    "# get ground truth labels for test dataset\n",
    "truth = pd.read_csv('ground_truth.csv')\n",
    "y_true = truth.as_matrix(columns=[\"task_1\", \"task_2\"])\n",
    "\n",
    "# get model predictions for test dataset\n",
    "y_pred = pd.read_csv(\"predictions.csv\")\n",
    "y_pred = y_pred.as_matrix(columns=[\"task_1\", \"task_2\"])\n",
    "\n",
    "# plot ROC curves and print scores\n",
    "plot_roc_auc(y_true, y_pred)\n",
    "# plot confusion matrix\n",
    "classes = ['benign', 'malignant']\n",
    "plot_confusion_matrix(y_true[:,0], y_pred[:,0], thresh, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
